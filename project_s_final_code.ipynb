{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9642efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "import time\n",
    "import concurrent.futures\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "from threading import Lock\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Thread-safe lock for file writing\n",
    "file_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from typing import List\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "end_date = \"2025-04-12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c56537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    # Ensure the 'Close' column is correctly accessed\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Calculate price changes\n",
    "    delta = close_prices.diff()\n",
    "\n",
    "    # Separate gains (positive) and losses (negative)\n",
    "    gains = delta.where(delta > 0, 0)\n",
    "    losses = -delta.where(delta < 0, 0)\n",
    "\n",
    "    # Initialize the averages\n",
    "    avg_gains = [np.nan] * len(close_prices)\n",
    "    avg_losses = [np.nan] * len(close_prices)\n",
    "\n",
    "    # Calculate first averages after initial window\n",
    "    first_avg_gain = gains[1:window+1].mean()\n",
    "    first_avg_loss = losses[1:window+1].mean()\n",
    "    avg_gains[window] = first_avg_gain\n",
    "    avg_losses[window] = first_avg_loss\n",
    "\n",
    "    # Calculate subsequent values using the Wilder's smoothing method\n",
    "    for i in range(window+1, len(close_prices)):\n",
    "        avg_gain = (avg_gains[i-1] * (window-1) + gains[i]) / window\n",
    "        avg_loss = (avg_losses[i-1] * (window-1) + losses[i]) / window\n",
    "        avg_gains[i] = avg_gain\n",
    "        avg_losses[i] = avg_loss\n",
    "\n",
    "    # Convert to Series with proper index\n",
    "    avg_gains = pd.Series(avg_gains, index=close_prices.index)\n",
    "    avg_losses = pd.Series(avg_losses, index=close_prices.index)\n",
    "\n",
    "    # Calculate RS and RSI\n",
    "    rs = avg_gains / avg_losses\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ab0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbol, period='1y', end_date=None):\n",
    "    \"\"\"Fetch stock data with proper error handling and ensure unique data per stock\n",
    "\n",
    "    Args:\n",
    "        symbol (str): Stock symbol\n",
    "        period (str): Data period to fetch (default='1y')\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses current date\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with stock data, actual_end_date)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(1)  # Add delay to prevent rate limiting\n",
    "\n",
    "        kwargs = {\n",
    "            'tickers': f\"{symbol}.NS\",\n",
    "            'period': period,\n",
    "            'progress': False,\n",
    "            'threads': False,\n",
    "            'ignore_tz': True,\n",
    "            'auto_adjust': True,\n",
    "            'prepost': False,\n",
    "            'repair': True\n",
    "        }\n",
    "\n",
    "        # Add end parameter if end_date is provided\n",
    "        if end_date:\n",
    "            kwargs['end'] = end_date\n",
    "            # Calculate start date based on period\n",
    "            if period == '1y':\n",
    "                start_date = pd.to_datetime(end_date) - pd.DateOffset(years=1)\n",
    "            elif period == '2y':\n",
    "                start_date = pd.to_datetime(end_date) - pd.DateOffset(years=2)\n",
    "            kwargs['start'] = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Download data\n",
    "        data = yf.download(**kwargs)\n",
    "\n",
    "        if data.empty:\n",
    "            logger.warning(f\"No data available for {symbol}\")\n",
    "            return pd.DataFrame(), None\n",
    "\n",
    "        # Get the actual last date from the data\n",
    "        actual_end_date = data.index[-1]\n",
    "\n",
    "        # Handle multi-index columns if present\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            df = pd.DataFrame(index=data.index)\n",
    "            column_map = {\n",
    "                'Open': ('Open', symbol + '.NS'),\n",
    "                'High': ('High', symbol + '.NS'),\n",
    "                'Low': ('Low', symbol + '.NS'),\n",
    "                'Close': ('Close', symbol + '.NS'),\n",
    "                'Volume': ('Volume', symbol + '.NS')\n",
    "            }\n",
    "            for col, multi_idx in column_map.items():\n",
    "                try:\n",
    "                    if multi_idx in data.columns:\n",
    "                        df[col] = data[multi_idx]\n",
    "                    else:\n",
    "                        df[col] = data[(multi_idx[0],)]\n",
    "                except:\n",
    "                    df[col] = data[multi_idx[0]]\n",
    "        else:\n",
    "            df = data.copy()\n",
    "\n",
    "        # Verify data is valid\n",
    "        if 'Close' in df.columns:\n",
    "            latest_price = df['Close'].iloc[-1]\n",
    "            logger.info(f\"Verified unique data for {symbol}: Latest price = Rs.{latest_price:.2f}\")\n",
    "            if end_date:\n",
    "                requested_date = pd.to_datetime(end_date).normalize()\n",
    "                actual_date = actual_end_date.normalize()\n",
    "                if actual_date != requested_date:\n",
    "                    logger.info(f\"Note: Last available date ({actual_date.strftime('%Y-%m-%d')}) differs from requested date ({requested_date.strftime('%Y-%m-%d')})\")\n",
    "        else:\n",
    "            logger.error(f\"Missing Close column for {symbol}\")\n",
    "            return pd.DataFrame(), None\n",
    "\n",
    "        return df, actual_end_date\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return pd.DataFrame(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571694b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    try:\n",
    "        # Make sure we're working with a copy of the data to avoid warnings\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0].copy()\n",
    "        else:\n",
    "            close_series = data['Close'].copy()\n",
    "\n",
    "        # Calculate EMAs\n",
    "        ema_fast = close_series.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = close_series.ewm(span=slow, adjust=False).mean()\n",
    "\n",
    "        # Calculate MACD components\n",
    "        macd_line = ema_fast - ema_slow\n",
    "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "        histogram = macd_line - signal_line\n",
    "\n",
    "        return macd_line, signal_line, histogram\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating MACD: {e}\")\n",
    "        empty_series = pd.Series(dtype=float)\n",
    "        return empty_series, empty_series, empty_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum(data, period=20):\n",
    "    try:\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0]\n",
    "        else:\n",
    "            close_series = data['Close']\n",
    "\n",
    "        momentum = close_series / close_series.shift(period) - 1\n",
    "        return momentum * 100\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating momentum: {e}\")\n",
    "        return pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da881ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum_index(data, period=126):\n",
    "    try:\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0]\n",
    "        else:\n",
    "            close_series = data['Close']\n",
    "\n",
    "        returns = close_series.pct_change().dropna()\n",
    "        momentum_std = returns.rolling(window=period).std() * np.sqrt(252)\n",
    "        return momentum_std\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating momentum index: {e}\")\n",
    "        return pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamental_data(symbol: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch fundamental data\"\"\"\n",
    "    try:\n",
    "        ticker = yf.Ticker(f\"{symbol}.NS\")\n",
    "        info = ticker.info\n",
    "        fundamental_data = {\n",
    "            'P/E Ratio': info.get('trailingPE', 'N/A'),\n",
    "            'Forward P/E': info.get('forwardPE', 'N/A'),\n",
    "            'Market Cap': info.get('marketCap', 'N/A'),\n",
    "            'EPS': info.get('trailingEps', 'N/A'),\n",
    "            'Dividend Yield': info.get('dividendYield', 'N/A'),\n",
    "            'Debt to Equity': info.get('debtToEquity', 'N/A'),\n",
    "            'Return on Equity': info.get('returnOnEquity', 'N/A'),\n",
    "            'Revenue Growth': info.get('revenueGrowth', 'N/A'),\n",
    "            'Profit Margins': info.get('profitMargins', 'N/A'),\n",
    "            'Beta': info.get('beta', 'N/A'),\n",
    "            'Current Ratio': info.get('currentRatio', 'N/A'),\n",
    "            'Book Value': info.get('bookValue', 'N/A'),\n",
    "            '52-Week High': info.get('fiftyTwoWeekHigh', 'N/A'),\n",
    "            '52-Week Low': info.get('fiftyTwoWeekLow', 'N/A'),\n",
    "            'Target Price': info.get('targetMeanPrice', 'N/A')\n",
    "        }\n",
    "        return fundamental_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching fundamental data for {symbol}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef869573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_strength(data, rsi, macd_line, signal_line):\n",
    "    # Ensure data is not empty\n",
    "    if isinstance(data, pd.DataFrame) and data.empty:\n",
    "        return [], []\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Extract the first level columns if multi-index\n",
    "        data_cols = {\n",
    "            'Open': data['Open'].iloc[:, 0],\n",
    "            'High': data['High'].iloc[:, 0],\n",
    "            'Low': data['Low'].iloc[:, 0],\n",
    "            'Close': data['Close'].iloc[:, 0],\n",
    "            'Volume': data['Volume'].iloc[:, 0]\n",
    "        }\n",
    "        data_df = pd.DataFrame(data_cols, index=data.index)\n",
    "    else:\n",
    "        data_df = data\n",
    "\n",
    "    current_price = data_df['Close'].iloc[-1]\n",
    "    sma_50 = data_df['Close'].rolling(window=50).mean().iloc[-1]\n",
    "    sma_200 = data_df['Close'].rolling(window=200).mean().iloc[-1]\n",
    "\n",
    "    # Convert any Series to scalar values\n",
    "    if isinstance(current_price, pd.Series):\n",
    "        current_price = current_price.iloc[0]\n",
    "    if isinstance(sma_50, pd.Series):\n",
    "        sma_50 = sma_50.iloc[0]\n",
    "    if isinstance(sma_200, pd.Series):\n",
    "        sma_200 = sma_200.iloc[0]\n",
    "\n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "\n",
    "    # RSI Analysis\n",
    "    if not isinstance(rsi, pd.Series) or len(rsi) == 0:\n",
    "        print(\"RSI data is empty\")\n",
    "    else:\n",
    "        last_rsi = rsi.iloc[-1]\n",
    "        if isinstance(last_rsi, pd.Series):\n",
    "            last_rsi = last_rsi.item()\n",
    "        if last_rsi > 70:\n",
    "            weaknesses.append(\"RSI indicates overbought conditions\")\n",
    "        elif last_rsi < 30:\n",
    "            strengths.append(\"RSI indicates oversold conditions (potential buying opportunity)\")\n",
    "        elif 40 <= last_rsi <= 60:\n",
    "            strengths.append(\"RSI in neutral zone showing balance between buyers and sellers\")\n",
    "        elif 60 < last_rsi < 70:\n",
    "            strengths.append(\"Strong RSI showing positive momentum\")\n",
    "\n",
    "    # MACD Analysis\n",
    "    if (not isinstance(macd_line, pd.Series) or len(macd_line) == 0 or\n",
    "        not isinstance(signal_line, pd.Series) or len(signal_line) == 0):\n",
    "        print(\"MACD or signal line data is empty\")\n",
    "    else:\n",
    "        macd_value = macd_line.iloc[-1]\n",
    "        signal_value = signal_line.iloc[-1]\n",
    "        if isinstance(macd_value, pd.Series):\n",
    "            macd_value = macd_value.item()\n",
    "        if isinstance(signal_value, pd.Series):\n",
    "            signal_value = signal_value.item()\n",
    "        if macd_value > signal_value:\n",
    "            strengths.append(\"MACD line above signal line indicating bullish momentum\")\n",
    "        else:\n",
    "            weaknesses.append(\"MACD line below signal line indicating bearish momentum\")\n",
    "\n",
    "    # Moving Average Analysis\n",
    "    if pd.notna(sma_50) and pd.notna(current_price):\n",
    "        if current_price > sma_50:\n",
    "            strengths.append(\"Price above 50-day SMA showing short-term strength\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price below 50-day SMA showing short-term weakness\")\n",
    "\n",
    "    if pd.notna(sma_200) and pd.notna(current_price):\n",
    "        if current_price > sma_200:\n",
    "            strengths.append(\"Price above 200-day SMA suggesting long-term uptrend\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price below 200-day SMA suggesting long-term downtrend\")\n",
    "\n",
    "    # Golden/Death Cross\n",
    "    if pd.notna(sma_50) and pd.notna(sma_200):\n",
    "        if sma_50 > sma_200 and (sma_50 / sma_200 - 1) < 0.03:\n",
    "            strengths.append(\"Recent golden cross or nearing golden cross (50-day SMA crossing above 200-day SMA)\")\n",
    "        elif sma_50 < sma_200 and (sma_200 / sma_50 - 1) < 0.03:\n",
    "            weaknesses.append(\"Recent death cross or nearing death cross (50-day SMA crossing below 200-day SMA)\")\n",
    "\n",
    "    # Volume Analysis\n",
    "    avg_volume = data_df['Volume'].mean()\n",
    "    if isinstance(avg_volume, pd.Series):\n",
    "        avg_volume = avg_volume.item()\n",
    "\n",
    "    recent_volume = data_df['Volume'].iloc[-5:].mean()\n",
    "    if isinstance(recent_volume, pd.Series):\n",
    "        recent_volume = recent_volume.item()\n",
    "\n",
    "    # Calculate price trend\n",
    "    recent_price_mean = data_df['Close'].iloc[-5:].mean()\n",
    "    if isinstance(recent_price_mean, pd.Series):\n",
    "        recent_price_mean = recent_price_mean.item()\n",
    "\n",
    "    price_trend_up = current_price > recent_price_mean\n",
    "\n",
    "    if recent_volume > avg_volume * 1.2:\n",
    "        if price_trend_up:\n",
    "            strengths.append(\"Strong volume supporting upward price movement\")\n",
    "        else:\n",
    "            weaknesses.append(\"High volume during price decline indicates selling pressure\")\n",
    "    elif recent_volume < avg_volume * 0.8:\n",
    "        if price_trend_up:\n",
    "            strengths.append(\"Price rising on low volume - potential weakness\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price declining on low volume - potential for reversal\")\n",
    "\n",
    "    # Price Movement\n",
    "    if len(data_df['Close']) >= 22:\n",
    "        latest_price = data_df['Close'].iloc[-1]\n",
    "        price_22_days_ago = data_df['Close'].iloc[-22]\n",
    "\n",
    "        if isinstance(latest_price, pd.Series):\n",
    "            latest_price = latest_price.item()\n",
    "        if isinstance(price_22_days_ago, pd.Series):\n",
    "            price_22_days_ago = price_22_days_ago.item()\n",
    "\n",
    "        monthly_return = (latest_price / price_22_days_ago - 1) * 100\n",
    "\n",
    "        if monthly_return > 5:\n",
    "            strengths.append(f\"Strong monthly return of {monthly_return:.2f}%\")\n",
    "        elif monthly_return < -5:\n",
    "            weaknesses.append(f\"Weak monthly return of {monthly_return:.2f}%\")\n",
    "\n",
    "    # Volatility\n",
    "    if len(data_df['Close']) >= 2:\n",
    "        returns = data_df['Close'].pct_change().dropna()\n",
    "        if len(returns) > 0:\n",
    "            volatility = returns.std() * np.sqrt(252) * 100\n",
    "            if isinstance(volatility, pd.Series):\n",
    "                volatility = volatility.item()\n",
    "\n",
    "            if volatility > 30:\n",
    "                weaknesses.append(f\"High volatility ({volatility:.2f}%) indicating increased risk\")\n",
    "            elif volatility < 15:\n",
    "                strengths.append(f\"Low volatility ({volatility:.2f}%) indicating stability\")\n",
    "\n",
    "    return strengths, weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekday(date):\n",
    "    \"\"\"Get weekday number (1=Monday to 7=Sunday)\"\"\"\n",
    "    return date.isoweekday()\n",
    "\n",
    "def get_next_friday(target_date):\n",
    "    \"\"\"Get the next Friday from a given date. If the date is Friday, return the same date.\"\"\"\n",
    "    weekday = get_weekday(target_date)\n",
    "    if weekday == 5:  # If Friday\n",
    "        return target_date\n",
    "\n",
    "    days_until_friday = {\n",
    "        1: 4,  # Monday -> +4 days\n",
    "        2: 3,  # Tuesday -> +3 days\n",
    "        3: 2,  # Wednesday -> +2 days\n",
    "        4: 1,  # Thursday -> +1 day\n",
    "        6: 6,  # Saturday -> +6 days\n",
    "        7: 5   # Sunday -> +5 days\n",
    "    }\n",
    "    days_to_add = days_until_friday[weekday]\n",
    "    return target_date + pd.Timedelta(days=days_to_add)\n",
    "\n",
    "def get_next_thursday(target_date):\n",
    "    \"\"\"Get the next Thursday from a given date. If the date is Friday, return the previous day.\"\"\"\n",
    "    weekday = get_weekday(target_date)\n",
    "    if weekday == 5:  # If Friday\n",
    "        return target_date - pd.Timedelta(days=1)\n",
    "    return get_next_friday(target_date) - pd.Timedelta(days=1)\n",
    "\n",
    "def find_closest_trading_day(data, target_date):\n",
    "    \"\"\"Find the closest trading day in the data for a given target date\"\"\"\n",
    "    target_friday = get_next_friday(target_date)\n",
    "    target_thursday = get_next_thursday(target_date)\n",
    "\n",
    "    # Convert index to datetime if it's not already and normalize to remove time component\n",
    "    date_index = pd.to_datetime(data.index).normalize()\n",
    "\n",
    "    # Normalize target dates to remove time component\n",
    "    target_friday = pd.to_datetime(target_friday).normalize()\n",
    "    target_thursday = pd.to_datetime(target_thursday).normalize()\n",
    "\n",
    "    # Try to find exact matches first\n",
    "    friday_match = date_index[date_index == target_friday]\n",
    "    thursday_match = date_index[date_index == target_thursday]\n",
    "\n",
    "    if not friday_match.empty:\n",
    "        return friday_match[0]\n",
    "    elif not thursday_match.empty:\n",
    "        return thursday_match[0]\n",
    "\n",
    "    # If no exact match, find the closest available date\n",
    "    all_dates = date_index\n",
    "    closest_date = min(all_dates, key=lambda x: abs(x - target_friday))\n",
    "\n",
    "    return closest_date\n",
    "\n",
    "def get_date_one_year_back(data, from_date=None):\n",
    "    \"\"\"Calculate the date from the given date (or last date in stock data) to 1 year back\"\"\"\n",
    "    if from_date:\n",
    "        last_date = pd.to_datetime(from_date).normalize()\n",
    "    else:\n",
    "        last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "    target_date = last_date - pd.DateOffset(months=12)\n",
    "    return find_closest_trading_day(data, target_date)\n",
    "\n",
    "def get_date_six_months_back(data, from_date=None):\n",
    "    \"\"\"Calculate the date from the given date (or last date in stock data) to 6 months back\"\"\"\n",
    "    if from_date:\n",
    "        last_date = pd.to_datetime(from_date).normalize()\n",
    "    else:\n",
    "        last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "    target_date = last_date - pd.DateOffset(months=6)\n",
    "    return find_closest_trading_day(data, target_date)\n",
    "\n",
    "def get_first_available_date(data):\n",
    "    \"\"\"Get the first date for which price data is available\"\"\"\n",
    "    return pd.to_datetime(data.index[0]).normalize()\n",
    "\n",
    "def get_prices_for_dates(data, end_date=None):\n",
    "    \"\"\"Get prices for specified date (or current date), 6 months back and 1 year back, and calculate price changes.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing price data and changes for different time periods\n",
    "    \"\"\"\n",
    "    # Get the actual last date from data\n",
    "    actual_last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    # If end_date is provided but different from actual_last_date, use actual_last_date\n",
    "    if end_date:\n",
    "        requested_date = pd.to_datetime(end_date).normalize()\n",
    "        if requested_date != actual_last_date:\n",
    "            logger.info(f\"Using last available date {actual_last_date.strftime('%Y-%m-%d')} instead of requested date {requested_date.strftime('%Y-%m-%d')}\")\n",
    "            current_date = actual_last_date\n",
    "        else:\n",
    "            current_date = requested_date\n",
    "    else:\n",
    "        current_date = actual_last_date\n",
    "\n",
    "    six_month_date = get_date_six_months_back(data, current_date)\n",
    "    one_year_date = get_date_one_year_back(data, current_date)\n",
    "    first_available_date = get_first_available_date(data)\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Get current price (should always be available since we're using actual last date)\n",
    "    current_price = close_prices[current_date]\n",
    "\n",
    "    # Get 6-month price if available, otherwise use first available date\n",
    "    try:\n",
    "        six_month_price = close_prices[six_month_date]\n",
    "        six_month_actual_date = six_month_date\n",
    "    except KeyError:\n",
    "        six_month_price = close_prices[first_available_date]\n",
    "        six_month_actual_date = first_available_date\n",
    "\n",
    "    # Get 1-year price if available, otherwise use first available date\n",
    "    try:\n",
    "        one_year_price = close_prices[one_year_date]\n",
    "        one_year_actual_date = one_year_date\n",
    "    except KeyError:\n",
    "        one_year_price = close_prices[first_available_date]\n",
    "        one_year_actual_date = first_available_date\n",
    "\n",
    "    # Calculate price changes\n",
    "    twelve_month_change = (current_price / one_year_price - 1) if one_year_price != 0 else 0\n",
    "    six_month_change = (current_price / six_month_price - 1) if six_month_price != 0 else 0\n",
    "\n",
    "    # Create a dictionary with dates, prices and price changes\n",
    "    price_data = {\n",
    "        'Current': {\n",
    "            'date': current_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(current_price, 2)\n",
    "        },\n",
    "        '6_months_back': {\n",
    "            'date': six_month_actual_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(six_month_price, 2),\n",
    "            'price_change_percent': round(six_month_change * 100, 2),\n",
    "            'is_first_available': six_month_actual_date == first_available_date\n",
    "        },\n",
    "        '1_year_back': {\n",
    "            'date': one_year_actual_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(one_year_price, 2),\n",
    "            'price_change_percent': round(twelve_month_change * 100, 2),\n",
    "            'is_first_available': one_year_actual_date == first_available_date\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_returns(data, end_date=None):\n",
    "    \"\"\"Calculate logarithmic returns between consecutive trading days for the past year\n",
    "    Returns the log returns as percentages along with the prices used in calculations.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (log_returns, price_data)\n",
    "    \"\"\"\n",
    "    # Get the dates for 1 year period\n",
    "    if end_date:\n",
    "        current_date = pd.to_datetime(end_date).normalize()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    one_year_back_date = get_date_one_year_back(data, end_date)\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Get the slice of data between one year back and current date\n",
    "    mask = (data.index >= one_year_back_date) & (data.index <= current_date)\n",
    "    period_prices = close_prices[mask]\n",
    "\n",
    "    # Create a DataFrame to store prices and calculations\n",
    "    price_data = pd.DataFrame({\n",
    "        'Current_Price': period_prices.astype(float),\n",
    "        'Previous_Price': period_prices.shift(1).astype(float)\n",
    "    })\n",
    "\n",
    "    # Calculate price ratios first (current/previous)\n",
    "    price_data['Price_Ratio'] = price_data['Current_Price'] / price_data['Previous_Price']\n",
    "\n",
    "    # Calculate log returns exactly as Excel does: LN(current/previous)\n",
    "    price_data['Log_Return_Percent'] = np.log(price_data['Price_Ratio']) * 100\n",
    "\n",
    "    # Round to 3 decimal places after all calculations\n",
    "    price_data['Log_Return_Percent'] = price_data['Log_Return_Percent'].round(3)\n",
    "\n",
    "    # Round other columns to 2 decimal places\n",
    "    for column in ['Current_Price', 'Previous_Price', 'Price_Ratio']:\n",
    "        price_data[column] = price_data[column].round(2)\n",
    "\n",
    "    # Drop the first row as it will have NaN values due to the shift operation\n",
    "    price_data = price_data.dropna()\n",
    "\n",
    "    # Create the log returns series\n",
    "    log_returns = price_data['Log_Return_Percent']\n",
    "\n",
    "    return log_returns, price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_stdev(data, end_date=None):\n",
    "    \"\"\"Calculate standard deviation using all log values from last 1 year till the specified date.\n",
    "    Similar to Excel's STDEV(A:B) where A:B would be the range of log values.\n",
    "    Returns the standard deviation divided by 100.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (one_year_stdev, six_month_stdev) where values are divided by 100\n",
    "    \"\"\"\n",
    "    # Get the dates\n",
    "    if end_date:\n",
    "        current_date = pd.to_datetime(end_date).normalize()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    six_month_date = get_date_six_months_back(data, end_date)\n",
    "    one_year_date = get_date_one_year_back(data, end_date)\n",
    "\n",
    "    # Get log returns for the entire period\n",
    "    log_returns, _ = calculate_log_returns(data, end_date)\n",
    "\n",
    "    try:\n",
    "        # Get all log values from one year ago to current date\n",
    "        one_year_mask = (log_returns.index >= one_year_date) & (log_returns.index <= current_date)\n",
    "        one_year_logs = log_returns[one_year_mask]\n",
    "        # Calculate standard deviation using all values in the period and divide by 100\n",
    "        one_year_stdev = np.std(one_year_logs) / 100\n",
    "    except KeyError:\n",
    "        one_year_stdev = None\n",
    "\n",
    "    try:\n",
    "        # Get all log values from six months ago to current date\n",
    "        six_month_mask = (log_returns.index >= six_month_date) & (log_returns.index <= current_date)\n",
    "        six_month_logs = log_returns[six_month_mask]\n",
    "        # Calculate standard deviation using all values in the period and divide by 100\n",
    "        six_month_stdev = np.std(six_month_logs) / 100\n",
    "    except KeyError:\n",
    "        six_month_stdev = None\n",
    "\n",
    "    return one_year_stdev, six_month_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum_ratio(data, end_date=None):\n",
    "    \"\"\"Calculate momentum ratios for 1 year and 6 months periods.\n",
    "    Momentum ratio = Price change percentage / (Standard deviation * 100)\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing momentum ratios for both periods\n",
    "    \"\"\"\n",
    "    # Get price changes\n",
    "    prices_data = get_prices_for_dates(data, end_date)\n",
    "    one_year_change = prices_data['1_year_back']['price_change_percent']\n",
    "    six_month_change = prices_data['6_months_back']['price_change_percent']\n",
    "\n",
    "    # Get standard deviations (already divided by 100 in calculate_log_stdev)\n",
    "    one_year_stdev, six_month_stdev = calculate_log_stdev(data, end_date)\n",
    "\n",
    "    # Calculate momentum ratios\n",
    "    # Note: We multiply stdev by 100 to match the scale of price_change_percent\n",
    "    one_year_momentum = one_year_change / (one_year_stdev * 100) if one_year_stdev else None\n",
    "    six_month_momentum = six_month_change / (six_month_stdev * 100) if six_month_stdev else None\n",
    "\n",
    "    result = {\n",
    "        'momentum_ratios': {\n",
    "            'one_year': round(one_year_momentum, 4) if one_year_momentum is not None else None,\n",
    "            'six_month': round(six_month_momentum, 4) if six_month_momentum is not None else None\n",
    "        },\n",
    "        'components': {\n",
    "            'one_year': {\n",
    "                'price_change_percent': one_year_change,\n",
    "                'standard_deviation': one_year_stdev * 100 if one_year_stdev else None\n",
    "            },\n",
    "            'six_month': {\n",
    "                'price_change_percent': six_month_change,\n",
    "                'standard_deviation': six_month_stdev * 100 if six_month_stdev else None\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a52051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_universe_stats(momentum_values):\n",
    "    \"\"\"Calculate standard deviation and mean for a list of momentum ratios.\n",
    "    Similar to Excel's STDEV and AVERAGE functions.\n",
    "\n",
    "    Args:\n",
    "        momentum_values: List of momentum ratio values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (standard_deviation, mean)\n",
    "    \"\"\"\n",
    "    # Remove None and NaN values\n",
    "    valid_values = [v for v in momentum_values if v is not None and not np.isnan(v)]\n",
    "\n",
    "    if not valid_values:\n",
    "        return None, None\n",
    "\n",
    "    # Calculate standard deviation (similar to Excel's STDEV)\n",
    "    stdev = np.std(valid_values, ddof=1)  # ddof=1 for sample standard deviation (like Excel)\n",
    "\n",
    "    # Calculate mean (similar to Excel's AVERAGE)\n",
    "    mean = np.mean(valid_values)\n",
    "\n",
    "    return stdev, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_z_score(value, mean, stdev):\n",
    "    \"\"\"Calculate z-score for a value.\n",
    "\n",
    "    Z-score = (value - mean) / standard_deviation\n",
    "\n",
    "    Args:\n",
    "        value: The value to calculate z-score for\n",
    "        mean: Mean of the universe\n",
    "        stdev: Standard deviation of the universe\n",
    "\n",
    "    Returns:\n",
    "        float: z-score or None if inputs are invalid\n",
    "    \"\"\"\n",
    "    if value is None or mean is None or stdev is None or stdev == 0:\n",
    "        return None\n",
    "\n",
    "    return (value - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_z_score(one_year_z_score, six_month_z_score, weights=None):\n",
    "    \"\"\"Calculate weighted z-score from 1-year and 6-month z-scores.\n",
    "\n",
    "    Args:\n",
    "        one_year_z_score: Z-score for 1-year momentum ratio\n",
    "        six_month_z_score: Z-score for 6-month momentum ratio\n",
    "        weights: Dictionary with weights for each period.\n",
    "                Default is {'one_year': 0.5, 'six_month': 0.5}\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted z-score or None if inputs are invalid\n",
    "    \"\"\"\n",
    "    if one_year_z_score is None or six_month_z_score is None:\n",
    "        return None\n",
    "\n",
    "    # Use default weights if none provided\n",
    "    if weights is None:\n",
    "        weights = {'one_year': 0.5, 'six_month': 0.5}\n",
    "\n",
    "    weighted_z_score = (\n",
    "        one_year_z_score * weights['one_year'] +\n",
    "        six_month_z_score * weights['six_month']\n",
    "    )\n",
    "\n",
    "    return weighted_z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_z_score(z_score):\n",
    "    \"\"\"Calculate normalized z-score using the formula: IF(z_score>0,(1+z_score),(1-z_score)^-1)\n",
    "\n",
    "    Args:\n",
    "        z_score: The z-score to normalize\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized z-score or None if input is invalid\n",
    "    \"\"\"\n",
    "    if z_score is None:\n",
    "        return None\n",
    "\n",
    "    return (1 + z_score) if z_score > 0 else (1 - z_score) ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e66abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(value, values_list):\n",
    "    \"\"\"Calculate the rank of a value within a list. Similar to Excel's RANK(number, ref, [order])\n",
    "\n",
    "    Args:\n",
    "        value: The number whose rank you want to find\n",
    "        values_list: A list of numbers that defines the relative ranking\n",
    "\n",
    "    Returns:\n",
    "        int: The rank of the number (1 being the highest)\n",
    "    \"\"\"\n",
    "    # Remove None and NaN values from the list\n",
    "    valid_values = [v for v in values_list if v is not None and not np.isnan(v)]\n",
    "\n",
    "    if value is None or np.isnan(value) or not valid_values:\n",
    "        return None\n",
    "\n",
    "    # Sort values in descending order (mimicking Excel's RANK with order=0)\n",
    "    sorted_values = sorted(set(valid_values), reverse=True)\n",
    "\n",
    "    try:\n",
    "        # Find the rank (adding 1 because index starts at 0)\n",
    "        rank = sorted_values.index(value) + 1\n",
    "        return rank\n",
    "    except ValueError:\n",
    "        # Try to find closest match for floating point comparison issues\n",
    "        closest_idx = min(range(len(sorted_values)),\n",
    "                         key=lambda i: abs(sorted_values[i] - value))\n",
    "        return closest_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d425fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rank_analysis_markdown(stock_data, symbol, one_year_rank=None, six_month_rank=None, total_stocks=None):\n",
    "    \"\"\"Generate markdown documentation for RANK analysis calculations.\n",
    "\n",
    "    This function creates detailed markdown explaining all calculations used in the analysis.\n",
    "    \"\"\"\n",
    "    md_content = \"## RANK Analysis\\n\\n\"\n",
    "\n",
    "    # Date Calculations Section\n",
    "    md_content += \"### Date Reference Points\\n\\n\"\n",
    "    current_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
    "    one_year_back_date = get_date_one_year_back(stock_data)\n",
    "    six_month_back_date = get_date_six_months_back(stock_data)\n",
    "\n",
    "    md_content += f\"- **Current Date:** {current_date}\\n\"\n",
    "    md_content += f\"- **1-Year Reference Date:** {one_year_back_date.strftime('%Y-%m-%d')}\\n\"\n",
    "    md_content += f\"- **6-Month Reference Date:** {six_month_back_date.strftime('%Y-%m-%d')}\\n\\n\"\n",
    "\n",
    "    # Price Changes Section\n",
    "    md_content += \"### Price Changes\\n\\n\"\n",
    "    current_price = stock_data['Close'].iloc[-1]\n",
    "    year_start_price = stock_data.loc[one_year_back_date, 'Close']\n",
    "    six_month_start_price = stock_data.loc[six_month_back_date, 'Close']\n",
    "\n",
    "    year_change = (current_price / year_start_price - 1) * 100\n",
    "    six_month_change = (current_price / six_month_start_price - 1) * 100\n",
    "\n",
    "    md_content += f\"**1-Year Price Change:**\\n\"\n",
    "    md_content += f\"- Start Price: ₹{year_start_price:.2f}\\n\"\n",
    "    md_content += f\"- Current Price: ₹{current_price:.2f}\\n\"\n",
    "    md_content += f\"- Change: {year_change:.2f}%\\n\\n\"\n",
    "\n",
    "    md_content += f\"**6-Month Price Change:**\\n\"\n",
    "    md_content += f\"- Start Price: ₹{six_month_start_price:.2f}\\n\"\n",
    "    md_content += f\"- Current Price: ₹{current_price:.2f}\\n\"\n",
    "    md_content += f\"- Change: {six_month_change:.2f}%\\n\\n\"\n",
    "\n",
    "    # Log Returns Section\n",
    "    md_content += \"### Log Returns and Standard Deviation\\n\\n\"\n",
    "    log_returns, price_data = calculate_log_returns(stock_data)\n",
    "    one_year_stdev, six_month_stdev = calculate_log_stdev(stock_data)\n",
    "\n",
    "    md_content += \"**Log Returns Calculation:**\\n\"\n",
    "    md_content += \"- Formula: `ln(current_price / previous_price) * 100`\\n\"\n",
    "    md_content += f\"- Recent Log Returns (last 5 days):\\n\"\n",
    "    for date, row in price_data.tail().iterrows():\n",
    "        md_content += f\"  * {date.strftime('%Y-%m-%d')}: {row['Log_Return_Percent']:.3f}%\\n\"\n",
    "\n",
    "    md_content += \"\\n**Standard Deviation:**\\n\"\n",
    "    md_content += f\"- 1-Year σ: {one_year_stdev*100:.3f}%\\n\"\n",
    "    md_content += f\"- 6-Month σ: {six_month_stdev*100:.3f}%\\n\\n\"\n",
    "\n",
    "    # Momentum Ratio Section\n",
    "    md_content += \"### Momentum Ratio\\n\\n\"\n",
    "    momentum_ratio = calculate_momentum_ratio(stock_data)\n",
    "\n",
    "    md_content += \"**Momentum Ratio Formula:** Price Change % / (Standard Deviation * 100)\\n\\n\"\n",
    "\n",
    "    md_content += \"**1-Year Momentum Components:**\\n\"\n",
    "    md_content += f\"- Price Change: {momentum_ratio['components']['one_year']['price_change_percent']:.2f}%\\n\"\n",
    "    md_content += f\"- Standard Deviation: {momentum_ratio['components']['one_year']['standard_deviation']:.2f}%\\n\"\n",
    "    md_content += f\"- Momentum Ratio: {momentum_ratio['momentum_ratios']['one_year']:.4f}\\n\"\n",
    "    if one_year_rank is not None and total_stocks is not None:\n",
    "        md_content += f\"- Rank: {one_year_rank} out of {total_stocks}\\n\\n\"\n",
    "\n",
    "    md_content += \"**6-Month Momentum Components:**\\n\"\n",
    "    md_content += f\"- Price Change: {momentum_ratio['components']['six_month']['price_change_percent']:.2f}%\\n\"\n",
    "    md_content += f\"- Standard Deviation: {momentum_ratio['components']['six_month']['standard_deviation']:.2f}%\\n\"\n",
    "    md_content += f\"- Momentum Ratio: {momentum_ratio['momentum_ratios']['six_month']:.4f}\\n\"\n",
    "    if six_month_rank is not None and total_stocks is not None:\n",
    "        md_content += f\"- Rank: {six_month_rank} out of {total_stocks}\\n\\n\"\n",
    "\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e75103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stocks(stocks_df, output_dir=\"stock_analysis\", max_workers=5, end_date=None):\n",
    "    \"\"\"Analyze a list of stocks and calculate momentum metrics for each.\n",
    "\n",
    "    Args:\n",
    "        stocks_df: List of tuples (company_name, symbol)\n",
    "        output_dir: Directory to store analysis results\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        end_date: End date for analysis in YYYY-MM-DD format\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = {}\n",
    "\n",
    "    # Global rank data structure to store universe-wide statistics\n",
    "    global rank_data\n",
    "    rank_data = {\n",
    "        'one_year': {'values': [], 'ranks': {}, 'universe_stats': {'stdev': None, 'mean': None}},\n",
    "        'six_month': {'values': [], 'ranks': {}, 'universe_stats': {'stdev': None, 'mean': None}},\n",
    "        'weighted_z_score': {'values': [], 'ranks': {}},\n",
    "        'normalized_z_score': {'values': [], 'ranks': {}}\n",
    "    }\n",
    "\n",
    "    def process_stock(stock_tuple):\n",
    "        company_name, symbol = stock_tuple\n",
    "        logger.info(f\"Processing {company_name} ({symbol})\")\n",
    "\n",
    "        try:\n",
    "            # Get stock data with actual end date\n",
    "            data, actual_end_date = get_stock_data(symbol, period='2y', end_date=end_date)\n",
    "            if data.empty:\n",
    "                logger.warning(f\"No data available for {symbol}\")\n",
    "                return None\n",
    "\n",
    "            # Use actual end date for all calculations\n",
    "            actual_end_date_str = actual_end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            # Calculate technical indicators\n",
    "            rsi = calculate_rsi(data)\n",
    "            macd_line, signal_line, histogram = calculate_macd(data)\n",
    "            momentum_period20 = calculate_momentum(data, period=20)\n",
    "            momentum_index = calculate_momentum_index(data)\n",
    "            momentum_ratio = calculate_momentum_ratio(data, actual_end_date_str)\n",
    "\n",
    "            # Get price data using actual end date\n",
    "            prices = get_prices_for_dates(data, actual_end_date_str)\n",
    "\n",
    "            # Determine strengths and weaknesses\n",
    "            strengths, weaknesses = determine_strength(data, rsi, macd_line, signal_line)\n",
    "\n",
    "            # Store the result with all necessary data\n",
    "            result = {\n",
    "                'company_name': company_name,\n",
    "                'symbol': symbol,\n",
    "                'actual_end_date': actual_end_date_str,\n",
    "                'current_price': prices['Current']['price'],\n",
    "                'price_data': prices,\n",
    "                'technical_indicators': {\n",
    "                    'rsi': rsi.iloc[-1] if not rsi.empty else None,\n",
    "                    'macd': {\n",
    "                        'macd_line': macd_line.iloc[-1] if not macd_line.empty else None,\n",
    "                        'signal_line': signal_line.iloc[-1] if not signal_line.empty else None,\n",
    "                        'histogram': histogram.iloc[-1] if not histogram.empty else None\n",
    "                    },\n",
    "                    'momentum': {\n",
    "                        'momentum_20day': momentum_period20.iloc[-1] if not momentum_period20.empty else None,\n",
    "                        'momentum_index': momentum_index.iloc[-1] if not momentum_index.empty else None,\n",
    "                        'momentum_ratio': momentum_ratio\n",
    "                    }\n",
    "                },\n",
    "                'analysis': {\n",
    "                    'strengths': strengths,\n",
    "                    'weaknesses': weaknesses\n",
    "                },\n",
    "                'data': data\n",
    "            }\n",
    "\n",
    "            # Store momentum values for universe calculations\n",
    "            one_year_momentum = momentum_ratio['momentum_ratios']['one_year']\n",
    "            six_month_momentum = momentum_ratio['momentum_ratios']['six_month']\n",
    "\n",
    "            if one_year_momentum is not None:\n",
    "                rank_data['one_year']['values'].append(one_year_momentum)\n",
    "            if six_month_momentum is not None:\n",
    "                rank_data['six_month']['values'].append(six_month_momentum)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {symbol}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Process stocks in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_stock, stock): stock for stock in stocks_df}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            stock = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[result['symbol']] = result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {stock}: {e}\")\n",
    "\n",
    "    # Calculate universe statistics for both periods\n",
    "    for period in ['one_year', 'six_month']:\n",
    "        values = rank_data[period]['values']\n",
    "        stdev, mean = calculate_universe_stats(values)\n",
    "        rank_data[period]['universe_stats']['stdev'] = stdev\n",
    "        rank_data[period]['universe_stats']['mean'] = mean\n",
    "\n",
    "        # Calculate ranks and z-scores\n",
    "        for symbol, result in results.items():\n",
    "            momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios'][period]\n",
    "            if momentum is not None:\n",
    "                rank = calculate_rank(momentum, values)\n",
    "                z_score = calculate_z_score(momentum, mean, stdev)\n",
    "                rank_data[period]['ranks'][symbol] = rank\n",
    "\n",
    "                # Update the result with rank and z-score information\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['ranks'] = result['technical_indicators']['momentum']['momentum_ratio'].get('ranks', {})\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['ranks'][period] = rank\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['z_scores'] = result['technical_indicators']['momentum']['momentum_ratio'].get('z_scores', {})\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['z_scores'][period] = z_score\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['universe_stats'] = {\n",
    "                    'stdev': stdev,\n",
    "                    'mean': mean\n",
    "                }\n",
    "\n",
    "    # Calculate weighted z-score ranks and normalized z-scores\n",
    "    weighted_z_scores = {}\n",
    "    normalized_z_scores = {}\n",
    "    for symbol, result in results.items():\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "        weighted_z_score = calculate_weighted_z_score(one_year_z_score, six_month_z_score)\n",
    "\n",
    "        if weighted_z_score is not None:\n",
    "            weighted_z_scores[symbol] = weighted_z_score\n",
    "            rank_data['weighted_z_score']['values'].append(weighted_z_score)\n",
    "\n",
    "            # Calculate normalized z-score\n",
    "            normalized_z_score = calculate_normalized_z_score(weighted_z_score)\n",
    "            if normalized_z_score is not None:\n",
    "                normalized_z_scores[symbol] = normalized_z_score\n",
    "                rank_data['normalized_z_score']['values'].append(normalized_z_score)\n",
    "\n",
    "    # Calculate ranks for weighted z-scores\n",
    "    rank_data['weighted_z_score']['values'] = [v for v in rank_data['weighted_z_score']['values'] if v is not None and not np.isnan(v)]\n",
    "    sorted_weighted_z = sorted(rank_data['weighted_z_score']['values'], reverse=True)\n",
    "    for symbol, weighted_z_score in weighted_z_scores.items():\n",
    "        if weighted_z_score is not None and not np.isnan(weighted_z_score) and sorted_weighted_z:\n",
    "            try:\n",
    "                rank = sorted_weighted_z.index(weighted_z_score) + 1\n",
    "            except ValueError:\n",
    "                # Find the closest value if exact match not found due to floating point issues\n",
    "                closest_idx = min(range(len(sorted_weighted_z)),\n",
    "                                key=lambda i: abs(sorted_weighted_z[i] - weighted_z_score))\n",
    "                rank = closest_idx + 1\n",
    "\n",
    "            rank_data['weighted_z_score']['ranks'][symbol] = rank\n",
    "            results[symbol]['technical_indicators']['momentum']['momentum_ratio']['weighted_z_score'] = {\n",
    "                'score': weighted_z_score,\n",
    "                'rank': rank,\n",
    "                'total_stocks': len(sorted_weighted_z)  # Use count of valid values\n",
    "            }\n",
    "\n",
    "    # Calculate ranks for normalized z-scores\n",
    "    rank_data['normalized_z_score']['values'] = [v for v in rank_data['normalized_z_score']['values'] if v is not None and not np.isnan(v)]\n",
    "    sorted_normalized_z = sorted(rank_data['normalized_z_score']['values'], reverse=True)\n",
    "    for symbol, normalized_z_score in normalized_z_scores.items():\n",
    "        if normalized_z_score is not None and not np.isnan(normalized_z_score) and sorted_normalized_z:\n",
    "            try:\n",
    "                rank = sorted_normalized_z.index(normalized_z_score) + 1\n",
    "            except ValueError:\n",
    "                # Find the closest value if exact match not found due to floating point issues\n",
    "                closest_idx = min(range(len(sorted_normalized_z)),\n",
    "                                key=lambda i: abs(sorted_normalized_z[i] - normalized_z_score))\n",
    "                rank = closest_idx + 1\n",
    "\n",
    "            rank_data['normalized_z_score']['ranks'][symbol] = rank\n",
    "            results[symbol]['technical_indicators']['momentum']['momentum_ratio']['normalized_z_score'] = {\n",
    "                'score': normalized_z_score,\n",
    "                'rank': rank,\n",
    "                'total_stocks': len(sorted_normalized_z)  # Use count of valid values\n",
    "            }\n",
    "\n",
    "    # Generate markdown files\n",
    "    for symbol, result in results.items():\n",
    "        company_name = result['company_name']\n",
    "\n",
    "        # Generate rank analysis markdown\n",
    "        rank_analysis_md = generate_rank_analysis_markdown(\n",
    "            result['data'],\n",
    "            symbol,\n",
    "            one_year_rank=rank_data['one_year']['ranks'].get(symbol),\n",
    "            six_month_rank=rank_data['six_month']['ranks'].get(symbol),\n",
    "            total_stocks=len(results)\n",
    "        )\n",
    "\n",
    "        # Create comprehensive markdown content\n",
    "        md_content = f\"# {company_name} ({symbol}) Analysis\\n\\n\"\n",
    "\n",
    "        md_content += f\"Company Name (ticker): {symbol}\\n\\n\"\n",
    "\n",
    "        # Extract the category/universe from the output_dir path\n",
    "        category = output_dir.split('/')[-1]  # This will extract \"nifty_50\", \"midcap_150\", etc.\n",
    "        # Format the category for display - remove underscore and capitalize\n",
    "        display_category = category.replace('_', ' ').title()\n",
    "        md_content += f\"Universe/Category: {display_category}\\n\\n\"\n",
    "\n",
    "        # Add actual date used for analysis\n",
    "        if end_date and result['actual_end_date'] != end_date:\n",
    "            md_content += f\"*Note: Analysis uses last available date {result['actual_end_date']} instead of requested date {end_date}*\\n\\n\"\n",
    "\n",
    "        # Current Price and Performance\n",
    "        md_content += \"## Current Price and Performance\\n\\n\"\n",
    "        md_content += f\"Current Price: Rs.{result['price_data']['Current']['price']:.2f}\\n\"\n",
    "        if '1_year_back' in result['price_data']:\n",
    "            md_content += f\"1-Year Change: {result['price_data']['1_year_back']['price_change_percent']:.2f}%\\n\"\n",
    "        if '6_months_back' in result['price_data']:\n",
    "            md_content += f\"6-Month Change: {result['price_data']['6_months_back']['price_change_percent']:.2f}%\\n\\n\"\n",
    "\n",
    "        # Technical Indicators\n",
    "        md_content += \"## Technical Indicators\\n\\n\"\n",
    "        if result['technical_indicators']['rsi'] is not None:\n",
    "            md_content += f\"RSI (14-day): {float(result['technical_indicators']['rsi']):.2f}\\n\"\n",
    "        if result['technical_indicators']['macd']['macd_line'] is not None:\n",
    "            md_content += f\"MACD Line: {float(result['technical_indicators']['macd']['macd_line']):.2f}\\n\"\n",
    "            md_content += f\"Signal Line: {float(result['technical_indicators']['macd']['signal_line']):.2f}\\n\"\n",
    "            md_content += f\"MACD Histogram: {float(result['technical_indicators']['macd']['histogram']):.2f}\\n\"\n",
    "\n",
    "        # Momentum Rankings and Universe Statistics\n",
    "        md_content += \"\\n## Momentum Rankings and Universe Statistics\\n\\n\"\n",
    "\n",
    "        # 1-Year Stats\n",
    "        md_content += \"### 1-Year Momentum\\n\"\n",
    "        one_year_rank = rank_data['one_year']['ranks'].get(symbol)\n",
    "        one_year_momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios']['one_year']\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "\n",
    "        if one_year_rank and one_year_momentum:\n",
    "            md_content += f\"- Rank: {one_year_rank} out of {len(results)}\\n\"\n",
    "            md_content += f\"- Momentum Ratio: {one_year_momentum:.4f}\\n\"\n",
    "            md_content += f\"- Universe Mean: {rank_data['one_year']['universe_stats']['mean']:.4f}\\n\"\n",
    "            md_content += f\"- Universe StDev: {rank_data['one_year']['universe_stats']['stdev']:.4f}\\n\"\n",
    "            if one_year_z_score is not None:\n",
    "                md_content += f\"- Z-Score: {one_year_z_score:.4f}\\n\"\n",
    "\n",
    "        # 6-Month Stats\n",
    "        md_content += \"\\n### 6-Month Momentum\\n\"\n",
    "        six_month_rank = rank_data['six_month']['ranks'].get(symbol)\n",
    "        six_month_momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios']['six_month']\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "\n",
    "        if six_month_rank and six_month_momentum:\n",
    "            md_content += f\"- Rank: {six_month_rank} out of {len(results)}\\n\"\n",
    "            md_content += f\"- Momentum Ratio: {six_month_momentum:.4f}\\n\"\n",
    "            md_content += f\"- Universe Mean: {rank_data['six_month']['universe_stats']['mean']:.4f}\\n\"\n",
    "            md_content += f\"- Universe StDev: {rank_data['six_month']['universe_stats']['stdev']:.4f}\\n\"\n",
    "            if six_month_z_score is not None:\n",
    "                md_content += f\"- Z-Score: {six_month_z_score:.4f}\\n\"\n",
    "\n",
    "        # Add dedicated Rank and Z-Score section\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "        weighted_z_score_data = result['technical_indicators']['momentum']['momentum_ratio'].get('weighted_z_score', {})\n",
    "        normalized_z_score_data = result['technical_indicators']['momentum']['momentum_ratio'].get('normalized_z_score', {})\n",
    "\n",
    "        md_content += \"\\n## Rank and Z-Score Analysis\\n\\n\"\n",
    "        md_content += \"### Z-Scores\\n\"\n",
    "        if one_year_z_score is not None:\n",
    "            md_content += f\"- 1-Year Z-Score: {one_year_z_score:.4f}\\n\"\n",
    "        if six_month_z_score is not None:\n",
    "            md_content += f\"- 6-Month Z-Score: {six_month_z_score:.4f}\\n\"\n",
    "        if weighted_z_score_data.get('score') is not None:\n",
    "            md_content += f\"- Weighted Z-Score: {weighted_z_score_data['score']:.4f}\\n\"\n",
    "            md_content += \"  *(Calculated as: 1-Year Z-Score × 0.5 + 6-Month Z-Score × 0.5)*\\n\"\n",
    "        if normalized_z_score_data.get('score') is not None:\n",
    "            md_content += f\"- Normalized Z-Score: {normalized_z_score_data['score']:.4f}\\n\"\n",
    "            md_content += \"  *(Calculated as: IF(weighted_z_score>0, (1+weighted_z_score), (1-weighted_z_score)^-1))*\\n\"\n",
    "\n",
    "        md_content += \"\\n### Rankings\\n\"\n",
    "        if one_year_rank:\n",
    "            md_content += f\"- 1-Year Momentum Rank: {one_year_rank} out of {len(results)}\\n\"\n",
    "        if six_month_rank:\n",
    "            md_content += f\"- 6-Month Momentum Rank: {six_month_rank} out of {len(results)}\\n\"\n",
    "        if weighted_z_score_data.get('rank') is not None:\n",
    "            md_content += f\"- Weighted Z-Score Rank: {weighted_z_score_data['rank']} out of {weighted_z_score_data['total_stocks']}\\n\"\n",
    "        if normalized_z_score_data.get('rank') is not None:\n",
    "            md_content += f\"- Normalized Z-Score Rank: {normalized_z_score_data['rank']} out of {normalized_z_score_data['total_stocks']}\\n\"\n",
    "\n",
    "        # Add universe statistics only if they exist\n",
    "        md_content += \"\\n### Universe Statistics\\n\"\n",
    "        md_content += \"**1-Year Momentum:**\\n\"\n",
    "        if rank_data['one_year']['universe_stats']['mean'] is not None and not np.isnan(rank_data['one_year']['universe_stats']['mean']):\n",
    "            md_content += f\"- Mean: {rank_data['one_year']['universe_stats']['mean']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Mean: Not available\\n\"\n",
    "\n",
    "        if rank_data['one_year']['universe_stats']['stdev'] is not None and not np.isnan(rank_data['one_year']['universe_stats']['stdev']):\n",
    "            md_content += f\"- Standard Deviation: {rank_data['one_year']['universe_stats']['stdev']:.4f}\\n\\n\"\n",
    "        else:\n",
    "            md_content += \"- Standard Deviation: Not available\\n\\n\"\n",
    "\n",
    "        md_content += \"**6-Month Momentum:**\\n\"\n",
    "        if rank_data['six_month']['universe_stats']['mean'] is not None and not np.isnan(rank_data['six_month']['universe_stats']['mean']):\n",
    "            md_content += f\"- Mean: {rank_data['six_month']['universe_stats']['mean']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Mean: Not available\\n\"\n",
    "\n",
    "        if rank_data['six_month']['universe_stats']['stdev'] is not None and not np.isnan(rank_data['six_month']['universe_stats']['stdev']):\n",
    "            md_content += f\"- Standard Deviation: {rank_data['six_month']['universe_stats']['stdev']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Standard Deviation: Not available\\n\"\n",
    "\n",
    "        # Strengths and Weaknesses\n",
    "        md_content += \"\\n## Analysis\\n\\n\"\n",
    "        md_content += \"### Strengths\\n\"\n",
    "        for strength in result['analysis']['strengths']:\n",
    "            md_content += f\"- {strength}\\n\"\n",
    "\n",
    "        md_content += \"\\n### Weaknesses\\n\"\n",
    "        for weakness in result['analysis']['weaknesses']:\n",
    "            md_content += f\"- {weakness}\\n\"\n",
    "\n",
    "        # Add rank analysis content\n",
    "        md_content += \"\\n\" + rank_analysis_md\n",
    "\n",
    "        # Save markdown file with all analysis\n",
    "        analysis_file = os.path.join(output_dir, f\"{symbol}.md\")\n",
    "        with file_lock:\n",
    "            with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(md_content)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stock_list(stock_list_text):\n",
    "    stocks = []\n",
    "    lines = stock_list_text.strip().split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        if '\\t' in line:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                company_name = parts[0].strip()\n",
    "                symbol = parts[1].strip()\n",
    "                if company_name not in ['Company Name', ''] and symbol not in ['Symbol', '']:\n",
    "                    stocks.append((company_name, symbol))\n",
    "\n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "stock_list_nifty = \"\"\"Company Name\tSymbol\n",
    "Adani Enterprises Ltd.\tADANIENT\n",
    "Adani Ports and Special Economic Zone Ltd.\tADANIPORTS\n",
    "Apollo Hospitals Enterprise Ltd.\tAPOLLOHOSP\n",
    "Asian Paints Ltd.\tASIANPAINT\n",
    "Axis Bank Ltd.\tAXISBANK\n",
    "Bajaj Auto Ltd.\tBAJAJ-AUTO\n",
    "Bajaj Finance Ltd.\tBAJFINANCE\n",
    "Bajaj Finserv Ltd.\tBAJAJFINSV\n",
    "Bharat Electronics Ltd.\tBEL\n",
    "Bharti Airtel Ltd.\tBHARTIARTL\n",
    "Cipla Ltd.\tCIPLA\n",
    "Coal India Ltd.\tCOALINDIA\n",
    "Dr. Reddy's Laboratories Ltd.\tDRREDDY\n",
    "Eicher Motors Ltd.\tEICHERMOT\n",
    "Grasim Industries Ltd.\tGRASIM\n",
    "HCL Technologies Ltd.\tHCLTECH\n",
    "HDFC Bank Ltd.\tHDFCBANK\n",
    "HDFC Life Insurance Company Ltd.\tHDFCLIFE\n",
    "Hero MotoCorp Ltd.\tHEROMOTOCO\n",
    "Hindalco Industries Ltd.\tHINDALCO\n",
    "Hindustan Unilever Ltd.\tHINDUNILVR\n",
    "ICICI Bank Ltd.\tICICIBANK\n",
    "ITC Ltd.\tITC\n",
    "IndusInd Bank Ltd.\tINDUSINDBK\n",
    "Infosys Ltd.\tINFY\n",
    "JSW Steel Ltd.\tJSWSTEEL\n",
    "Jio Financial Services Ltd.\tJIOFIN\n",
    "Kotak Mahindra Bank Ltd.\tKOTAKBANK\n",
    "Larsen & Toubro Ltd.\tLT\n",
    "Mahindra & Mahindra Ltd.\tM&M\n",
    "Maruti Suzuki India Ltd.\tMARUTI\n",
    "NTPC Ltd.\tNTPC\n",
    "Nestle India Ltd.\tNESTLEIND\n",
    "Oil & Natural Gas Corporation Ltd.\tONGC\n",
    "Power Grid Corporation of India Ltd.\tPOWERGRID\n",
    "Reliance Industries Ltd.\tRELIANCE\n",
    "SBI Life Insurance Company Ltd.\tSBILIFE\n",
    "Shriram Finance Ltd.\tSHRIRAMFIN\n",
    "State Bank of India\tSBIN\n",
    "Sun Pharmaceutical Industries Ltd.\tSUNPHARMA\n",
    "Tata Consultancy Services Ltd.\tTCS\n",
    "Tata Consumer Products Ltd.\tTATACONSUM\n",
    "Tata Motors Ltd.\tTATAMOTORS\n",
    "Tata Steel Ltd.\tTATASTEEL\n",
    "Tech Mahindra Ltd.\tTECHM\n",
    "Titan Company Ltd.\tTITAN\n",
    "Trent Ltd.\tTRENT\n",
    "UltraTech Cement Ltd.\tULTRACEMCO\n",
    "Wipro Ltd.\tWIPRO\n",
    "Zomato Ltd.\tZOMATO\"\"\"\n",
    "\n",
    "stocks_nifty = parse_stock_list(stock_list_nifty)\n",
    "results_nifty50 = analyze_stocks(stocks_nifty, output_dir=\"stock_analysis/reports_v2/nifty_50\", max_workers=10, end_date=\"2025-04-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_midcap = \"\"\"Company Name\tSymbol\n",
    "360 ONE WAM Ltd.\t360ONE\n",
    "3M India Ltd.\t3MINDIA\n",
    "ACC Ltd.\tACC\n",
    "AIA Engineering Ltd.\tAIAENG\n",
    "APL Apollo Tubes Ltd.\tAPLAPOLLO\n",
    "AU Small Finance Bank Ltd.\tAUBANK\n",
    "Abbott India Ltd.\tABBOTINDIA\n",
    "Adani Total Gas Ltd.\tATGL\n",
    "Adani Wilmar Ltd.\tAWL\n",
    "Aditya Birla Capital Ltd.\tABCAPITAL\n",
    "Aditya Birla Fashion and Retail Ltd.\tABFRL\n",
    "Ajanta Pharmaceuticals Ltd.\tAJANTPHARM\n",
    "Alkem Laboratories Ltd.\tALKEM\n",
    "Apar Industries Ltd.\tAPARINDS\n",
    "Apollo Tyres Ltd.\tAPOLLOTYRE\n",
    "Ashok Leyland Ltd.\tASHOKLEY\n",
    "Astral Ltd.\tASTRAL\n",
    "Aurobindo Pharma Ltd.\tAUROPHARMA\n",
    "BSE Ltd.\tBSE\n",
    "Balkrishna Industries Ltd.\tBALKRISIND\n",
    "Bandhan Bank Ltd.\tBANDHANBNK\n",
    "Bank of India\tBANKINDIA\n",
    "Bank of Maharashtra\tMAHABANK\n",
    "Berger Paints India Ltd.\tBERGEPAINT\n",
    "Bharat Dynamics Ltd.\tBDL\n",
    "Bharat Forge Ltd.\tBHARATFORG\n",
    "Bharat Heavy Electricals Ltd.\tBHEL\n",
    "Bharti Hexacom Ltd.\tBHARTIHEXA\n",
    "Biocon Ltd.\tBIOCON\n",
    "Blue Star Ltd.\tBLUESTARCO\n",
    "CRISIL Ltd.\tCRISIL\n",
    "Cochin Shipyard Ltd.\tCOCHINSHIP\n",
    "Coforge Ltd.\tCOFORGE\n",
    "Colgate Palmolive (India) Ltd.\tCOLPAL\n",
    "Container Corporation of India Ltd.\tCONCOR\n",
    "Coromandel International Ltd.\tCOROMANDEL\n",
    "Cummins India Ltd.\tCUMMINSIND\n",
    "Dalmia Bharat Ltd.\tDALBHARAT\n",
    "Deepak Nitrite Ltd.\tDEEPAKNTR\n",
    "Dixon Technologies (India) Ltd.\tDIXON\n",
    "Emami Ltd.\tEMAMILTD\n",
    "Endurance Technologies Ltd.\tENDURANCE\n",
    "Escorts Kubota Ltd.\tESCORTS\n",
    "Exide Industries Ltd.\tEXIDEIND\n",
    "FSN E-Commerce Ventures Ltd.\tNYKAA\n",
    "Federal Bank Ltd.\tFEDERALBNK\n",
    "Fortis Healthcare Ltd.\tFORTIS\n",
    "GE Vernova T&D India Ltd.\tGVT&D\n",
    "GMR Airports Ltd.\tGMRAIRPORT\n",
    "General Insurance Corporation of India\tGICRE\n",
    "Gland Pharma Ltd.\tGLAND\n",
    "Glaxosmithkline Pharmaceuticals Ltd.\tGLAXO\n",
    "Glenmark Pharmaceuticals Ltd.\tGLENMARK\n",
    "Global Health Ltd.\tMEDANTA\n",
    "Godrej Industries Ltd.\tGODREJIND\n",
    "Godrej Properties Ltd.\tGODREJPROP\n",
    "Gujarat Fluorochemicals Ltd.\tFLUOROCHEM\n",
    "Gujarat Gas Ltd.\tGUJGASLTD\n",
    "HDFC Asset Management Company Ltd.\tHDFCAMC\n",
    "Hindustan Petroleum Corporation Ltd.\tHINDPETRO\n",
    "Hindustan Zinc Ltd.\tHINDZINC\n",
    "Hitachi Energy India Ltd.\tPOWERINDIA\n",
    "Honeywell Automation India Ltd.\tHONAUT\n",
    "Housing & Urban Development Corporation Ltd.\tHUDCO\n",
    "IDFC First Bank Ltd.\tIDFCFIRSTB\n",
    "IRB Infrastructure Developers Ltd.\tIRB\n",
    "Indian Bank\tINDIANB\n",
    "Indian Railway Catering And Tourism Corporation Ltd.\tIRCTC\n",
    "Indian Renewable Energy Development Agency Ltd.\tIREDA\n",
    "Indraprastha Gas Ltd.\tIGL\n",
    "Indus Towers Ltd.\tINDUSTOWER\n",
    "Ipca Laboratories Ltd.\tIPCALAB\n",
    "J.K. Cement Ltd.\tJKCEMENT\n",
    "JSW Infrastructure Ltd.\tJSWINFRA\n",
    "Jindal Stainless Ltd.\tJSL\n",
    "Jubilant Foodworks Ltd.\tJUBLFOOD\n",
    "K.P.R. Mill Ltd.\tKPRMILL\n",
    "KEI Industries Ltd.\tKEI\n",
    "KPIT Technologies Ltd.\tKPITTECH\n",
    "Kalyan Jewellers India Ltd.\tKALYANKJIL\n",
    "L&T Finance Ltd.\tLTF\n",
    "L&T Technology Services Ltd.\tLTTS\n",
    "LIC Housing Finance Ltd.\tLICHSGFIN\n",
    "Linde India Ltd.\tLINDEINDIA\n",
    "Lloyds Metals And Energy Ltd.\tLLOYDSME\n",
    "Lupin Ltd.\tLUPIN\n",
    "MRF Ltd.\tMRF\n",
    "Mahindra & Mahindra Financial Services Ltd.\tM&MFIN\n",
    "Mangalore Refinery & Petrochemicals Ltd.\tMRPL\n",
    "Mankind Pharma Ltd.\tMANKIND\n",
    "Marico Ltd.\tMARICO\n",
    "Max Financial Services Ltd.\tMFSL\n",
    "Max Healthcare Institute Ltd.\tMAXHEALTH\n",
    "Mazagoan Dock Shipbuilders Ltd.\tMAZDOCK\n",
    "Motherson Sumi Wiring India Ltd.\tMSUMI\n",
    "Motilal Oswal Financial Services Ltd.\tMOTILALOFS\n",
    "MphasiS Ltd.\tMPHASIS\n",
    "Muthoot Finance Ltd.\tMUTHOOTFIN\n",
    "NHPC Ltd.\tNHPC\n",
    "NLC India Ltd.\tNLCINDIA\n",
    "NMDC Ltd.\tNMDC\n",
    "NTPC Green Energy Ltd.\tNTPCGREEN\n",
    "National Aluminium Co. Ltd.\tNATIONALUM\n",
    "Nippon Life India Asset Management Ltd.\tNAM-INDIA\n",
    "Oberoi Realty Ltd.\tOBEROIRLTY\n",
    "Oil India Ltd.\tOIL\n",
    "Ola Electric Mobility Ltd.\tOLAELEC\n",
    "One 97 Communications Ltd.\tPAYTM\n",
    "Oracle Financial Services Software Ltd.\tOFSS\n",
    "PB Fintech Ltd.\tPOLICYBZR\n",
    "PI Industries Ltd.\tPIIND\n",
    "Page Industries Ltd.\tPAGEIND\n",
    "Patanjali Foods Ltd.\tPATANJALI\n",
    "Persistent Systems Ltd.\tPERSISTENT\n",
    "Petronet LNG Ltd.\tPETRONET\n",
    "Phoenix Mills Ltd.\tPHOENIXLTD\n",
    "Polycab India Ltd.\tPOLYCAB\n",
    "Premier Energies Ltd.\tPREMIERENE\n",
    "Prestige Estates Projects Ltd.\tPRESTIGE\n",
    "Rail Vikas Nigam Ltd.\tRVNL\n",
    "SBI Cards and Payment Services Ltd.\tSBICARD\n",
    "SJVN Ltd.\tSJVN\n",
    "SRF Ltd.\tSRF\n",
    "Schaeffler India Ltd.\tSCHAEFFLER\n",
    "Solar Industries India Ltd.\tSOLARINDS\n",
    "Sona BLW Precision Forgings Ltd.\tSONACOMS\n",
    "Star Health and Allied Insurance Company Ltd.\tSTARHEALTH\n",
    "Steel Authority of India Ltd.\tSAIL\n",
    "Sun TV Network Ltd.\tSUNTV\n",
    "Sundaram Finance Ltd.\tSUNDARMFIN\n",
    "Supreme Industries Ltd.\tSUPREMEIND\n",
    "Suzlon Energy Ltd.\tSUZLON\n",
    "Syngene International Ltd.\tSYNGENE\n",
    "Tata Communications Ltd.\tTATACOMM\n",
    "Tata Elxsi Ltd.\tTATAELXSI\n",
    "Tata Investment Corporation Ltd.\tTATAINVEST\n",
    "Tata Technologies Ltd.\tTATATECH\n",
    "The New India Assurance Company Ltd.\tNIACL\n",
    "Thermax Ltd.\tTHERMAX\n",
    "Torrent Power Ltd.\tTORNTPOWER\n",
    "Tube Investments of India Ltd.\tTIINDIA\n",
    "UNO Minda Ltd.\tUNOMINDA\n",
    "UPL Ltd.\tUPL\n",
    "Union Bank of India\tUNIONBANK\n",
    "United Breweries Ltd.\tUBL\n",
    "Vishal Mega Mart Ltd.\tVMM\n",
    "Vodafone Idea Ltd.\tIDEA\n",
    "Voltas Ltd.\tVOLTAS\n",
    "Waaree Energies Ltd.\tWAAREEENER\n",
    "Yes Bank Ltd.\tYESBANK\"\"\"\n",
    "\n",
    "\n",
    "stocks_midcap = parse_stock_list(stock_list_midcap)\n",
    "results_midcap = analyze_stocks(stocks_midcap, output_dir=\"stock_analysis/reports_v2/midcap_150\", max_workers=10, end_date=\"2025-04-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_smallcap = \"\"\"Company Name\tSymbol\n",
    "ACME Solar Holdings Ltd.\tACMESOLAR\n",
    "Aadhar Housing Finance Ltd.\tAADHARHFC\n",
    "Aarti Industries Ltd.\tAARTIIND\n",
    "Aavas Financiers Ltd.\tAAVAS\n",
    "Action Construction Equipment Ltd.\tACE\n",
    "Aditya Birla Real Estate Ltd.\tABREL\n",
    "Aditya Birla Sun Life AMC Ltd.\tABSLAMC\n",
    "Aegis Logistics Ltd.\tAEGISLOG\n",
    "Afcons Infrastructure Ltd.\tAFCONS\n",
    "Affle (India) Ltd.\tAFFLE\n",
    "Akums Drugs and Pharmaceuticals Ltd.\tAKUMS\n",
    "Alembic Pharmaceuticals Ltd.\tAPLLTD\n",
    "Alivus Life Sciences Ltd.\tALIVUS\n",
    "Alkyl Amines Chemicals Ltd.\tALKYLAMINE\n",
    "Alok Industries Ltd.\tALOKINDS\n",
    "Amara Raja Energy & Mobility Ltd.\tARE&M\n",
    "Amber Enterprises India Ltd.\tAMBER\n",
    "Anand Rathi Wealth Ltd.\tANANDRATHI\n",
    "Anant Raj Ltd.\tANANTRAJ\n",
    "Angel One Ltd.\tANGELONE\n",
    "Aptus Value Housing Finance India Ltd.\tAPTUS\n",
    "Asahi India Glass Ltd.\tASAHIINDIA\n",
    "Aster DM Healthcare Ltd.\tASTERDM\n",
    "AstraZenca Pharma India Ltd.\tASTRAZEN\n",
    "Atul Ltd.\tATUL\n",
    "Authum Investment & Infrastructure Ltd.\tAIIL\n",
    "BASF India Ltd.\tBASF\n",
    "BEML Ltd.\tBEML\n",
    "BLS International Services Ltd.\tBLS\n",
    "Balrampur Chini Mills Ltd.\tBALRAMCHIN\n",
    "Bata India Ltd.\tBATAINDIA\n",
    "Bayer Cropscience Ltd.\tBAYERCROP\n",
    "Bikaji Foods International Ltd.\tBIKAJI\n",
    "Birlasoft Ltd.\tBSOFT\n",
    "Blue Dart Express Ltd.\tBLUEDART\n",
    "Bombay Burmah Trading Corporation Ltd.\tBBTC\n",
    "Brainbees Solutions Ltd.\tFIRSTCRY\n",
    "Brigade Enterprises Ltd.\tBRIGADE\n",
    "C.E. Info Systems Ltd.\tMAPMYINDIA\n",
    "CCL Products (I) Ltd.\tCCL\n",
    "CESC Ltd.\tCESC\n",
    "Campus Activewear Ltd.\tCAMPUS\n",
    "Can Fin Homes Ltd.\tCANFINHOME\n",
    "Caplin Point Laboratories Ltd.\tCAPLIPOINT\n",
    "Capri Global Capital Ltd.\tCGCL\n",
    "Carborundum Universal Ltd.\tCARBORUNIV\n",
    "Castrol India Ltd.\tCASTROLIND\n",
    "Ceat Ltd.\tCEATLTD\n",
    "Central Bank of India\tCENTRALBK\n",
    "Central Depository Services (India) Ltd.\tCDSL\n",
    "Century Plyboards (India) Ltd.\tCENTURYPLY\n",
    "Cera Sanitaryware Ltd\tCERA\n",
    "Chalet Hotels Ltd.\tCHALET\n",
    "Chambal Fertilizers & Chemicals Ltd.\tCHAMBLFERT\n",
    "Chennai Petroleum Corporation Ltd.\tCHENNPETRO\n",
    "Cholamandalam Financial Holdings Ltd.\tCHOLAHLDNG\n",
    "City Union Bank Ltd.\tCUB\n",
    "Clean Science and Technology Ltd.\tCLEAN\n",
    "Computer Age Management Services Ltd.\tCAMS\n",
    "Concord Biotech Ltd.\tCONCORDBIO\n",
    "Craftsman Automation Ltd.\tCRAFTSMAN\n",
    "CreditAccess Grameen Ltd.\tCREDITACC\n",
    "Crompton Greaves Consumer Electricals Ltd.\tCROMPTON\n",
    "Cyient Ltd.\tCYIENT\n",
    "DCM Shriram Ltd.\tDCMSHRIRAM\n",
    "DOMS Industries Ltd.\tDOMS\n",
    "Data Patterns (India) Ltd.\tDATAPATTNS\n",
    "Deepak Fertilisers & Petrochemicals Corp. Ltd.\tDEEPAKFERT\n",
    "Delhivery Ltd.\tDELHIVERY\n",
    "Devyani International Ltd.\tDEVYANI\n",
    "Dr. Lal Path Labs Ltd.\tLALPATHLAB\n",
    "E.I.D. Parry (India) Ltd.\tEIDPARRY\n",
    "EIH Ltd.\tEIHOTEL\n",
    "Elecon Engineering Co. Ltd.\tELECON\n",
    "Elgi Equipments Ltd.\tELGIEQUIP\n",
    "Emcure Pharmaceuticals Ltd.\tEMCURE\n",
    "Engineers India Ltd.\tENGINERSIN\n",
    "Eris Lifesciences Ltd.\tERIS\n",
    "Fertilisers and Chemicals Travancore Ltd.\tFACT\n",
    "Finolex Cables Ltd.\tFINCABLES\n",
    "Finolex Industries Ltd.\tFINPIPE\n",
    "Firstsource Solutions Ltd.\tFSL\n",
    "Five-Star Business Finance Ltd.\tFIVESTAR\n",
    "Garden Reach Shipbuilders & Engineers Ltd.\tGRSE\n",
    "Gillette India Ltd.\tGILLETTE\n",
    "Go Digit General Insurance Ltd.\tGODIGIT\n",
    "Godawari Power & Ispat Ltd.\tGPIL\n",
    "Godfrey Phillips India Ltd.\tGODFRYPHLP\n",
    "Godrej Agrovet Ltd.\tGODREJAGRO\n",
    "Granules India Ltd.\tGRANULES\n",
    "Graphite India Ltd.\tGRAPHITE\n",
    "Gravita India Ltd.\tGRAVITA\n",
    "Great Eastern Shipping Co. Ltd.\tGESHIP\n",
    "Gujarat Mineral Development Corporation Ltd.\tGMDCLTD\n",
    "Gujarat Narmada Valley Fertilizers and Chemicals Ltd.\tGNFC\n",
    "Gujarat Pipavav Port Ltd.\tGPPL\n",
    "Gujarat State Petronet Ltd.\tGSPL\n",
    "H.E.G. Ltd.\tHEG\n",
    "HBL Engineering Ltd.\tHBLENGINE\n",
    "HFCL Ltd.\tHFCL\n",
    "Happiest Minds Technologies Ltd.\tHAPPSTMNDS\n",
    "Himadri Speciality Chemical Ltd.\tHSCL\n",
    "Hindustan Copper Ltd.\tHINDCOPPER\n",
    "Home First Finance Company India Ltd.\tHOMEFIRST\n",
    "Honasa Consumer Ltd.\tHONASA\n",
    "IDBI Bank Ltd.\tIDBI\n",
    "IFCI Ltd.\tIFCI\n",
    "IIFL Finance Ltd.\tIIFL\n",
    "INOX India Ltd.\tINOXINDIA\n",
    "IRCON International Ltd.\tIRCON\n",
    "ITI Ltd.\tITI\n",
    "Indegene Ltd.\tINDGN\n",
    "India Cements Ltd.\tINDIACEM\n",
    "Indiamart Intermesh Ltd.\tINDIAMART\n",
    "Indian Energy Exchange Ltd.\tIEX\n",
    "Indian Overseas Bank\tIOB\n",
    "Inox Wind Ltd.\tINOXWIND\n",
    "Intellect Design Arena Ltd.\tINTELLECT\n",
    "International Gemmological Institute (India) Ltd.\tIGIL\n",
    "Inventurus Knowledge Solutions Ltd.\tIKS\n",
    "J.B. Chemicals & Pharmaceuticals Ltd.\tJBCHEPHARM\n",
    "JBM Auto Ltd.\tJBMA\n",
    "JK Tyre & Industries Ltd.\tJKTYRE\n",
    "JM Financial Ltd.\tJMFINANCIL\n",
    "JSW Holdings Ltd.\tJSWHL\n",
    "Jaiprakash Power Ventures Ltd.\tJPPOWER\n",
    "Jammu & Kashmir Bank Ltd.\tJ&KBANK\n",
    "Jindal Saw Ltd.\tJINDALSAW\n",
    "Jubilant Ingrevia Ltd.\tJUBLINGREA\n",
    "Jubilant Pharmova Ltd.\tJUBLPHARMA\n",
    "Jupiter Wagons Ltd.\tJWL\n",
    "Justdial Ltd.\tJUSTDIAL\n",
    "Jyothy Labs Ltd.\tJYOTHYLAB\n",
    "Jyoti CNC Automation Ltd.\tJYOTICNC\n",
    "KNR Constructions Ltd.\tKNRCON\n",
    "Kajaria Ceramics Ltd.\tKAJARIACER\n",
    "Kalpataru Projects International Ltd.\tKPIL\n",
    "Kansai Nerolac Paints Ltd.\tKANSAINER\n",
    "Karur Vysya Bank Ltd.\tKARURVYSYA\n",
    "Kaynes Technology India Ltd.\tKAYNES\n",
    "Kec International Ltd.\tKEC\n",
    "Kfin Technologies Ltd.\tKFINTECH\n",
    "Kirloskar Brothers Ltd.\tKIRLOSBROS\n",
    "Kirloskar Oil Eng Ltd.\tKIRLOSENG\n",
    "Krishna Institute of Medical Sciences Ltd.\tKIMS\n",
    "LT Foods Ltd.\tLTFOODS\n",
    "Latent View Analytics Ltd.\tLATENTVIEW\n",
    "Laurus Labs Ltd.\tLAURUSLABS\n",
    "Lemon Tree Hotels Ltd.\tLEMONTREE\n",
    "MMTC Ltd.\tMMTC\n",
    "Mahanagar Gas Ltd.\tMGL\n",
    "Maharashtra Seamless Ltd.\tMAHSEAMLES\n",
    "Manappuram Finance Ltd.\tMANAPPURAM\n",
    "Mastek Ltd.\tMASTEK\n",
    "Metropolis Healthcare Ltd.\tMETROPOLIS\n",
    "Minda Corporation Ltd.\tMINDACORP\n",
    "Multi Commodity Exchange of India Ltd.\tMCX\n",
    "NATCO Pharma Ltd.\tNATCOPHARM\n",
    "NBCC (India) Ltd.\tNBCC\n",
    "NCC Ltd.\tNCC\n",
    "NMDC Steel Ltd.\tNSLNISP\n",
    "Narayana Hrudayalaya Ltd.\tNH\n",
    "Nava Ltd.\tNAVA\n",
    "Navin Fluorine International Ltd.\tNAVINFLUOR\n",
    "Netweb Technologies India Ltd.\tNETWEB\n",
    "Network18 Media & Investments Ltd.\tNETWORK18\n",
    "Neuland Laboratories Ltd.\tNEULANDLAB\n",
    "Newgen Software Technologies Ltd.\tNEWGEN\n",
    "Niva Bupa Health Insurance Company Ltd.\tNIVABUPA\n",
    "Nuvama Wealth Management Ltd.\tNUVAMA\n",
    "Olectra Greentech Ltd.\tOLECTRA\n",
    "PCBL Chemical Ltd.\tPCBL\n",
    "PG Electroplast Ltd.\tPGEL\n",
    "PNB Housing Finance Ltd.\tPNBHOUSING\n",
    "PNC Infratech Ltd.\tPNCINFRA\n",
    "PTC Industries Ltd.\tPTCIL\n",
    "PVR INOX Ltd.\tPVRINOX\n",
    "Pfizer Ltd.\tPFIZER\n",
    "Piramal Enterprises Ltd.\tPEL\n",
    "Piramal Pharma Ltd.\tPPLPHARMA\n",
    "Poly Medicure Ltd.\tPOLYMED\n",
    "Poonawalla Fincorp Ltd.\tPOONAWALLA\n",
    "Praj Industries Ltd.\tPRAJIND\n",
    "Quess Corp Ltd.\tQUESS\n",
    "R R Kabel Ltd.\tRRKABEL\n",
    "RBL Bank Ltd.\tRBLBANK\n",
    "RHI MAGNESITA INDIA LTD.\tRHIM\n",
    "RITES Ltd.\tRITES\n",
    "Radico Khaitan Ltd\tRADICO\n",
    "Railtel Corporation Of India Ltd.\tRAILTEL\n",
    "Rainbow Childrens Medicare Ltd.\tRAINBOW\n",
    "Ramkrishna Forgings Ltd.\tRKFORGE\n",
    "Rashtriya Chemicals & Fertilizers Ltd.\tRCF\n",
    "RattanIndia Enterprises Ltd.\tRTNINDIA\n",
    "Raymond Lifestyle Ltd.\tRAYMONDLSL\n",
    "Raymond Ltd.\tRAYMOND\n",
    "Redington Ltd.\tREDINGTON\n",
    "Reliance Power Ltd.\tRPOWER\n",
    "Route Mobile Ltd.\tROUTE\n",
    "SBFC Finance Ltd.\tSBFC\n",
    "SKF India Ltd.\tSKFINDIA\n",
    "Sagility India Ltd.\tSAGILITY\n",
    "Sai Life Sciences Ltd.\tSAILIFE\n",
    "Sammaan Capital Ltd.\tSAMMAANCAP\n",
    "Sapphire Foods India Ltd.\tSAPPHIRE\n",
    "Sarda Energy and Minerals Ltd.\tSARDAEN\n",
    "Saregama India Ltd\tSAREGAMA\n",
    "Schneider Electric Infrastructure Ltd.\tSCHNEIDER\n",
    "Shipping Corporation of India Ltd.\tSCI\n",
    "Shree Renuka Sugars Ltd.\tRENUKA\n",
    "Shyam Metalics and Energy Ltd.\tSHYAMMETL\n",
    "Signatureglobal (India) Ltd.\tSIGNATURE\n",
    "Sobha Ltd.\tSOBHA\n",
    "Sonata Software Ltd.\tSONATSOFTW\n",
    "Sterling and Wilson Renewable Energy Ltd.\tSWSOLAR\n",
    "Sumitomo Chemical India Ltd.\tSUMICHEM\n",
    "Suven Pharmaceuticals Ltd.\tSUVENPHAR\n",
    "Swan Energy Ltd.\tSWANENERGY\n",
    "Syrma SGS Technology Ltd.\tSYRMA\n",
    "TBO Tek Ltd.\tTBOTEK\n",
    "Tanla Platforms Ltd.\tTANLA\n",
    "Tata Chemicals Ltd.\tTATACHEM\n",
    "Tata Teleservices (Maharashtra) Ltd.\tTTML\n",
    "Techno Electric & Engineering Company Ltd.\tTECHNOE\n",
    "Tejas Networks Ltd.\tTEJASNET\n",
    "The Ramco Cements Ltd.\tRAMCOCEM\n",
    "Timken India Ltd.\tTIMKEN\n",
    "Titagarh Rail Systems Ltd.\tTITAGARH\n",
    "Transformers And Rectifiers (India) Ltd.\tTARIL\n",
    "Trident Ltd.\tTRIDENT\n",
    "Triveni Engineering & Industries Ltd.\tTRIVENI\n",
    "Triveni Turbine Ltd.\tTRITURBINE\n",
    "UCO Bank\tUCOBANK\n",
    "UTI Asset Management Company Ltd.\tUTIAMC\n",
    "Usha Martin Ltd.\tUSHAMART\n",
    "V-Guard Industries Ltd.\tVGUARD\n",
    "Valor Estate Ltd.\tDBREALTY\n",
    "Vardhman Textiles Ltd.\tVTL\n",
    "Vedant Fashions Ltd.\tMANYAVAR\n",
    "Vijaya Diagnostic Centre Ltd.\tVIJAYA\n",
    "Welspun Corp Ltd.\tWELCORP\n",
    "Welspun Living Ltd.\tWELSPUNLIV\n",
    "Westlife Foodworld Ltd.\tWESTLIFE\n",
    "Whirlpool of India Ltd.\tWHIRLPOOL\n",
    "Wockhardt Ltd.\tWOCKPHARMA\n",
    "ZF Commercial Vehicle Control Systems India Ltd.\tZFCVINDIA\n",
    "Zee Entertainment Enterprises Ltd.\tZEEL\n",
    "Zen Technologies Ltd.\tZENTEC\n",
    "Zensar Technolgies Ltd.\tZENSARTECH\n",
    "eClerx Services Ltd.\tECLERX\"\"\"\n",
    "\n",
    "\n",
    "stocks_smallcap = parse_stock_list(stock_list_smallcap)\n",
    "results_smallcap = analyze_stocks(stocks_smallcap, output_dir=\"stock_analysis/reports_v2/smallcap_250\", max_workers=10, end_date=\"2025-04-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_microcap = \"\"\"Company Name\tSymbol\n",
    "AGI Greenpac Ltd.\tAGI\n",
    "ASK Automotive Ltd.\tASKAUTOLTD\n",
    "Aarti Drugs Ltd.\tAARTIDRUGS\n",
    "Aarti Pharmalabs Ltd.\tAARTIPHARM\n",
    "Aditya Vision Ltd.\tAVL\n",
    "Advanced Enzyme Tech Ltd.\tADVENZYMES\n",
    "Aether Industries Ltd.\tAETHER\n",
    "Ahluwalia Contracts (India) Ltd.\tAHLUCONT\n",
    "Akzo Nobel India Ltd.\tAKZOINDIA\n",
    "Allcargo Logistics Ltd.\tALLCARGO\n",
    "Allied Blenders and Distillers Ltd.\tABDL\n",
    "Ami Organics Ltd.\tAMIORG\n",
    "Apeejay Surrendra Park Hotels Ltd.\tPARKHOTELS\n",
    "Archean Chemical Industries Ltd.\tACI\n",
    "Arvind Fashions Ltd.\tARVINDFASN\n",
    "Arvind Ltd.\tARVIND\n",
    "Ashoka Buildcon Ltd.\tASHOKA\n",
    "Astra Microwave Products Ltd.\tASTRAMICRO\n",
    "Aurionpro Solution Ltd.\tAURIONPRO\n",
    "Avalon Technologies Ltd.\tAVALON\n",
    "Avanti Feeds Ltd.\tAVANTIFEED\n",
    "Awfis Space Solutions Ltd.\tAWFIS\n",
    "Azad Engineering Ltd.\tAZAD\n",
    "Bajaj Hindusthan Sugar Ltd.\tBAJAJHIND\n",
    "Balaji Amines Ltd.\tBALAMINES\n",
    "Balu Forge Industries Ltd.\tBALUFORGE\n",
    "Banco Products (India) Ltd.\tBANCOINDIA\n",
    "Bansal Wire Industries Ltd.\tBANSALWIRE\n",
    "Bhansali Engineering Polymers Ltd.\tBEPL\n",
    "Bharat Bijlee Ltd.\tBBL\n",
    "Birla Corporation Ltd.\tBIRLACORPN\n",
    "Blue Jet Healthcare Ltd.\tBLUEJET\n",
    "Bombay Dyeing & Manufacturing Co. Ltd.\tBOMDYEING\n",
    "Borosil Ltd.\tBOROLTD\n",
    "Borosil Renewables Ltd.\tBORORENEW\n",
    "CIE Automotive India Ltd.\tCIEINDIA\n",
    "CMS Info Systems Ltd.\tCMSINFO\n",
    "CSB Bank Ltd.\tCSBBANK\n",
    "Cartrade Tech Ltd.\tCARTRADE\n",
    "Ceigall India Ltd.\tCEIGALL\n",
    "Cello World Ltd.\tCELLO\n",
    "Chemplast Sanmar Ltd.\tCHEMPLASTS\n",
    "Choice International Ltd.\tCHOICEIN\n",
    "Cigniti Technologies Ltd.\tCIGNITITEC\n",
    "Cyient DLM Ltd.\tCYIENTDLM\n",
    "DCB Bank Ltd.\tDCBBANK\n",
    "DCX Systems Ltd.\tDCXINDIA\n",
    "Datamatics Global Services Ltd.\tDATAMATICS\n",
    "Dhani Services Ltd.\tDHANI\n",
    "Dilip Buildcon Ltd.\tDBL\n",
    "Dishman Carbogen Amcis Ltd.\tDCAL\n",
    "Dodla Dairy Ltd.\tDODLA\n",
    "Dynamatic Technologies Ltd.\tDYNAMATECH\n",
    "EPL Ltd.\tEPL\n",
    "Easy Trip Planners Ltd.\tEASEMYTRIP\n",
    "Edelweiss Financial Services Ltd.\tEDELWEISS\n",
    "Electronics Mart India Ltd.\tEMIL\n",
    "Electrosteel Castings Ltd.\tELECTCAST\n",
    "Embassy Developments Ltd.\tEMBDL\n",
    "Entero Healthcare Solutions Ltd.\tENTERO\n",
    "Enviro Infra Engineers Ltd.\tEIEL\n",
    "Epigral Ltd.\tEPIGRAL\n",
    "Equitas Small Finance Bank Ltd.\tEQUITASBNK\n",
    "Ethos Ltd.\tETHOSLTD\n",
    "Eureka Forbes Ltd.\tEUREKAFORB\n",
    "FDC Ltd.\tFDC\n",
    "Fiem Industries Ltd\tFIEMIND\n",
    "Fine Organic Industries Ltd.\tFINEORG\n",
    "Fineotex Chemical Ltd.\tFCL\n",
    "Force Motors Ltd.\tFORCEMOT\n",
    "G R Infraprojects Ltd.\tGRINFRA\n",
    "GHCL Ltd.\tGHCL\n",
    "GMM Pfaudler Ltd.\tGMMPFAUDLR\n",
    "GMR Power and Urban Infra Ltd.\tGMRP&UI\n",
    "Gabriel India Ltd.\tGABRIEL\n",
    "Ganesh Housing Corporation Ltd.\tGANESHHOUC\n",
    "Ganesha Ecosphere Ltd.\tGANECOS\n",
    "Garware Hi-Tech Films Ltd.\tGRWRHITECH\n",
    "Garware Technical Fibres Ltd.\tGARFIBRES\n",
    "Gateway Distriparks Ltd.\tGATEWAY\n",
    "Gokaldas Exports Ltd.\tGOKEX\n",
    "Gopal Snacks Ltd.\tGOPAL\n",
    "Greaves Cotton Ltd.\tGREAVESCOT\n",
    "Greenpanel Industries Ltd.\tGREENPANEL\n",
    "Greenply Industries Ltd.\tGREENPLY\n",
    "Gujarat Ambuja Exports Ltd.\tGAEL\n",
    "Gujarat State Fertilizers & Chemicals Ltd.\tGSFC\n",
    "Gulf Oil Lubricants India Ltd.\tGULFOILLUB\n",
    "H.G. Infra Engineering Ltd.\tHGINFRA\n",
    "Hathway Cable & Datacom Ltd.\tHATHWAY\n",
    "Healthcare Global Enterprises Ltd.\tHCG\n",
    "HeidelbergCement India Ltd.\tHEIDELBERG\n",
    "Hemisphere Properties India Ltd.\tHEMIPROP\n",
    "Heritage Foods Ltd.\tHERITGFOOD\n",
    "Hikal Ltd.\tHIKAL\n",
    "Hindustan Construction Co. Ltd.\tHCC\n",
    "IFB Industries Ltd.\tIFBIND\n",
    "IIFL Capital Services Ltd.\tIIFLCAPS\n",
    "ITD Cementation India Ltd.\tITDCEM\n",
    "Imagicaaworld Entertainment Ltd.\tIMAGICAA\n",
    "India Glycols Ltd.\tINDIAGLYCO\n",
    "India Shelter Finance Corporation Ltd.\tINDIASHLTR\n",
    "Indian Metals & Ferro Alloys Ltd.\tIMFA\n",
    "Indigo Paints Ltd.\tINDIGOPNTS\n",
    "Indo Count Industries Ltd.\tICIL\n",
    "Infibeam Avenues Ltd.\tINFIBEAM\n",
    "Ingersoll Rand (India) Ltd.\tINGERRAND\n",
    "Innova Captab Ltd.\tINNOVACAP\n",
    "Inox Green Energy Services Ltd.\tINOXGREEN\n",
    "Ion Exchange (India) Ltd.\tIONEXCHANG\n",
    "Isgec Heavy Engineering Ltd.\tISGEC\n",
    "J.Kumar Infraprojects Ltd.\tJKIL\n",
    "JK Lakshmi Cement Ltd.\tJKLAKSHMI\n",
    "JK Paper Ltd.\tJKPAPER\n",
    "JTL Industries Ltd.\tJTLIND\n",
    "Jai Balaji Industries Ltd.\tJAIBALAJI\n",
    "Jai Corp Ltd.\tJAICORPLTD\n",
    "Jain Irrigation Systems Ltd.\tJISLJALEQS\n",
    "Jamna Auto Industries Ltd.\tJAMNAAUTO\n",
    "Jana Small Finance Bank Ltd.\tJSFB\n",
    "Jindal Worldwide Ltd.\tJINDWORLD\n",
    "Johnson Controls - Hitachi Air Conditioning India Ltd.\tJCHAC\n",
    "KPI Green Energy Ltd.\tKPIGREEN\n",
    "KRBL Ltd.\tKRBL\n",
    "KSB Ltd.\tKSB\n",
    "Kalyani Steels Ltd.\tKSL\n",
    "Karnataka Bank Ltd.\tKTKBANK\n",
    "Kaveri Seed Company Ltd.\tKSCL\n",
    "Kirloskar Pneumatic Company Ltd.\tKIRLPNU\n",
    "LMW Ltd.\tLMW\n",
    "Laxmi Organic Industries Ltd.\tLXCHEM\n",
    "Le Travenues Technology Ltd.\tIXIGO\n",
    "Lloyds Engineering Works Ltd.\tLLOYDSENGG\n",
    "Lloyds Enterprises Ltd.\tLLOYDSENT\n",
    "Lux Industries Ltd.\tLUXIND\n",
    "MOIL Ltd.\tMOIL\n",
    "MSTC Ltd.\tMSTCLTD\n",
    "MTAR Technologies Ltd.\tMTARTECH\n",
    "Maharashtra Scooters Ltd.\tMAHSCOOTER\n",
    "Mahindra Lifespace Developers Ltd.\tMAHLIFE\n",
    "Man Infraconstruction Ltd.\tMANINFRA\n",
    "Marksans Pharma Ltd.\tMARKSANS\n",
    "Max Estates Ltd.\tMAXESTATES\n",
    "Medplus Health Services Ltd.\tMEDPLUS\n",
    "Mishra Dhatu Nigam Ltd.\tMIDHANI\n",
    "Mrs. Bectors Food Specialities Ltd.\tBECTORFOOD\n",
    "NEOGEN CHEMICALS LTD.\tNEOGEN\n",
    "NESCO Ltd.\tNESCO\n",
    "NOCIL Ltd.\tNOCIL\n",
    "National Fertilizers Ltd.\tNFL\n",
    "Nazara Technologies Ltd.\tNAZARA\n",
    "Nuvoco Vistas Corporation Ltd.\tNUVOCO\n",
    "Optiemus Infracom Ltd.\tOPTIEMUS\n",
    "Orchid Pharma Ltd.\tORCHPHARMA\n",
    "Orient Cement Ltd.\tORIENTCEM\n",
    "Orissa Min Dev Co Ltd.\tORISSAMINE\n",
    "P N Gadgil Jewellers Ltd.\tPNGJL\n",
    "PC Jeweller Ltd.\tPCJEWELLER\n",
    "PTC India Ltd.\tPTC\n",
    "Paisalo Digital Ltd.\tPAISALO\n",
    "Paradeep Phosphates Ltd.\tPARADEEP\n",
    "Paras Defence and Space Technologies Ltd.\tPARAS\n",
    "Patel Engineering Ltd.\tPATELENG\n",
    "Pearl Global Industries Ltd.\tPGIL\n",
    "Polyplex Corporation Ltd.\tPOLYPLEX\n",
    "Power Mech Projects Ltd.\tPOWERMECH\n",
    "Pricol Ltd.\tPRICOLLTD\n",
    "Prince Pipes and Fittings Ltd.\tPRINCEPIPE\n",
    "Prism Johnson Ltd.\tPRSMJOHNSN\n",
    "Prudent Corporate Advisory Services Ltd.\tPRUDENT\n",
    "Rain Industries Ltd\tRAIN\n",
    "Rajesh Exports Ltd.\tRAJESHEXPO\n",
    "Rallis India Ltd.\tRALLIS\n",
    "Rategain Travel Technologies Ltd.\tRATEGAIN\n",
    "RattanIndia Power Ltd.\tRTNPOWER\n",
    "Redtape Ltd.\tREDTAPE\n",
    "Refex Industries Ltd.\tREFEX\n",
    "Reliance Infrastructure Ltd.\tRELINFRA\n",
    "Religare Enterprises Ltd.\tRELIGARE\n",
    "Responsive Industries Ltd.\tRESPONIND\n",
    "Restaurant Brands Asia Ltd.\tRBA\n",
    "Rossari Biotech Ltd.\tROSSARI\n",
    "Safari Industries (India) Ltd.\tSAFARI\n",
    "Samhi Hotels Ltd.\tSAMHI\n",
    "Sanofi Consumer Healthcare India Ltd.\tSANOFICONR\n",
    "Sanofi India Ltd.\tSANOFI\n",
    "Sansera Engineering Ltd.\tSANSERA\n",
    "Senco Gold Ltd.\tSENCO\n",
    "Sequent Scientific Ltd.\tSEQUENT\n",
    "Shaily Engineering Plastics Ltd.\tSHAILY\n",
    "Shakti Pumps (India) Ltd.\tSHAKTIPUMP\n",
    "Sharda Cropchem Ltd.\tSHARDACROP\n",
    "Share India Securities Ltd.\tSHAREINDIA\n",
    "Sheela Foam Ltd.\tSFL\n",
    "Shilpa Medicare Ltd.\tSHILPAMED\n",
    "Shivalik Bimetal Controls Ltd.\tSBCL\n",
    "Shoppers Stop Ltd.\tSHOPERSTOP\n",
    "Shriram Pistons & Rings Ltd.\tSHRIPISTON\n",
    "Skipper Ltd.\tSKIPPER\n",
    "South Indian Bank Ltd.\tSOUTHBANK\n",
    "Spandana Sphoorty Financial Ltd.\tSPANDANA\n",
    "Star Cement Ltd.\tSTARCEMENT\n",
    "Sterlite Technologies Ltd.\tSTLTECH\n",
    "Strides Pharma Science Ltd.\tSTAR\n",
    "Stylam Industries Ltd.\tSTYLAMIND\n",
    "Subros Ltd.\tSUBROS\n",
    "Sudarshan Chemical Industries Ltd.\tSUDARSCHEM\n",
    "Sula Vineyards Ltd.\tSULA\n",
    "Sun Pharma Advanced Research Company Ltd.\tSPARC\n",
    "Sunflag Iron & Steel Company Ltd.\tSUNFLAG\n",
    "Sunteck Realty Ltd.\tSUNTECK\n",
    "Suprajit Engineering Ltd.\tSUPRAJIT\n",
    "Supriya Lifescience Ltd.\tSUPRIYA\n",
    "Surya Roshni Ltd.\tSURYAROSNI\n",
    "Symphony Ltd.\tSYMPHONY\n",
    "TARC Ltd.\tTARC\n",
    "TD Power Systems Ltd.\tTDPOWERSYS\n",
    "TVS Supply Chain Solutions Ltd.\tTVSSCS\n",
    "Teamlease Services Ltd.\tTEAMLEASE\n",
    "Technocraft Industries (India) Ltd.\tTIIL\n",
    "Tega Industries Ltd.\tTEGA\n",
    "Texmaco Rail & Eng. Ltd.\tTEXRAIL\n",
    "Thangamayil Jewellery Ltd.\tTHANGAMAYL\n",
    "The Anup Engineering Ltd.\tANUP\n",
    "Thirumalai Chemicals Ltd.\tTIRUMALCHM\n",
    "Thomas Cook (India) Ltd.\tTHOMASCOOK\n",
    "Tilaknagar Industries Ltd.\tTI\n",
    "Time Technoplast Ltd.\tTIMETECHNO\n",
    "Tips Music Ltd.\tTIPSMUSIC\n",
    "Transrail Lighting Ltd.\tTRANSRAILL\n",
    "Ujjivan Small Finance Bank Ltd.\tUJJIVANSFB\n",
    "Unimech Aerospace and Manufacturing Ltd.\tUNIMECH\n",
    "V-Mart Retail Ltd.\tVMART\n",
    "V.I.P. Industries Ltd.\tVIPIND\n",
    "VST Industries Ltd.\tVSTIND\n",
    "Va Tech Wabag Ltd.\tWABAG\n",
    "Vaibhav Global Ltd.\tVAIBHAVGBL\n",
    "Varroc Engineering Ltd.\tVARROC\n",
    "Ventive Hospitality Ltd.\tVENTIVE\n",
    "Venus Pipes & Tubes Ltd.\tVENUSPIPES\n",
    "Vesuvius India Ltd.\tVESUVIUS\n",
    "Voltamp Transformers Ltd\tVOLTAMP\n",
    "Websol Energy System Ltd.\tWEBELSOLAR\n",
    "Welspun Enterprises Ltd.\tWELENT\n",
    "Wonderla Holidays Ltd.\tWONDERLA\n",
    "Yatharth Hospital & Trauma Care Services Ltd.\tYATHARTH\n",
    "Zaggle Prepaid Ocean Services Ltd.\tZAGGLE\n",
    "Zinka Logistics Solutions Ltd.\tBLACKBUCK\n",
    "Zydus Wellness Ltd.\tZYDUSWELL\n",
    "eMudhra Ltd.\tEMUDHRA\"\"\"\n",
    "\n",
    "\n",
    "stocks_microcap = parse_stock_list(stock_list_microcap)\n",
    "results_smallcap = analyze_stocks(stocks_microcap, output_dir=\"stock_analysis/reports_v2/microcap_250\", max_workers=10, end_date=\"2025-04-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green color ANSI escape code\n",
    "GREEN = '\\033[92m'\n",
    "RESET = '\\033[0m'  # Reset color code\n",
    "\n",
    "print(f\"{GREEN}Nifty Stocks Processed {len(stocks_nifty)} \\nMidcap Stocks Processed {len(stocks_midcap)} \\nSmallcap Stocks Processed {len(stocks_smallcap)} \\nMicrocap Stocks Processed {len(stocks_microcap)}{RESET}\")\n",
    "print(f\"{GREEN}Total Stocks Processed {len(stocks_nifty)+len(stocks_midcap)+len(stocks_smallcap)+len(stocks_microcap)}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065eaab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "db_name = \"vector_db\"\n",
    "path = \"stock_analysis/reports_v2/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(path)\n",
    "print(\"folders \", folders)\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    # Extract category name from folder path (nifty_50, midcap_150, etc.)\n",
    "    category = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        # Extract stock symbol from filename (e.g., HDFC.md -> HDFC)\n",
    "        filename = os.path.basename(doc.metadata[\"source\"])\n",
    "        symbol = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add structured metadata\n",
    "        doc.metadata[\"category\"] = category  # Explicit category field\n",
    "        doc.metadata[\"stock_category\"] = category  # Alternative name for filtering\n",
    "        doc.metadata[\"symbol\"] = symbol  # Add symbol for stock-specific searches\n",
    "        doc.metadata[\"doc_type\"] = category  # Keep original for compatibility\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"Loaded {len(folder_docs)} documents from {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add preprocessing to ensure key information like normalized z-score rank is prominently included\n",
    "documents_with_enhanced_context = []\n",
    "for doc in documents:\n",
    "    # Extract the most important ranking information from the document\n",
    "    normalized_z_rank = re.findall(r'Normalized Z-Score Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    weighted_z_rank = re.findall(r'Weighted Z-Score Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    one_year_rank = re.findall(r'1-Year Momentum Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    six_month_rank = re.findall(r'6-Month Momentum Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "\n",
    "    # Get universe/category information\n",
    "    universe_match = re.search(r'Universe/Category: ([^\\n]+)', doc.page_content)\n",
    "    universe = universe_match.group(1) if universe_match else None\n",
    "\n",
    "    # Get stock name and symbol from the first line (usually contains \"# Company Name (SYMBOL) Analysis\")\n",
    "    stock_info = re.search(r'# (.*?) \\((.*?)\\)', doc.page_content)\n",
    "    company_name = stock_info.group(1) if stock_info else \"Unknown Company\"\n",
    "    symbol = stock_info.group(2) if stock_info else \"UNKNOWN\"\n",
    "\n",
    "    # Extract current price and performance metrics\n",
    "    current_price_match = re.search(r'Current Price: Rs\\.([0-9.]+)', doc.page_content)\n",
    "    current_price = current_price_match.group(1) if current_price_match else None\n",
    "\n",
    "    one_year_change_match = re.search(r'1-Year Change: ([0-9.-]+)%', doc.page_content)\n",
    "    one_year_change = one_year_change_match.group(1) if one_year_change_match else None\n",
    "\n",
    "    six_month_change_match = re.search(r'6-Month Change: ([0-9.-]+)%', doc.page_content)\n",
    "    six_month_change = six_month_change_match.group(1) if six_month_change_match else None\n",
    "\n",
    "    # If the document contains rank information, enhance its representation\n",
    "    rank_summary = f\"STOCK RANKING INFORMATION - HIGH PRIORITY:\\n\"\n",
    "    rank_summary += f\"Stock: {company_name} ({symbol})\\n\"\n",
    "\n",
    "    if universe:\n",
    "        rank_summary += f\"Universe/Category: {universe}\\n\"\n",
    "\n",
    "    if current_price:\n",
    "        rank_summary += f\"Current Price: Rs.{current_price}\\n\"\n",
    "\n",
    "    if one_year_change:\n",
    "        rank_summary += f\"1-Year Change: {one_year_change}%\\n\"\n",
    "\n",
    "    if six_month_change:\n",
    "        rank_summary += f\"6-Month Change: {six_month_change}%\\n\"\n",
    "\n",
    "    # Add ranking information with special formatting to make it stand out\n",
    "    if normalized_z_rank:\n",
    "        rank_summary += f\"===NORMALIZED Z-SCORE RANK: {normalized_z_rank[0][0]} OUT OF {normalized_z_rank[0][1]}===\\n\"\n",
    "\n",
    "    if weighted_z_rank:\n",
    "        rank_summary += f\"===WEIGHTED Z-SCORE RANK: {weighted_z_rank[0][0]} OUT OF {weighted_z_rank[0][1]}===\\n\"\n",
    "\n",
    "    if one_year_rank:\n",
    "        rank_summary += f\"===1-YEAR MOMENTUM RANK: {one_year_rank[0][0]} OUT OF {one_year_rank[0][1]}===\\n\"\n",
    "\n",
    "    if six_month_rank:\n",
    "        rank_summary += f\"===6-MONTH MOMENTUM RANK: {six_month_rank[0][0]} OUT OF {six_month_rank[0][1]}===\\n\"\n",
    "\n",
    "    # Prepend the summary to the document content (THREE times to heavily emphasize it)\n",
    "    # This ensures that the ranking information gets the highest weight in the embedding\n",
    "    doc.page_content = rank_summary + \"\\n\" + rank_summary + \"\\n\" + rank_summary + \"\\n\" + doc.page_content\n",
    "\n",
    "    # Also add key information to metadata for filtering\n",
    "    if normalized_z_rank:\n",
    "        doc.metadata['normalized_z_rank'] = int(normalized_z_rank[0][0])\n",
    "        doc.metadata['normalized_z_total'] = int(normalized_z_rank[0][1])\n",
    "\n",
    "    if weighted_z_rank:\n",
    "        doc.metadata['weighted_z_rank'] = int(weighted_z_rank[0][0])\n",
    "        doc.metadata['weighted_z_total'] = int(weighted_z_rank[0][1])\n",
    "\n",
    "    if one_year_rank:\n",
    "        doc.metadata['one_year_rank'] = int(one_year_rank[0][0])\n",
    "        doc.metadata['one_year_total'] = int(one_year_rank[0][1])\n",
    "\n",
    "    if six_month_rank:\n",
    "        doc.metadata['six_month_rank'] = int(six_month_rank[0][0])\n",
    "        doc.metadata['six_month_total'] = int(six_month_rank[0][1])\n",
    "\n",
    "    documents_with_enhanced_context.append(doc)\n",
    "\n",
    "# Use the enhanced documents for chunking - adjust the chunking strategy to preserve entire rank sections\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1800,  # Increase chunk size to ensure rank sections stay together\n",
    "    chunk_overlap=400,  # Increase overlap for better context preservation\n",
    "    separator=\"\\n\\n\",  # Split on paragraph boundaries\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents_with_enhanced_context)\n",
    "\n",
    "\n",
    "# Print some examples of the metadata to verify\n",
    "for i in range(min(3, len(chunks))):\n",
    "    print(f\"Chunk {i} metadata: {chunks[i].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36321c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49957e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_name,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"}  # Explicitly specify distance metric\n",
    ")\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84060f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework for chroma vectors\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['nifty_50', 'midcap_150', 'smallcap_250','microcap_250'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_visualization():\n",
    "    # We humans find it easier to visalize things in 2D!\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Create the 2D scatter plot with improved layout for side-by-side display\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=colors, opacity=0.8),\n",
    "        text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(r=10, b=10, l=10, t=10),  # Minimize margins\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,\n",
    "        hovermode=\"closest\",\n",
    "        height=None,  # Let height be determined by container\n",
    "        width=None    # Let width be determined by container\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_3d_visualization():\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Create the 3D scatter plot with improved layout for side-by-side display\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        z=reduced_vectors[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color=colors, opacity=0.7),  # Smaller markers for better performance\n",
    "        text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='x',\n",
    "            yaxis_title='y',\n",
    "            zaxis_title='z',\n",
    "            aspectmode='data'  # Better fitting for the container\n",
    "        ),\n",
    "        margin=dict(r=0, b=0, l=0, t=0),  # Minimize margins\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,\n",
    "        height=None,  # Let height be determined by container\n",
    "        width=None    # Let width be determined by container\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks in vectorstore: {len(chunks)}\")\n",
    "print(f\"Total unique stocks: 621\")\n",
    "print(f\"Average chunks per stock: {len(chunks)/621:.2f}\")\n",
    "\n",
    "# Calculate recommended k value\n",
    "recommended_k = len(chunks)  # Start with maximum possible chunks\n",
    "print(f\"Recommended k value: {recommended_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are a financial advisor specializing in the Indian stock market analysis.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "Always respond in English only.\n",
    "Focus on providing factual information from the given context.\n",
    "\n",
    "If the question is about a specific category of stocks (like \"Nifty 50\", \"midcap\", \"smallcap\", or \"microcap\"):\n",
    "- Group your answer by the requested category\n",
    "- Present information as a well-organized list\n",
    "- Include key metrics like price, momentum scores, and rankings when available\n",
    "- When asked for rankings or \"top stocks\", ALWAYS sort by Normalized Z-Score Rank in ascending order (Rank 1 is best) unless another specific rank type is mentioned\n",
    "- When asked for \"top 10\" or similar, return EXACTLY the stocks with ranks 1-10, not any random 10 stocks\n",
    "- Be very precise about the category - never mix stocks from different categories (e.g., don't include microcap stocks when asked about smallcap or don't mix stocks from one universe to other)\n",
    "\n",
    "If the question mentions \"normalized z-score\" or \"normalized z-score rank\":\n",
    "- THIS IS EXTREMELY IMPORTANT: Sort results by Normalized Z-Score Rank in ascending order (Rank 1 is best)\n",
    "- Include ONLY stocks that have this specific rank information\n",
    "- Present results with lowest rank numbers first (1, 2, 3, etc.)\n",
    "- Include the stock symbol, company name, and the exact Normalized Z-Score Rank value\n",
    "- Example format for each stock: \"Company Name (SYMBOL) - Normalized Z-Score Rank: X out of Y\"\n",
    "\n",
    "If the question mentions \"weighted z-score\" or \"weighted z-score rank\":\n",
    "- Sort results by Weighted Z-Score Rank in ascending order (Rank 1 is best)\n",
    "- Include ONLY stocks that have this specific rank information\n",
    "- Present results with lowest rank numbers first (1, 2, 3, etc.)\n",
    "- Include the stock symbol, company name, and the exact Weighted Z-Score Rank value\n",
    "\n",
    "If the question mentions \"momentum\", \"positive momentum\", \"strong performance\" or similar terms:\n",
    "- Focus on stocks with positive momentum indicators such as:\n",
    "  * Positive 1-year or 6-month price change (>0%)\n",
    "  * Strong RSI values (above 50)\n",
    "  * Good momentum ranks (in top 40% of their category)\n",
    "  * Positive MACD indicators\n",
    "  * Price above key moving averages\n",
    "- Sort results by momentum ranks when available (Rank 1 is best)\n",
    "- Be explicit about which momentum criteria you're using\n",
    "\n",
    "If the question is about a specific stock, make sure to include:\n",
    "- Technical indicators (RSI, MACD, etc.) if available\n",
    "- Price performance data\n",
    "- Strength and weakness analysis\n",
    "- Momentum rankings if available\n",
    "\n",
    "If you don't know the answer based on the given context, just say you don't have enough information.\n",
    "Don't make up information that isn't provided in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer (in English only):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f00163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base retriever parameters\n",
    "max_k = min(100, len(chunks) // 10)  # Cap at 100 or 10% of chunks, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a398746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_query(query, vectorstore):\n",
    "    \"\"\"Add category-specific processing for better retrieval\"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Define category keywords and mappings\n",
    "    categories = {\n",
    "        \"nifty 50\": \"nifty_50\",\n",
    "        \"nifty50\": \"nifty_50\",\n",
    "        \"nifty\": \"nifty_50\",\n",
    "        \"midcap\": \"midcap_150\",\n",
    "        \"mid-cap\": \"midcap_150\",\n",
    "        \"mid cap\": \"midcap_150\",\n",
    "        \"smallcap\": \"smallcap_250\",\n",
    "        \"small-cap\": \"smallcap_250\",\n",
    "        \"small cap\": \"smallcap_250\",\n",
    "        \"microcap\": \"microcap_250\",\n",
    "        \"micro-cap\": \"microcap_250\",\n",
    "        \"micro cap\": \"microcap_250\"\n",
    "    }\n",
    "\n",
    "    # Check if query contains category keywords\n",
    "    detected_category = None\n",
    "    for keyword, folder_name in categories.items():\n",
    "        pattern = r'\\b' + re.escape(keyword) + r'\\b'\n",
    "        if re.search(pattern, query_lower):\n",
    "            detected_category = folder_name\n",
    "            print(f\"Detected category: {detected_category}\")\n",
    "            break\n",
    "        if keyword in query_lower:\n",
    "            detected_category = folder_name\n",
    "            print(f\"Detected category: {detected_category}\")\n",
    "            break\n",
    "\n",
    "    # Check for momentum or performance filter\n",
    "    momentum_filter = False\n",
    "    momentum_keywords = [\"momentum\", \"positive momentum\", \"strong momentum\", \"good performance\",\n",
    "                         \"top performing\", \"top stocks\", \"best stocks\", \"positive return\"]\n",
    "\n",
    "    # Check for rank-based queries\n",
    "    rank_based_query = bool(re.search(r'top\\s+\\d+|best\\s+\\d+', query_lower))\n",
    "    rank_keywords = [\"rank\", \"ranking\", \"ranked\", \"momentum rank\", \"z-score rank\"]\n",
    "\n",
    "    for keyword in momentum_keywords:\n",
    "        if keyword in query_lower:\n",
    "            momentum_filter = True\n",
    "            print(f\"Detected momentum filter: {keyword}\")\n",
    "            break\n",
    "\n",
    "    for keyword in rank_keywords:\n",
    "        if keyword in query_lower:\n",
    "            momentum_filter = True\n",
    "            print(f\"Detected rank filter: {keyword}\")\n",
    "            break\n",
    "\n",
    "    # Around line 2940-2965\n",
    "    if detected_category:\n",
    "        # Use metadata filtering with the vectorstore\n",
    "        filtered_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": max_k,\n",
    "                \"fetch_k\": max_k * 3,\n",
    "                \"lambda_mult\": 0.6,\n",
    "                \"filter\": {\n",
    "                    # Use multiple metadata fields for redundant filtering\n",
    "                    \"$and\": [\n",
    "                        {\"doc_type\": detected_category},\n",
    "                        {\"category\": detected_category}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return filtered_retriever, detected_category, momentum_filter or rank_based_query\n",
    "\n",
    "    # If no category detected, return the default retriever\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": max_k,\n",
    "            \"fetch_k\": max_k * 2,\n",
    "            \"lambda_mult\": 0.7,\n",
    "        }\n",
    "    ), None, momentum_filter or rank_based_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(query):\n",
    "    \"\"\"Test function to check what documents are being retrieved for a query\"\"\"\n",
    "    # Get the query keywords\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Check for category keywords\n",
    "    categories = {\n",
    "        \"nifty 50\": \"nifty_50\",\n",
    "        \"nifty50\": \"nifty_50\",\n",
    "        \"nifty\": \"nifty_50\",\n",
    "        \"midcap\": \"midcap_150\",\n",
    "        \"mid-cap\": \"midcap_150\",\n",
    "        \"mid cap\": \"midcap_150\",\n",
    "        \"smallcap\": \"smallcap_250\",\n",
    "        \"small-cap\": \"smallcap_250\",\n",
    "        \"small cap\": \"smallcap_250\",\n",
    "        \"microcap\": \"microcap_250\",\n",
    "        \"micro-cap\": \"microcap_250\",\n",
    "        \"micro cap\": \"microcap_250\"\n",
    "    }\n",
    "\n",
    "    detected = False\n",
    "    for keyword, category in categories.items():\n",
    "        if keyword in query_lower:\n",
    "            print(f\"Found category keyword '{keyword}' -> '{category}'\")\n",
    "            detected = True\n",
    "\n",
    "    if not detected:\n",
    "        print(\"No category keyword detected in query\")\n",
    "\n",
    "    # Test retrieval with the process_category_query\n",
    "    specific_retriever = process_category_query(query, vectorstore)\n",
    "    docs = specific_retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Check document metadata\n",
    "    print(f\"\\nRetrieved {len(docs)} documents\")\n",
    "    categories_found = {}\n",
    "\n",
    "    for i, doc in enumerate(docs[:5]):  # Check first 5 docs\n",
    "        doc_type = doc.metadata.get('doc_type', 'unknown')\n",
    "        categories_found[doc_type] = categories_found.get(doc_type, 0) + 1\n",
    "        if i < 3:  # Show details for first 3 docs\n",
    "            print(f\"\\nDocument {i+1}:\")\n",
    "            print(f\"  Metadata: {doc.metadata}\")\n",
    "            print(f\"  Content (first 100 chars): {doc.page_content[:100]}...\")\n",
    "\n",
    "    print(\"\\nDocument categories distribution:\")\n",
    "    for cat, count in categories_found.items():\n",
    "        print(f\"  - {cat}: {count} documents\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "# Uncomment and run this to debug specific queries\n",
    "# debug_retrieval(\"Show me Nifty 50 stocks\")\n",
    "# debug_retrieval(\"List midcap stocks with good RSI\")\n",
    "# debug_retrieval(\"What are the smallcap stocks with strong momentum?\")\n",
    "# debug_retrieval(\"Show me microcap stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCategoryRetriever(BaseRetriever):\n",
    "    vectorstore: Any = Field(description=\"Vector store for embeddings\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Get the appropriate retriever based on query content\n",
    "        specific_retriever, category, has_momentum_or_rank_filter = process_category_query(query, self.vectorstore)\n",
    "\n",
    "        # Use it to retrieve documents\n",
    "        docs = specific_retriever.get_relevant_documents(query)\n",
    "\n",
    "        # Check if this is a rank-based query (like \"top 10 stocks\")\n",
    "        rank_based_query = bool(re.search(r'top\\s+\\d+|best\\s+\\d+', query.lower()))\n",
    "\n",
    "        # Apply additional momentum/ranking filtering if needed\n",
    "        if (has_momentum_or_rank_filter or rank_based_query) and len(docs) > 0:\n",
    "            # We'll increase the number of documents retrieved for rank-based queries\n",
    "            # to ensure we have enough data to find the truly top-ranked stocks\n",
    "            if rank_based_query and category:\n",
    "                print(f\"Rank-based query detected for {category}. Retrieved {len(docs)} documents.\")\n",
    "                # The actual sorting will be handled by the LLM with the improved prompt\n",
    "\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Just call the synchronous version for simplicity\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8301af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with GenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7, google_api_key = GOOGLE_API_KEY)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "# Create the dynamic retriever\n",
    "retriever = DynamicCategoryRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Create the conversation chain as a global variable so it's accessible\n",
    "global conversation_chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect the vectorstore for category distribution\n",
    "def inspect_vectorstore_categories(vectorstore):\n",
    "    collection = vectorstore._collection\n",
    "    result = collection.get(include=['metadatas'], limit=1000)\n",
    "\n",
    "    metadatas = result['metadatas']\n",
    "    categories = {}\n",
    "\n",
    "    print(f\"Inspecting {len(metadatas)} documents in vectorstore\")\n",
    "\n",
    "    # Count documents by doc_type (category)\n",
    "    for metadata in metadatas:\n",
    "        doc_type = metadata.get('doc_type', 'unknown')\n",
    "        categories[doc_type] = categories.get(doc_type, 0) + 1\n",
    "\n",
    "    print(\"\\nDocument distribution by category:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"  - {category}: {count} documents\")\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Run this to check your vectorstore category distribution\n",
    "categories_in_db = inspect_vectorstore_categories(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e590369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug function to inspect metadata in the vectorstore\n",
    "# def inspect_vectorstore_metadata(vectorstore, n_samples=5):\n",
    "#     collection = vectorstore._collection\n",
    "#     result = collection.get(include=['embeddings', 'documents', 'metadatas'], limit=100)\n",
    "\n",
    "#     metadatas = result['metadatas']\n",
    "#     categories = {}\n",
    "\n",
    "#     print(f\"\\n=== Inspecting metadata for {len(metadatas)} documents ===\")\n",
    "\n",
    "#     # Count documents by category\n",
    "#     for metadata in metadatas:\n",
    "#         category = metadata.get('category', metadata.get('doc_type', 'unknown'))\n",
    "#         categories[category] = categories.get(category, 0) + 1\n",
    "\n",
    "#     print(\"\\nDocuments by category:\")\n",
    "#     for category, count in categories.items():\n",
    "#         print(f\"  - {category}: {count} documents\")\n",
    "\n",
    "#     # Show sample metadata entries\n",
    "#     print(f\"\\nSample metadata entries ({n_samples}):\")\n",
    "#     for i, metadata in enumerate(metadatas[:n_samples]):\n",
    "#         print(f\"\\nDocument {i+1}:\")\n",
    "#         for key, value in metadata.items():\n",
    "#             print(f\"  {key}: {value}\")\n",
    "\n",
    "#     return categories\n",
    "\n",
    "# # Run the inspection after creating the vectorstore\n",
    "# categories = inspect_vectorstore_metadata(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c304937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    global conversation_chain  # Explicitly use the global variable\n",
    "    try:\n",
    "        # Check for normalized z-score keywords in the query\n",
    "        normalized_z_keywords = [\"normalized z-score\", \"normalized z score\", \"normalized z-score rank\", \"normalized z score rank\"]\n",
    "        has_normalized_z = any(keyword in message.lower() for keyword in normalized_z_keywords)\n",
    "\n",
    "        # Check for weighted z-score keywords in the query\n",
    "        weighted_z_keywords = [\"weighted z-score\", \"weighted z score\", \"weighted z-score rank\", \"weighted z score rank\"]\n",
    "        has_weighted_z = any(keyword in message.lower() for keyword in weighted_z_keywords)\n",
    "\n",
    "        # Check for generic rank keywords that should use normalized z-score as default\n",
    "        generic_rank_keywords = [\"rank\", \"ranking\", \"top stocks\", \"best stocks\"]\n",
    "        has_generic_rank = any(keyword in message.lower() for keyword in generic_rank_keywords) and not has_normalized_z and not has_weighted_z\n",
    "\n",
    "        # Check for momentum keywords in the query\n",
    "        momentum_keywords = [\"momentum\", \"positive momentum\", \"strong momentum\", \"good performance\",\n",
    "                           \"top performing\", \"top stocks\", \"best stocks\", \"positive return\"]\n",
    "        has_momentum_filter = any(keyword in message.lower() for keyword in momentum_keywords)\n",
    "\n",
    "        # Check for rank-based queries (top N stocks)\n",
    "        rank_based_query = bool(re.search(r'top\\s+\\d+|best\\s+\\d+', message.lower()))\n",
    "        rank_keywords = [\"rank\", \"ranking\", \"ranked\", \"momentum rank\"]\n",
    "        has_rank_filter = any(keyword in message.lower() for keyword in rank_keywords)\n",
    "\n",
    "        # If it's a normalized z-score query or generic rank query, add instructions\n",
    "        if has_normalized_z or has_generic_rank:\n",
    "            print(\"Detected normalized z-score or generic rank query, using normalized z-score rank...\")\n",
    "            message = message + \" (Please sort results by Normalized Z-Score Rank in ascending order, with Rank 1 being the best. Format each result as 'Company Name (SYMBOL) - Normalized Z-Score Rank: X out of Y')\"\n",
    "\n",
    "        # If it's a weighted z-score query, add instructions\n",
    "        elif has_weighted_z:\n",
    "            print(\"Detected weighted z-score query, adding special instructions...\")\n",
    "            message = message + \" (Please sort results by Weighted Z-Score Rank in ascending order, with Rank 1 being the best. Format each result as 'Company Name (SYMBOL) - Weighted Z-Score Rank: X out of Y')\"\n",
    "\n",
    "        # If it's a momentum or rank query, add instructions\n",
    "        elif has_momentum_filter or rank_based_query or has_rank_filter:\n",
    "            print(\"Detected momentum or rank-based query, adding context...\")\n",
    "\n",
    "            # For \"top N\" queries specifically about ranks, add explicit sorting instructions\n",
    "            if rank_based_query or has_rank_filter:\n",
    "                print(\"Adding rank sorting instructions...\")\n",
    "                # Modify the query to explicitly instruct sorting by rank (1 is best)\n",
    "                message = message + \" (Please sort by momentum rank in ascending order, with Rank 1 being the best)\"\n",
    "\n",
    "        result = conversation_chain.invoke({\"question\": message})\n",
    "        return result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat function: {e}\")\n",
    "        return f\"Sorry, an error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82727fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom CSS for better spacing and layout\n",
    "\n",
    "# css = \"\"\"\n",
    "# .gradio-container {max-width: 1200px !important; margin-left: auto !important; margin-right: auto !important;}\n",
    "# .plot-container {width: 100% !important; display: flex !important; justify-content: center !important;}\n",
    "# .visualization-row {display: flex !important; justify-content: space-between !important; width: 100% !important;}\n",
    "# .visualization-column {flex: 1 !important; padding: 0 10px !important;}\n",
    "# \"\"\"\n",
    "\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 100% !important;\n",
    "    width: 100% !important;\n",
    "    margin: 0 !important;\n",
    "    padding: 0 !important;\n",
    "    min-height: 100vh !important;\n",
    "}\n",
    "\n",
    "/* Main row container */\n",
    ".app-container {\n",
    "    display: flex;\n",
    "    min-height: 100vh;\n",
    "}\n",
    "\n",
    "/* Chat container styling */\n",
    ".chat-container {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    min-height: 100vh;\n",
    "}\n",
    "\n",
    "/* Target the chatbot message container */\n",
    ".chat-container > .prose {\n",
    "    flex: 1;\n",
    "}\n",
    "\n",
    "/* Make the message container scrollable */\n",
    ".message-wrap, .chatbot-message-container {\n",
    "    height: calc(100vh - 180px) !important;\n",
    "    max-height: none !important;\n",
    "    overflow-y: auto !important;\n",
    "}\n",
    "\n",
    "/* Visualization container styling */\n",
    ".plots-container {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    min-height: 100vh;\n",
    "}\n",
    "\n",
    ".plot-3d-container, .plot-2d-container {\n",
    "    height: 50vh;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "/* Make plot elements fill their containers */\n",
    ".plot-container {\n",
    "    width: 100% !important;\n",
    "    height: 100% !important;\n",
    "}\n",
    "\n",
    "/* Responsive layout for smaller screens */\n",
    "@media (max-width: 992px) {\n",
    "    .app-container {\n",
    "        flex-direction: column;\n",
    "    }\n",
    "\n",
    "    .chat-container, .plots-containers {\n",
    "        min-height: 50vh;\n",
    "    }\n",
    "\n",
    "    .message-wrap, .chatbot-message-container {\n",
    "        height: calc(50vh - 120px) !important;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7436eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Blocks interface with visualizations\n",
    "with gr.Blocks(css=css, theme=\"soft\") as view:\n",
    "    with gr.Row(elem_classes=\"app-container\"):\n",
    "        # Left side - Chat interface\n",
    "        with gr.Column(elem_classes=\"chat-container\"):\n",
    "            gr.HTML(\"<h2 style='text-align:center;'>Stock Market Analysis By AJ14314</h2>\")\n",
    "            chat_interface = gr.ChatInterface(\n",
    "                fn=chat,\n",
    "                examples=[\n",
    "                    \"Show me all Nifty 50 stocks\",\n",
    "                    \"List the top 10 performing midcap stocks\",\n",
    "                    \"What are the smallcap stocks with positive momentum?\",\n",
    "                    \"Show microcap stocks with highest ranks\",\n",
    "                    \"Compare Reliance and TCS performance\"\n",
    "                ],\n",
    "                description=\"Ask questions about Indian stocks to get insights based on technical analysis reports.\",\n",
    "                theme=\"soft\",\n",
    "                autofocus=True,\n",
    "                fill_height=True\n",
    "            )\n",
    "\n",
    "        # Right side - Plots container with 3D on top and 2D below\n",
    "        with gr.Column(elem_classes=\"plots-container\"):\n",
    "            # 3D Plot on top\n",
    "            with gr.Column(elem_classes=\"plot-3d-container\"):\n",
    "                gr.HTML(\"<h3 style='text-align:center;'>3D Vector Space Visualization</h3>\")\n",
    "                plot_3d = gr.Plot(create_3d_visualization(), elem_id=\"plot-3d\", container=True)\n",
    "\n",
    "            # 2D Plot below\n",
    "            with gr.Column(elem_classes=\"plot-2d-container\"):\n",
    "                gr.HTML(\"<h3 style='text-align:center;'>2D Vector Space Visualization</h3>\")\n",
    "                plot_2d = gr.Plot(create_2d_visualization(), elem_id=\"plot-2d\", container=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "view.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee217852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
