{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9642efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import concurrent.futures\n",
    "from typing import Dict, Any\n",
    "import logging\n",
    "from threading import Lock\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Thread-safe lock for file writing\n",
    "file_lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e8d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gradio as gr\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3647e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document, BaseRetriever\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from typing import List\n",
    "from pydantic import Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc58ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analysis_date():\n",
    "    \"\"\"Get analysis date from user input or determine the most recent trading day\"\"\"\n",
    "    from datetime import datetime, timedelta\n",
    "    import pandas as pd\n",
    "\n",
    "    # Define Indian trading holidays for 2025\n",
    "    indian_trading_holidays_2025 = [\n",
    "        \"2025-01-26\", \"2025-03-01\", \"2025-03-17\", \"2025-04-02\",\n",
    "        \"2025-04-14\", \"2025-04-18\", \"2025-05-01\", \"2025-08-15\",\n",
    "        \"2025-09-02\", \"2025-10-02\", \"2025-10-24\", \"2025-11-12\", \"2025-12-25\"\n",
    "    ]\n",
    "\n",
    "    user_input = input(\"Enter date (YYYY-MM-DD) or press Enter for most recent trading day: \").strip()\n",
    "\n",
    "    if user_input:\n",
    "        try:\n",
    "            input_date = pd.to_datetime(user_input).normalize()\n",
    "            today = pd.to_datetime(datetime.now()).normalize()\n",
    "\n",
    "            if input_date > today:\n",
    "                print(f\"Date {user_input} is in the future. Using today instead.\")\n",
    "                return today.strftime('%Y-%m-%d')\n",
    "\n",
    "            return input_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Invalid date format: {e}\")\n",
    "            print(\"Will calculate the last valid trading day instead.\")\n",
    "\n",
    "    # No input given: find recent Friday/Saturday from today\n",
    "    today = pd.to_datetime(datetime.now()).normalize()\n",
    "    weekday = today.weekday()  # Monday = 0 ... Sunday = 6\n",
    "\n",
    "    # Move back until we hit a Thursday or Friday\n",
    "    while weekday not in [3, 4]:  # Thursday = 3, Friday = 4\n",
    "        today -= timedelta(days=1)\n",
    "        weekday = today.weekday()\n",
    "\n",
    "    if weekday == 4:  # Friday\n",
    "        friday = today\n",
    "        saturday = friday + timedelta(days=1)\n",
    "        friday_str = friday.strftime('%Y-%m-%d')\n",
    "        if friday_str not in indian_trading_holidays_2025:\n",
    "            print(f\"Using Saturday after valid Friday: {saturday.strftime('%Y-%m-%d')}\")\n",
    "            return saturday.strftime('%Y-%m-%d')\n",
    "        else:\n",
    "            print(f\"Friday {friday_str} was a holiday. Using Friday anyway.\")\n",
    "            return friday_str\n",
    "\n",
    "    elif weekday == 3:  # Thursday\n",
    "        friday = today + timedelta(days=1)\n",
    "        friday_str = friday.strftime('%Y-%m-%d')\n",
    "        print(f\"Using Friday after Thursday: {friday_str}\")\n",
    "        return friday_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec0f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = get_analysis_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1783e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "EMBEDDING_MODEL  = \"intfloat/e5-large-v2\"\n",
    "# \"intfloat/e5-base-v2\"\n",
    "# \"intfloat/e5-large-v2\"\n",
    "# \"openlm-research/open_llama_7b\"\n",
    "# \"sentence-transformers/all-mpnet-base-v2\"\n",
    "# \"BAAI/bge-large-en\"\n",
    "# \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# \"paraphrase-albert-small-v2\"\n",
    "\n",
    "db_name = \"vector_db\"\n",
    "path = \"stock_analysis/reports_v2/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c56537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(data, window=14):\n",
    "    # Ensure the 'Close' column is correctly accessed\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Calculate price changes\n",
    "    delta = close_prices.diff()\n",
    "\n",
    "    # Separate gains (positive) and losses (negative)\n",
    "    gains = delta.where(delta > 0, 0)\n",
    "    losses = -delta.where(delta < 0, 0)\n",
    "\n",
    "    # Initialize the averages\n",
    "    avg_gains = [np.nan] * len(close_prices)\n",
    "    avg_losses = [np.nan] * len(close_prices)\n",
    "\n",
    "    # Calculate first averages after initial window\n",
    "    first_avg_gain = gains[1:window+1].mean()\n",
    "    first_avg_loss = losses[1:window+1].mean()\n",
    "    avg_gains[window] = first_avg_gain\n",
    "    avg_losses[window] = first_avg_loss\n",
    "\n",
    "    # Calculate subsequent values using the Wilder's smoothing method\n",
    "    for i in range(window+1, len(close_prices)):\n",
    "        avg_gain = (avg_gains[i-1] * (window-1) + gains[i]) / window\n",
    "        avg_loss = (avg_losses[i-1] * (window-1) + losses[i]) / window\n",
    "        avg_gains[i] = avg_gain\n",
    "        avg_losses[i] = avg_loss\n",
    "\n",
    "    # Convert to Series with proper index\n",
    "    avg_gains = pd.Series(avg_gains, index=close_prices.index)\n",
    "    avg_losses = pd.Series(avg_losses, index=close_prices.index)\n",
    "\n",
    "    # Calculate RS and RSI\n",
    "    rs = avg_gains / avg_losses\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ab0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbol, period='1y', end_date=None):\n",
    "    \"\"\"Fetch stock data with proper error handling and ensure unique data per stock\n",
    "\n",
    "    Args:\n",
    "        symbol (str): Stock symbol\n",
    "        period (str): Data period to fetch (default='1y')\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses current date\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame with stock data, actual_end_date)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        time.sleep(1)  # Add delay to prevent rate limiting\n",
    "\n",
    "        kwargs = {\n",
    "            'tickers': f\"{symbol}.NS\",\n",
    "            'period': period,\n",
    "            'progress': False,\n",
    "            'threads': False,\n",
    "            'ignore_tz': True,\n",
    "            'auto_adjust': True,\n",
    "            'prepost': False,\n",
    "            'repair': True\n",
    "        }\n",
    "\n",
    "        # Add end parameter if end_date is provided\n",
    "        if end_date:\n",
    "            kwargs['end'] = end_date\n",
    "            # Calculate start date based on period\n",
    "            if period == '1y':\n",
    "                start_date = pd.to_datetime(end_date) - pd.DateOffset(years=1)\n",
    "            elif period == '2y':\n",
    "                start_date = pd.to_datetime(end_date) - pd.DateOffset(years=2)\n",
    "            kwargs['start'] = start_date.strftime('%Y-%m-%d')\n",
    "\n",
    "        # Download data\n",
    "        data = yf.download(**kwargs)\n",
    "\n",
    "        if data.empty:\n",
    "            logger.warning(f\"No data available for {symbol}\")\n",
    "            return pd.DataFrame(), None\n",
    "\n",
    "        # Get the actual last date from the data\n",
    "        actual_end_date = data.index[-1]\n",
    "\n",
    "        # Handle multi-index columns if present\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            df = pd.DataFrame(index=data.index)\n",
    "            column_map = {\n",
    "                'Open': ('Open', symbol + '.NS'),\n",
    "                'High': ('High', symbol + '.NS'),\n",
    "                'Low': ('Low', symbol + '.NS'),\n",
    "                'Close': ('Close', symbol + '.NS'),\n",
    "                'Volume': ('Volume', symbol + '.NS')\n",
    "            }\n",
    "            for col, multi_idx in column_map.items():\n",
    "                try:\n",
    "                    if multi_idx in data.columns:\n",
    "                        df[col] = data[multi_idx]\n",
    "                    else:\n",
    "                        df[col] = data[(multi_idx[0],)]\n",
    "                except:\n",
    "                    df[col] = data[multi_idx[0]]\n",
    "        else:\n",
    "            df = data.copy()\n",
    "\n",
    "        # Verify data is valid\n",
    "        if 'Close' in df.columns:\n",
    "            latest_price = df['Close'].iloc[-1]\n",
    "            logger.info(f\"Verified unique data for {symbol}: Latest price = Rs.{latest_price:.2f}\")\n",
    "            if end_date:\n",
    "                requested_date = pd.to_datetime(end_date).normalize()\n",
    "                actual_date = actual_end_date.normalize()\n",
    "                if actual_date != requested_date:\n",
    "                    logger.info(f\"Note: Last available date ({actual_date.strftime('%Y-%m-%d')}) differs from requested date ({requested_date.strftime('%Y-%m-%d')})\")\n",
    "        else:\n",
    "            logger.error(f\"Missing Close column for {symbol}\")\n",
    "            return pd.DataFrame(), None\n",
    "\n",
    "        return df, actual_end_date\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return pd.DataFrame(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571694b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
    "    try:\n",
    "        # Make sure we're working with a copy of the data to avoid warnings\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0].copy()\n",
    "        else:\n",
    "            close_series = data['Close'].copy()\n",
    "\n",
    "        # Calculate EMAs\n",
    "        ema_fast = close_series.ewm(span=fast, adjust=False).mean()\n",
    "        ema_slow = close_series.ewm(span=slow, adjust=False).mean()\n",
    "\n",
    "        # Calculate MACD components\n",
    "        macd_line = ema_fast - ema_slow\n",
    "        signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "        histogram = macd_line - signal_line\n",
    "\n",
    "        return macd_line, signal_line, histogram\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating MACD: {e}\")\n",
    "        empty_series = pd.Series(dtype=float)\n",
    "        return empty_series, empty_series, empty_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ee0eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum(data, period=20):\n",
    "    try:\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0]\n",
    "        else:\n",
    "            close_series = data['Close']\n",
    "\n",
    "        momentum = close_series / close_series.shift(period) - 1\n",
    "        return momentum * 100\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating momentum: {e}\")\n",
    "        return pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da881ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum_index(data, period=126):\n",
    "    try:\n",
    "        if isinstance(data.columns, pd.MultiIndex):\n",
    "            close_series = data['Close'].iloc[:, 0]\n",
    "        else:\n",
    "            close_series = data['Close']\n",
    "\n",
    "        returns = close_series.pct_change().dropna()\n",
    "        momentum_std = returns.rolling(window=period).std() * np.sqrt(252)\n",
    "        return momentum_std\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating momentum index: {e}\")\n",
    "        return pd.Series(dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fundamental_data(symbol: str) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch fundamental data\"\"\"\n",
    "    try:\n",
    "        ticker = yf.Ticker(f\"{symbol}.NS\")\n",
    "        info = ticker.info\n",
    "        fundamental_data = {\n",
    "            'P/E Ratio': info.get('trailingPE', 'N/A'),\n",
    "            'Forward P/E': info.get('forwardPE', 'N/A'),\n",
    "            'Market Cap': info.get('marketCap', 'N/A'),\n",
    "            'EPS': info.get('trailingEps', 'N/A'),\n",
    "            'Dividend Yield': info.get('dividendYield', 'N/A'),\n",
    "            'Debt to Equity': info.get('debtToEquity', 'N/A'),\n",
    "            'Return on Equity': info.get('returnOnEquity', 'N/A'),\n",
    "            'Revenue Growth': info.get('revenueGrowth', 'N/A'),\n",
    "            'Profit Margins': info.get('profitMargins', 'N/A'),\n",
    "            'Beta': info.get('beta', 'N/A'),\n",
    "            'Current Ratio': info.get('currentRatio', 'N/A'),\n",
    "            'Book Value': info.get('bookValue', 'N/A'),\n",
    "            '52-Week High': info.get('fiftyTwoWeekHigh', 'N/A'),\n",
    "            '52-Week Low': info.get('fiftyTwoWeekLow', 'N/A'),\n",
    "            'Target Price': info.get('targetMeanPrice', 'N/A')\n",
    "        }\n",
    "        return fundamental_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching fundamental data for {symbol}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef869573",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_strength(data, rsi, macd_line, signal_line):\n",
    "    # Ensure data is not empty\n",
    "    if isinstance(data, pd.DataFrame) and data.empty:\n",
    "        return [], []\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        # Extract the first level columns if multi-index\n",
    "        data_cols = {\n",
    "            'Open': data['Open'].iloc[:, 0],\n",
    "            'High': data['High'].iloc[:, 0],\n",
    "            'Low': data['Low'].iloc[:, 0],\n",
    "            'Close': data['Close'].iloc[:, 0],\n",
    "            'Volume': data['Volume'].iloc[:, 0]\n",
    "        }\n",
    "        data_df = pd.DataFrame(data_cols, index=data.index)\n",
    "    else:\n",
    "        data_df = data\n",
    "\n",
    "    current_price = data_df['Close'].iloc[-1]\n",
    "    sma_50 = data_df['Close'].rolling(window=50).mean().iloc[-1]\n",
    "    sma_200 = data_df['Close'].rolling(window=200).mean().iloc[-1]\n",
    "\n",
    "    # Convert any Series to scalar values\n",
    "    if isinstance(current_price, pd.Series):\n",
    "        current_price = current_price.iloc[0]\n",
    "    if isinstance(sma_50, pd.Series):\n",
    "        sma_50 = sma_50.iloc[0]\n",
    "    if isinstance(sma_200, pd.Series):\n",
    "        sma_200 = sma_200.iloc[0]\n",
    "\n",
    "    strengths = []\n",
    "    weaknesses = []\n",
    "\n",
    "    # RSI Analysis\n",
    "    if not isinstance(rsi, pd.Series) or len(rsi) == 0:\n",
    "        print(\"RSI data is empty\")\n",
    "    else:\n",
    "        last_rsi = rsi.iloc[-1]\n",
    "        if isinstance(last_rsi, pd.Series):\n",
    "            last_rsi = last_rsi.item()\n",
    "        if last_rsi > 70:\n",
    "            weaknesses.append(\"RSI indicates overbought conditions\")\n",
    "        elif last_rsi < 30:\n",
    "            strengths.append(\"RSI indicates oversold conditions (potential buying opportunity)\")\n",
    "        elif 40 <= last_rsi <= 60:\n",
    "            strengths.append(\"RSI in neutral zone showing balance between buyers and sellers\")\n",
    "        elif 60 < last_rsi < 70:\n",
    "            strengths.append(\"Strong RSI showing positive momentum\")\n",
    "\n",
    "    # MACD Analysis\n",
    "    if (not isinstance(macd_line, pd.Series) or len(macd_line) == 0 or\n",
    "        not isinstance(signal_line, pd.Series) or len(signal_line) == 0):\n",
    "        print(\"MACD or signal line data is empty\")\n",
    "    else:\n",
    "        macd_value = macd_line.iloc[-1]\n",
    "        signal_value = signal_line.iloc[-1]\n",
    "        if isinstance(macd_value, pd.Series):\n",
    "            macd_value = macd_value.item()\n",
    "        if isinstance(signal_value, pd.Series):\n",
    "            signal_value = signal_value.item()\n",
    "        if macd_value > signal_value:\n",
    "            strengths.append(\"MACD line above signal line indicating bullish momentum\")\n",
    "        else:\n",
    "            weaknesses.append(\"MACD line below signal line indicating bearish momentum\")\n",
    "\n",
    "    # Moving Average Analysis\n",
    "    if pd.notna(sma_50) and pd.notna(current_price):\n",
    "        if current_price > sma_50:\n",
    "            strengths.append(\"Price above 50-day SMA showing short-term strength\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price below 50-day SMA showing short-term weakness\")\n",
    "\n",
    "    if pd.notna(sma_200) and pd.notna(current_price):\n",
    "        if current_price > sma_200:\n",
    "            strengths.append(\"Price above 200-day SMA suggesting long-term uptrend\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price below 200-day SMA suggesting long-term downtrend\")\n",
    "\n",
    "    # Golden/Death Cross\n",
    "    if pd.notna(sma_50) and pd.notna(sma_200):\n",
    "        if sma_50 > sma_200 and (sma_50 / sma_200 - 1) < 0.03:\n",
    "            strengths.append(\"Recent golden cross or nearing golden cross (50-day SMA crossing above 200-day SMA)\")\n",
    "        elif sma_50 < sma_200 and (sma_200 / sma_50 - 1) < 0.03:\n",
    "            weaknesses.append(\"Recent death cross or nearing death cross (50-day SMA crossing below 200-day SMA)\")\n",
    "\n",
    "    # Volume Analysis\n",
    "    avg_volume = data_df['Volume'].mean()\n",
    "    if isinstance(avg_volume, pd.Series):\n",
    "        avg_volume = avg_volume.item()\n",
    "\n",
    "    recent_volume = data_df['Volume'].iloc[-5:].mean()\n",
    "    if isinstance(recent_volume, pd.Series):\n",
    "        recent_volume = recent_volume.item()\n",
    "\n",
    "    # Calculate price trend\n",
    "    recent_price_mean = data_df['Close'].iloc[-5:].mean()\n",
    "    if isinstance(recent_price_mean, pd.Series):\n",
    "        recent_price_mean = recent_price_mean.item()\n",
    "\n",
    "    price_trend_up = current_price > recent_price_mean\n",
    "\n",
    "    if recent_volume > avg_volume * 1.2:\n",
    "        if price_trend_up:\n",
    "            strengths.append(\"Strong volume supporting upward price movement\")\n",
    "        else:\n",
    "            weaknesses.append(\"High volume during price decline indicates selling pressure\")\n",
    "    elif recent_volume < avg_volume * 0.8:\n",
    "        if price_trend_up:\n",
    "            strengths.append(\"Price rising on low volume - potential weakness\")\n",
    "        else:\n",
    "            weaknesses.append(\"Price declining on low volume - potential for reversal\")\n",
    "\n",
    "    # Price Movement\n",
    "    if len(data_df['Close']) >= 22:\n",
    "        latest_price = data_df['Close'].iloc[-1]\n",
    "        price_22_days_ago = data_df['Close'].iloc[-22]\n",
    "\n",
    "        if isinstance(latest_price, pd.Series):\n",
    "            latest_price = latest_price.item()\n",
    "        if isinstance(price_22_days_ago, pd.Series):\n",
    "            price_22_days_ago = price_22_days_ago.item()\n",
    "\n",
    "        monthly_return = (latest_price / price_22_days_ago - 1) * 100\n",
    "\n",
    "        if monthly_return > 5:\n",
    "            strengths.append(f\"Strong monthly return of {monthly_return:.2f}%\")\n",
    "        elif monthly_return < -5:\n",
    "            weaknesses.append(f\"Weak monthly return of {monthly_return:.2f}%\")\n",
    "\n",
    "    # Volatility\n",
    "    if len(data_df['Close']) >= 2:\n",
    "        returns = data_df['Close'].pct_change().dropna()\n",
    "        if len(returns) > 0:\n",
    "            volatility = returns.std() * np.sqrt(252) * 100\n",
    "            if isinstance(volatility, pd.Series):\n",
    "                volatility = volatility.item()\n",
    "\n",
    "            if volatility > 30:\n",
    "                weaknesses.append(f\"High volatility ({volatility:.2f}%) indicating increased risk\")\n",
    "            elif volatility < 15:\n",
    "                strengths.append(f\"Low volatility ({volatility:.2f}%) indicating stability\")\n",
    "\n",
    "    return strengths, weaknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd54b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekday(date):\n",
    "    \"\"\"Get weekday number (1=Monday to 7=Sunday)\"\"\"\n",
    "    return date.isoweekday()\n",
    "\n",
    "def get_next_friday(target_date):\n",
    "    \"\"\"Get the next Friday from a given date. If the date is Friday, return the same date.\"\"\"\n",
    "    weekday = get_weekday(target_date)\n",
    "    if weekday == 5:  # If Friday\n",
    "        return target_date\n",
    "\n",
    "    days_until_friday = {\n",
    "        1: 4,  # Monday -> +4 days\n",
    "        2: 3,  # Tuesday -> +3 days\n",
    "        3: 2,  # Wednesday -> +2 days\n",
    "        4: 1,  # Thursday -> +1 day\n",
    "        6: 6,  # Saturday -> +6 days\n",
    "        7: 5   # Sunday -> +5 days\n",
    "    }\n",
    "    days_to_add = days_until_friday[weekday]\n",
    "    return target_date + pd.Timedelta(days=days_to_add)\n",
    "\n",
    "def get_next_thursday(target_date):\n",
    "    \"\"\"Get the next Thursday from a given date. If the date is Friday, return the previous day.\"\"\"\n",
    "    weekday = get_weekday(target_date)\n",
    "    if weekday == 5:  # If Friday\n",
    "        return target_date - pd.Timedelta(days=1)\n",
    "    return get_next_friday(target_date) - pd.Timedelta(days=1)\n",
    "\n",
    "def find_closest_trading_day(data, target_date):\n",
    "    \"\"\"Find the closest trading day in the data for a given target date\"\"\"\n",
    "    target_friday = get_next_friday(target_date)\n",
    "    target_thursday = get_next_thursday(target_date)\n",
    "\n",
    "    # Convert index to datetime if it's not already and normalize to remove time component\n",
    "    date_index = pd.to_datetime(data.index).normalize()\n",
    "\n",
    "    # Normalize target dates to remove time component\n",
    "    target_friday = pd.to_datetime(target_friday).normalize()\n",
    "    target_thursday = pd.to_datetime(target_thursday).normalize()\n",
    "\n",
    "    # Try to find exact matches first\n",
    "    friday_match = date_index[date_index == target_friday]\n",
    "    thursday_match = date_index[date_index == target_thursday]\n",
    "\n",
    "    if not friday_match.empty:\n",
    "        return friday_match[0]\n",
    "    elif not thursday_match.empty:\n",
    "        return thursday_match[0]\n",
    "\n",
    "    # If no exact match, find the closest available date\n",
    "    all_dates = date_index\n",
    "    closest_date = min(all_dates, key=lambda x: abs(x - target_friday))\n",
    "\n",
    "    return closest_date\n",
    "\n",
    "def get_date_one_year_back(data, from_date=None):\n",
    "    \"\"\"Calculate the date from the given date (or last date in stock data) to 1 year back\"\"\"\n",
    "    if from_date:\n",
    "        last_date = pd.to_datetime(from_date).normalize()\n",
    "    else:\n",
    "        last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "    target_date = last_date - pd.DateOffset(months=12)\n",
    "    return find_closest_trading_day(data, target_date)\n",
    "\n",
    "def get_date_six_months_back(data, from_date=None):\n",
    "    \"\"\"Calculate the date from the given date (or last date in stock data) to 6 months back\"\"\"\n",
    "    if from_date:\n",
    "        last_date = pd.to_datetime(from_date).normalize()\n",
    "    else:\n",
    "        last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "    target_date = last_date - pd.DateOffset(months=6)\n",
    "    return find_closest_trading_day(data, target_date)\n",
    "\n",
    "def get_first_available_date(data):\n",
    "    \"\"\"Get the first date for which price data is available\"\"\"\n",
    "    return pd.to_datetime(data.index[0]).normalize()\n",
    "\n",
    "def get_prices_for_dates(data, end_date=None):\n",
    "    \"\"\"Get prices for specified date (or current date), 6 months back and 1 year back, and calculate price changes.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing price data and changes for different time periods\n",
    "    \"\"\"\n",
    "    # Get the actual last date from data\n",
    "    actual_last_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    # If end_date is provided but different from actual_last_date, use actual_last_date\n",
    "    if end_date:\n",
    "        requested_date = pd.to_datetime(end_date).normalize()\n",
    "        if requested_date != actual_last_date:\n",
    "            logger.info(f\"Using last available date {actual_last_date.strftime('%Y-%m-%d')} instead of requested date {requested_date.strftime('%Y-%m-%d')}\")\n",
    "            current_date = actual_last_date\n",
    "        else:\n",
    "            current_date = requested_date\n",
    "    else:\n",
    "        current_date = actual_last_date\n",
    "\n",
    "    six_month_date = get_date_six_months_back(data, current_date)\n",
    "    one_year_date = get_date_one_year_back(data, current_date)\n",
    "    first_available_date = get_first_available_date(data)\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Get current price (should always be available since we're using actual last date)\n",
    "    current_price = close_prices[current_date]\n",
    "\n",
    "    # Get 6-month price if available, otherwise use first available date\n",
    "    try:\n",
    "        six_month_price = close_prices[six_month_date]\n",
    "        six_month_actual_date = six_month_date\n",
    "    except KeyError:\n",
    "        six_month_price = close_prices[first_available_date]\n",
    "        six_month_actual_date = first_available_date\n",
    "\n",
    "    # Get 1-year price if available, otherwise use first available date\n",
    "    try:\n",
    "        one_year_price = close_prices[one_year_date]\n",
    "        one_year_actual_date = one_year_date\n",
    "    except KeyError:\n",
    "        one_year_price = close_prices[first_available_date]\n",
    "        one_year_actual_date = first_available_date\n",
    "\n",
    "    # Calculate price changes\n",
    "    twelve_month_change = (current_price / one_year_price - 1) if one_year_price != 0 else 0\n",
    "    six_month_change = (current_price / six_month_price - 1) if six_month_price != 0 else 0\n",
    "\n",
    "    # Create a dictionary with dates, prices and price changes\n",
    "    price_data = {\n",
    "        'Current': {\n",
    "            'date': current_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(current_price, 2)\n",
    "        },\n",
    "        '6_months_back': {\n",
    "            'date': six_month_actual_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(six_month_price, 2),\n",
    "            'price_change_percent': round(six_month_change * 100, 2),\n",
    "            'is_first_available': six_month_actual_date == first_available_date\n",
    "        },\n",
    "        '1_year_back': {\n",
    "            'date': one_year_actual_date.strftime('%Y-%m-%d'),\n",
    "            'price': round(one_year_price, 2),\n",
    "            'price_change_percent': round(twelve_month_change * 100, 2),\n",
    "            'is_first_available': one_year_actual_date == first_available_date\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_returns(data, end_date=None):\n",
    "    \"\"\"Calculate logarithmic returns between consecutive trading days for the past year\n",
    "    Returns the log returns as percentages along with the prices used in calculations.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (log_returns, price_data)\n",
    "    \"\"\"\n",
    "    # Get the dates for 1 year period\n",
    "    if end_date:\n",
    "        current_date = pd.to_datetime(end_date).normalize()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    one_year_back_date = get_date_one_year_back(data, end_date)\n",
    "\n",
    "    # Handle multi-index columns if they exist\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        close_prices = data['Close'].iloc[:, 0]\n",
    "    else:\n",
    "        close_prices = data['Close']\n",
    "\n",
    "    # Get the slice of data between one year back and current date\n",
    "    mask = (data.index >= one_year_back_date) & (data.index <= current_date)\n",
    "    period_prices = close_prices[mask]\n",
    "\n",
    "    # Create a DataFrame to store prices and calculations\n",
    "    price_data = pd.DataFrame({\n",
    "        'Current_Price': period_prices.astype(float),\n",
    "        'Previous_Price': period_prices.shift(1).astype(float)\n",
    "    })\n",
    "\n",
    "    # Calculate price ratios first (current/previous)\n",
    "    price_data['Price_Ratio'] = price_data['Current_Price'] / price_data['Previous_Price']\n",
    "\n",
    "    # Calculate log returns exactly as Excel does: LN(current/previous)\n",
    "    price_data['Log_Return_Percent'] = np.log(price_data['Price_Ratio']) * 100\n",
    "\n",
    "    # Round to 3 decimal places after all calculations\n",
    "    price_data['Log_Return_Percent'] = price_data['Log_Return_Percent'].round(3)\n",
    "\n",
    "    # Round other columns to 2 decimal places\n",
    "    for column in ['Current_Price', 'Previous_Price', 'Price_Ratio']:\n",
    "        price_data[column] = price_data[column].round(2)\n",
    "\n",
    "    # Drop the first row as it will have NaN values due to the shift operation\n",
    "    price_data = price_data.dropna()\n",
    "\n",
    "    # Create the log returns series\n",
    "    log_returns = price_data['Log_Return_Percent']\n",
    "\n",
    "    return log_returns, price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_stdev(data, end_date=None):\n",
    "    \"\"\"Calculate standard deviation using all log values from last 1 year till the specified date.\n",
    "    Similar to Excel's STDEV(A:B) where A:B would be the range of log values.\n",
    "    Returns the standard deviation divided by 100.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        tuple: (one_year_stdev, six_month_stdev) where values are divided by 100\n",
    "    \"\"\"\n",
    "    # Get the dates\n",
    "    if end_date:\n",
    "        current_date = pd.to_datetime(end_date).normalize()\n",
    "    else:\n",
    "        current_date = pd.to_datetime(data.index[-1]).normalize()\n",
    "\n",
    "    six_month_date = get_date_six_months_back(data, end_date)\n",
    "    one_year_date = get_date_one_year_back(data, end_date)\n",
    "\n",
    "    # Get log returns for the entire period\n",
    "    log_returns, _ = calculate_log_returns(data, end_date)\n",
    "\n",
    "    try:\n",
    "        # Get all log values from one year ago to current date\n",
    "        one_year_mask = (log_returns.index >= one_year_date) & (log_returns.index <= current_date)\n",
    "        one_year_logs = log_returns[one_year_mask]\n",
    "        # Calculate standard deviation using all values in the period and divide by 100\n",
    "        one_year_stdev = np.std(one_year_logs) / 100\n",
    "    except KeyError:\n",
    "        one_year_stdev = None\n",
    "\n",
    "    try:\n",
    "        # Get all log values from six months ago to current date\n",
    "        six_month_mask = (log_returns.index >= six_month_date) & (log_returns.index <= current_date)\n",
    "        six_month_logs = log_returns[six_month_mask]\n",
    "        # Calculate standard deviation using all values in the period and divide by 100\n",
    "        six_month_stdev = np.std(six_month_logs) / 100\n",
    "    except KeyError:\n",
    "        six_month_stdev = None\n",
    "\n",
    "    return one_year_stdev, six_month_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7fa245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_momentum_ratio(data, end_date=None):\n",
    "    \"\"\"Calculate momentum ratios for 1 year and 6 months periods.\n",
    "    Momentum ratio = Price change percentage / (Standard deviation * 100)\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Stock price data with 'Close' prices\n",
    "        end_date (str): End date in 'YYYY-MM-DD' format. If None, uses last date in data\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing momentum ratios for both periods\n",
    "    \"\"\"\n",
    "    # Get price changes\n",
    "    prices_data = get_prices_for_dates(data, end_date)\n",
    "    one_year_change = prices_data['1_year_back']['price_change_percent']\n",
    "    six_month_change = prices_data['6_months_back']['price_change_percent']\n",
    "\n",
    "    # Get standard deviations (already divided by 100 in calculate_log_stdev)\n",
    "    one_year_stdev, six_month_stdev = calculate_log_stdev(data, end_date)\n",
    "\n",
    "    # Calculate momentum ratios\n",
    "    # Note: We multiply stdev by 100 to match the scale of price_change_percent\n",
    "    one_year_momentum = one_year_change / (one_year_stdev * 100) if one_year_stdev else None\n",
    "    six_month_momentum = six_month_change / (six_month_stdev * 100) if six_month_stdev else None\n",
    "\n",
    "    result = {\n",
    "        'momentum_ratios': {\n",
    "            'one_year': round(one_year_momentum, 4) if one_year_momentum is not None else None,\n",
    "            'six_month': round(six_month_momentum, 4) if six_month_momentum is not None else None\n",
    "        },\n",
    "        'components': {\n",
    "            'one_year': {\n",
    "                'price_change_percent': one_year_change,\n",
    "                'standard_deviation': one_year_stdev * 100 if one_year_stdev else None\n",
    "            },\n",
    "            'six_month': {\n",
    "                'price_change_percent': six_month_change,\n",
    "                'standard_deviation': six_month_stdev * 100 if six_month_stdev else None\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a52051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_universe_stats(momentum_values):\n",
    "    \"\"\"Calculate standard deviation and mean for a list of momentum ratios.\n",
    "    Similar to Excel's STDEV and AVERAGE functions.\n",
    "\n",
    "    Args:\n",
    "        momentum_values: List of momentum ratio values\n",
    "\n",
    "    Returns:\n",
    "        tuple: (standard_deviation, mean)\n",
    "    \"\"\"\n",
    "    # Remove None and NaN values\n",
    "    valid_values = [v for v in momentum_values if v is not None and not np.isnan(v)]\n",
    "\n",
    "    if not valid_values:\n",
    "        return None, None\n",
    "\n",
    "    # Calculate standard deviation (similar to Excel's STDEV)\n",
    "    stdev = np.std(valid_values, ddof=1)  # ddof=1 for sample standard deviation (like Excel)\n",
    "\n",
    "    # Calculate mean (similar to Excel's AVERAGE)\n",
    "    mean = np.mean(valid_values)\n",
    "\n",
    "    return stdev, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecd5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_z_score(value, mean, stdev):\n",
    "    \"\"\"Calculate z-score for a value.\n",
    "\n",
    "    Z-score = (value - mean) / standard_deviation\n",
    "\n",
    "    Args:\n",
    "        value: The value to calculate z-score for\n",
    "        mean: Mean of the universe\n",
    "        stdev: Standard deviation of the universe\n",
    "\n",
    "    Returns:\n",
    "        float: z-score or None if inputs are invalid\n",
    "    \"\"\"\n",
    "    if value is None or mean is None or stdev is None or stdev == 0:\n",
    "        return None\n",
    "\n",
    "    return (value - mean) / stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aad1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_z_score(one_year_z_score, six_month_z_score, weights=None):\n",
    "    \"\"\"Calculate weighted z-score from 1-year and 6-month z-scores.\n",
    "\n",
    "    Args:\n",
    "        one_year_z_score: Z-score for 1-year momentum ratio\n",
    "        six_month_z_score: Z-score for 6-month momentum ratio\n",
    "        weights: Dictionary with weights for each period.\n",
    "                Default is {'one_year': 0.5, 'six_month': 0.5}\n",
    "\n",
    "    Returns:\n",
    "        float: Weighted z-score or None if inputs are invalid\n",
    "    \"\"\"\n",
    "    if one_year_z_score is None or six_month_z_score is None:\n",
    "        return None\n",
    "\n",
    "    # Use default weights if none provided\n",
    "    if weights is None:\n",
    "        weights = {'one_year': 0.5, 'six_month': 0.5}\n",
    "\n",
    "    weighted_z_score = (\n",
    "        one_year_z_score * weights['one_year'] +\n",
    "        six_month_z_score * weights['six_month']\n",
    "    )\n",
    "\n",
    "    return weighted_z_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_normalized_z_score(z_score):\n",
    "    \"\"\"Calculate normalized z-score using the formula: IF(z_score>0,(1+z_score),(1-z_score)^-1)\n",
    "\n",
    "    Args:\n",
    "        z_score: The z-score to normalize\n",
    "\n",
    "    Returns:\n",
    "        float: Normalized z-score or None if input is invalid\n",
    "    \"\"\"\n",
    "    if z_score is None:\n",
    "        return None\n",
    "\n",
    "    return (1 + z_score) if z_score > 0 else (1 - z_score) ** -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e66abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rank(value, values_list):\n",
    "    \"\"\"Calculate the rank of a value within a list. Similar to Excel's RANK(number, ref, [order])\n",
    "\n",
    "    Args:\n",
    "        value: The number whose rank you want to find\n",
    "        values_list: A list of numbers that defines the relative ranking\n",
    "\n",
    "    Returns:\n",
    "        int: The rank of the number (1 being the highest)\n",
    "    \"\"\"\n",
    "    # Remove None and NaN values from the list\n",
    "    valid_values = [v for v in values_list if v is not None and not np.isnan(v)]\n",
    "\n",
    "    if value is None or np.isnan(value) or not valid_values:\n",
    "        return None\n",
    "\n",
    "    # Sort values in descending order (mimicking Excel's RANK with order=0)\n",
    "    sorted_values = sorted(set(valid_values), reverse=True)\n",
    "\n",
    "    try:\n",
    "        # Find the rank (adding 1 because index starts at 0)\n",
    "        rank = sorted_values.index(value) + 1\n",
    "        return rank\n",
    "    except ValueError:\n",
    "        # Try to find closest match for floating point comparison issues\n",
    "        closest_idx = min(range(len(sorted_values)),\n",
    "                         key=lambda i: abs(sorted_values[i] - value))\n",
    "        return closest_idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d425fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rank_analysis_markdown(stock_data, symbol, one_year_rank=None, six_month_rank=None, total_stocks=None):\n",
    "    \"\"\"Generate markdown documentation for RANK analysis calculations.\n",
    "\n",
    "    This function creates detailed markdown explaining all calculations used in the analysis.\n",
    "    \"\"\"\n",
    "    md_content = \"## RANK Analysis\\n\\n\"\n",
    "\n",
    "    # Date Calculations Section\n",
    "    md_content += \"### Date Reference Points\\n\\n\"\n",
    "    current_date = stock_data.index[-1].strftime('%Y-%m-%d')\n",
    "    one_year_back_date = get_date_one_year_back(stock_data)\n",
    "    six_month_back_date = get_date_six_months_back(stock_data)\n",
    "\n",
    "    md_content += f\"- **Current Date:** {current_date}\\n\"\n",
    "    md_content += f\"- **1-Year Reference Date:** {one_year_back_date.strftime('%Y-%m-%d')}\\n\"\n",
    "    md_content += f\"- **6-Month Reference Date:** {six_month_back_date.strftime('%Y-%m-%d')}\\n\\n\"\n",
    "\n",
    "    # Price Changes Section\n",
    "    md_content += \"### Price Changes\\n\\n\"\n",
    "    current_price = stock_data['Close'].iloc[-1]\n",
    "    year_start_price = stock_data.loc[one_year_back_date, 'Close']\n",
    "    six_month_start_price = stock_data.loc[six_month_back_date, 'Close']\n",
    "\n",
    "    year_change = (current_price / year_start_price - 1) * 100\n",
    "    six_month_change = (current_price / six_month_start_price - 1) * 100\n",
    "\n",
    "    md_content += f\"**1-Year Price Change:**\\n\"\n",
    "    md_content += f\"- Start Price: ₹{year_start_price:.2f}\\n\"\n",
    "    md_content += f\"- Current Price: ₹{current_price:.2f}\\n\"\n",
    "    md_content += f\"- Change: {year_change:.2f}%\\n\\n\"\n",
    "\n",
    "    md_content += f\"**6-Month Price Change:**\\n\"\n",
    "    md_content += f\"- Start Price: ₹{six_month_start_price:.2f}\\n\"\n",
    "    md_content += f\"- Current Price: ₹{current_price:.2f}\\n\"\n",
    "    md_content += f\"- Change: {six_month_change:.2f}%\\n\\n\"\n",
    "\n",
    "    # Log Returns Section\n",
    "    md_content += \"### Log Returns and Standard Deviation\\n\\n\"\n",
    "    log_returns, price_data = calculate_log_returns(stock_data)\n",
    "    one_year_stdev, six_month_stdev = calculate_log_stdev(stock_data)\n",
    "\n",
    "    md_content += \"**Log Returns Calculation:**\\n\"\n",
    "    md_content += \"- Formula: `ln(current_price / previous_price) * 100`\\n\"\n",
    "    md_content += f\"- Recent Log Returns (last 5 days):\\n\"\n",
    "    for date, row in price_data.tail().iterrows():\n",
    "        md_content += f\"  * {date.strftime('%Y-%m-%d')}: {row['Log_Return_Percent']:.3f}%\\n\"\n",
    "\n",
    "    md_content += \"\\n**Standard Deviation:**\\n\"\n",
    "    md_content += f\"- 1-Year σ: {one_year_stdev*100:.3f}%\\n\"\n",
    "    md_content += f\"- 6-Month σ: {six_month_stdev*100:.3f}%\\n\\n\"\n",
    "\n",
    "    # Momentum Ratio Section\n",
    "    md_content += \"### Momentum Ratio\\n\\n\"\n",
    "    momentum_ratio = calculate_momentum_ratio(stock_data)\n",
    "\n",
    "    md_content += \"**Momentum Ratio Formula:** Price Change % / (Standard Deviation * 100)\\n\\n\"\n",
    "\n",
    "    md_content += \"**1-Year Momentum Components:**\\n\"\n",
    "    md_content += f\"- Price Change: {momentum_ratio['components']['one_year']['price_change_percent']:.2f}%\\n\"\n",
    "    md_content += f\"- Standard Deviation: {momentum_ratio['components']['one_year']['standard_deviation']:.2f}%\\n\"\n",
    "    md_content += f\"- Momentum Ratio: {momentum_ratio['momentum_ratios']['one_year']:.4f}\\n\"\n",
    "    if one_year_rank is not None and total_stocks is not None:\n",
    "        md_content += f\"- Rank: {one_year_rank} out of {total_stocks}\\n\\n\"\n",
    "\n",
    "    md_content += \"**6-Month Momentum Components:**\\n\"\n",
    "    md_content += f\"- Price Change: {momentum_ratio['components']['six_month']['price_change_percent']:.2f}%\\n\"\n",
    "    md_content += f\"- Standard Deviation: {momentum_ratio['components']['six_month']['standard_deviation']:.2f}%\\n\"\n",
    "    md_content += f\"- Momentum Ratio: {momentum_ratio['momentum_ratios']['six_month']:.4f}\\n\"\n",
    "    if six_month_rank is not None and total_stocks is not None:\n",
    "        md_content += f\"- Rank: {six_month_rank} out of {total_stocks}\\n\\n\"\n",
    "\n",
    "    return md_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e75103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_stocks(stocks_df, output_dir=\"stock_analysis\", max_workers=5, end_date=None):\n",
    "    \"\"\"Analyze a list of stocks and calculate momentum metrics for each.\n",
    "\n",
    "    Args:\n",
    "        stocks_df: List of tuples (company_name, symbol)\n",
    "        output_dir: Directory to store analysis results\n",
    "        max_workers: Maximum number of parallel workers\n",
    "        end_date: End date for analysis in YYYY-MM-DD format\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    results = {}\n",
    "\n",
    "    # Global rank data structure to store universe-wide statistics\n",
    "    global rank_data\n",
    "    rank_data = {\n",
    "        'one_year': {'values': [], 'ranks': {}, 'universe_stats': {'stdev': None, 'mean': None}},\n",
    "        'six_month': {'values': [], 'ranks': {}, 'universe_stats': {'stdev': None, 'mean': None}},\n",
    "        'weighted_z_score': {'values': [], 'ranks': {}},\n",
    "        'normalized_z_score': {'values': [], 'ranks': {}}\n",
    "    }\n",
    "\n",
    "    def process_stock(stock_tuple):\n",
    "        company_name, symbol = stock_tuple\n",
    "        logger.info(f\"Processing {company_name} ({symbol})\")\n",
    "\n",
    "        try:\n",
    "            # Get stock data with actual end date\n",
    "            data, actual_end_date = get_stock_data(symbol, period='2y', end_date=end_date)\n",
    "            if data.empty:\n",
    "                logger.warning(f\"No data available for {symbol}\")\n",
    "                return None\n",
    "\n",
    "            # Use actual end date for all calculations\n",
    "            actual_end_date_str = actual_end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "            # Calculate technical indicators\n",
    "            rsi = calculate_rsi(data)\n",
    "            macd_line, signal_line, histogram = calculate_macd(data)\n",
    "            momentum_period20 = calculate_momentum(data, period=20)\n",
    "            momentum_index = calculate_momentum_index(data)\n",
    "            momentum_ratio = calculate_momentum_ratio(data, actual_end_date_str)\n",
    "\n",
    "            # Get price data using actual end date\n",
    "            prices = get_prices_for_dates(data, actual_end_date_str)\n",
    "\n",
    "            # Determine strengths and weaknesses\n",
    "            strengths, weaknesses = determine_strength(data, rsi, macd_line, signal_line)\n",
    "\n",
    "            # Store the result with all necessary data\n",
    "            result = {\n",
    "                'company_name': company_name,\n",
    "                'symbol': symbol,\n",
    "                'actual_end_date': actual_end_date_str,\n",
    "                'current_price': prices['Current']['price'],\n",
    "                'price_data': prices,\n",
    "                'technical_indicators': {\n",
    "                    'rsi': rsi.iloc[-1] if not rsi.empty else None,\n",
    "                    'macd': {\n",
    "                        'macd_line': macd_line.iloc[-1] if not macd_line.empty else None,\n",
    "                        'signal_line': signal_line.iloc[-1] if not signal_line.empty else None,\n",
    "                        'histogram': histogram.iloc[-1] if not histogram.empty else None\n",
    "                    },\n",
    "                    'momentum': {\n",
    "                        'momentum_20day': momentum_period20.iloc[-1] if not momentum_period20.empty else None,\n",
    "                        'momentum_index': momentum_index.iloc[-1] if not momentum_index.empty else None,\n",
    "                        'momentum_ratio': momentum_ratio\n",
    "                    }\n",
    "                },\n",
    "                'analysis': {\n",
    "                    'strengths': strengths,\n",
    "                    'weaknesses': weaknesses\n",
    "                },\n",
    "                'data': data\n",
    "            }\n",
    "\n",
    "            # Store momentum values for universe calculations\n",
    "            one_year_momentum = momentum_ratio['momentum_ratios']['one_year']\n",
    "            six_month_momentum = momentum_ratio['momentum_ratios']['six_month']\n",
    "\n",
    "            if one_year_momentum is not None:\n",
    "                rank_data['one_year']['values'].append(one_year_momentum)\n",
    "            if six_month_momentum is not None:\n",
    "                rank_data['six_month']['values'].append(six_month_momentum)\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {symbol}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Process stocks in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_stock, stock): stock for stock in stocks_df}\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            stock = futures[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    results[result['symbol']] = result\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing {stock}: {e}\")\n",
    "\n",
    "    # Calculate universe statistics for both periods\n",
    "    for period in ['one_year', 'six_month']:\n",
    "        values = rank_data[period]['values']\n",
    "        stdev, mean = calculate_universe_stats(values)\n",
    "        rank_data[period]['universe_stats']['stdev'] = stdev\n",
    "        rank_data[period]['universe_stats']['mean'] = mean\n",
    "\n",
    "        # Calculate ranks and z-scores\n",
    "        for symbol, result in results.items():\n",
    "            momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios'][period]\n",
    "            if momentum is not None:\n",
    "                rank = calculate_rank(momentum, values)\n",
    "                z_score = calculate_z_score(momentum, mean, stdev)\n",
    "                rank_data[period]['ranks'][symbol] = rank\n",
    "\n",
    "                # Update the result with rank and z-score information\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['ranks'] = result['technical_indicators']['momentum']['momentum_ratio'].get('ranks', {})\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['ranks'][period] = rank\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['z_scores'] = result['technical_indicators']['momentum']['momentum_ratio'].get('z_scores', {})\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['z_scores'][period] = z_score\n",
    "                result['technical_indicators']['momentum']['momentum_ratio']['universe_stats'] = {\n",
    "                    'stdev': stdev,\n",
    "                    'mean': mean\n",
    "                }\n",
    "\n",
    "    # Calculate weighted z-score ranks and normalized z-scores\n",
    "    weighted_z_scores = {}\n",
    "    normalized_z_scores = {}\n",
    "    for symbol, result in results.items():\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "        weighted_z_score = calculate_weighted_z_score(one_year_z_score, six_month_z_score)\n",
    "\n",
    "        if weighted_z_score is not None:\n",
    "            weighted_z_scores[symbol] = weighted_z_score\n",
    "            rank_data['weighted_z_score']['values'].append(weighted_z_score)\n",
    "\n",
    "            # Calculate normalized z-score\n",
    "            normalized_z_score = calculate_normalized_z_score(weighted_z_score)\n",
    "            if normalized_z_score is not None:\n",
    "                normalized_z_scores[symbol] = normalized_z_score\n",
    "                rank_data['normalized_z_score']['values'].append(normalized_z_score)\n",
    "\n",
    "    # Calculate ranks for weighted z-scores\n",
    "    rank_data['weighted_z_score']['values'] = [v for v in rank_data['weighted_z_score']['values'] if v is not None and not np.isnan(v)]\n",
    "    sorted_weighted_z = sorted(rank_data['weighted_z_score']['values'], reverse=True)\n",
    "    for symbol, weighted_z_score in weighted_z_scores.items():\n",
    "        if weighted_z_score is not None and not np.isnan(weighted_z_score) and sorted_weighted_z:\n",
    "            try:\n",
    "                rank = sorted_weighted_z.index(weighted_z_score) + 1\n",
    "            except ValueError:\n",
    "                # Find the closest value if exact match not found due to floating point issues\n",
    "                closest_idx = min(range(len(sorted_weighted_z)),\n",
    "                                key=lambda i: abs(sorted_weighted_z[i] - weighted_z_score))\n",
    "                rank = closest_idx + 1\n",
    "\n",
    "            rank_data['weighted_z_score']['ranks'][symbol] = rank\n",
    "            results[symbol]['technical_indicators']['momentum']['momentum_ratio']['weighted_z_score'] = {\n",
    "                'score': weighted_z_score,\n",
    "                'rank': rank,\n",
    "                'total_stocks': len(sorted_weighted_z)  # Use count of valid values\n",
    "            }\n",
    "\n",
    "    # Calculate ranks for normalized z-scores\n",
    "    rank_data['normalized_z_score']['values'] = [v for v in rank_data['normalized_z_score']['values'] if v is not None and not np.isnan(v)]\n",
    "    sorted_normalized_z = sorted(rank_data['normalized_z_score']['values'], reverse=True)\n",
    "    for symbol, normalized_z_score in normalized_z_scores.items():\n",
    "        if normalized_z_score is not None and not np.isnan(normalized_z_score) and sorted_normalized_z:\n",
    "            try:\n",
    "                rank = sorted_normalized_z.index(normalized_z_score) + 1\n",
    "            except ValueError:\n",
    "                # Find the closest value if exact match not found due to floating point issues\n",
    "                closest_idx = min(range(len(sorted_normalized_z)),\n",
    "                                key=lambda i: abs(sorted_normalized_z[i] - normalized_z_score))\n",
    "                rank = closest_idx + 1\n",
    "\n",
    "            rank_data['normalized_z_score']['ranks'][symbol] = rank\n",
    "            results[symbol]['technical_indicators']['momentum']['momentum_ratio']['normalized_z_score'] = {\n",
    "                'score': normalized_z_score,\n",
    "                'rank': rank,\n",
    "                'total_stocks': len(sorted_normalized_z)  # Use count of valid values\n",
    "            }\n",
    "\n",
    "    # Generate markdown files\n",
    "    for symbol, result in results.items():\n",
    "        company_name = result['company_name']\n",
    "\n",
    "        # Generate rank analysis markdown\n",
    "        rank_analysis_md = generate_rank_analysis_markdown(\n",
    "            result['data'],\n",
    "            symbol,\n",
    "            one_year_rank=rank_data['one_year']['ranks'].get(symbol),\n",
    "            six_month_rank=rank_data['six_month']['ranks'].get(symbol),\n",
    "            total_stocks=len(results)\n",
    "        )\n",
    "\n",
    "        # Create comprehensive markdown content\n",
    "        md_content = f\"# {company_name} ({symbol}) Analysis\\n\\n\"\n",
    "\n",
    "        md_content += f\"Company Name (ticker): {symbol}\\n\\n\"\n",
    "\n",
    "        # Extract the category/universe from the output_dir path\n",
    "        category = output_dir.split('/')[-1]  # This will extract \"nifty_50\", \"midcap_150\", etc.\n",
    "        # Format the category for display - remove underscore and capitalize\n",
    "        display_category = category.replace('_', ' ').title()\n",
    "        md_content += f\"Universe/Category: {display_category}\\n\\n\"\n",
    "\n",
    "        # Add actual date used for analysis\n",
    "        if end_date and result['actual_end_date'] != end_date:\n",
    "            md_content += f\"*Note: Analysis uses last available date {result['actual_end_date']} instead of requested date {end_date}*\\n\\n\"\n",
    "\n",
    "        # Current Price and Performance\n",
    "        md_content += \"## Current Price and Performance\\n\\n\"\n",
    "        md_content += f\"Current Price: Rs.{result['price_data']['Current']['price']:.2f}\\n\"\n",
    "        if '1_year_back' in result['price_data']:\n",
    "            md_content += f\"1-Year Change: {result['price_data']['1_year_back']['price_change_percent']:.2f}%\\n\"\n",
    "        if '6_months_back' in result['price_data']:\n",
    "            md_content += f\"6-Month Change: {result['price_data']['6_months_back']['price_change_percent']:.2f}%\\n\\n\"\n",
    "\n",
    "        # Technical Indicators\n",
    "        md_content += \"## Technical Indicators\\n\\n\"\n",
    "        if result['technical_indicators']['rsi'] is not None:\n",
    "            md_content += f\"RSI (14-day): {float(result['technical_indicators']['rsi']):.2f}\\n\"\n",
    "        if result['technical_indicators']['macd']['macd_line'] is not None:\n",
    "            md_content += f\"MACD Line: {float(result['technical_indicators']['macd']['macd_line']):.2f}\\n\"\n",
    "            md_content += f\"Signal Line: {float(result['technical_indicators']['macd']['signal_line']):.2f}\\n\"\n",
    "            md_content += f\"MACD Histogram: {float(result['technical_indicators']['macd']['histogram']):.2f}\\n\"\n",
    "\n",
    "        # Momentum Rankings and Universe Statistics\n",
    "        md_content += \"\\n## Momentum Rankings and Universe Statistics\\n\\n\"\n",
    "\n",
    "        # 1-Year Stats\n",
    "        md_content += \"### 1-Year Momentum\\n\"\n",
    "        one_year_rank = rank_data['one_year']['ranks'].get(symbol)\n",
    "        one_year_momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios']['one_year']\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "\n",
    "        if one_year_rank and one_year_momentum:\n",
    "            md_content += f\"- Rank: {one_year_rank} out of {len(results)}\\n\"\n",
    "            md_content += f\"- Momentum Ratio: {one_year_momentum:.4f}\\n\"\n",
    "            md_content += f\"- Universe Mean: {rank_data['one_year']['universe_stats']['mean']:.4f}\\n\"\n",
    "            md_content += f\"- Universe StDev: {rank_data['one_year']['universe_stats']['stdev']:.4f}\\n\"\n",
    "            if one_year_z_score is not None:\n",
    "                md_content += f\"- Z-Score: {one_year_z_score:.4f}\\n\"\n",
    "\n",
    "        # 6-Month Stats\n",
    "        md_content += \"\\n### 6-Month Momentum\\n\"\n",
    "        six_month_rank = rank_data['six_month']['ranks'].get(symbol)\n",
    "        six_month_momentum = result['technical_indicators']['momentum']['momentum_ratio']['momentum_ratios']['six_month']\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "\n",
    "        if six_month_rank and six_month_momentum:\n",
    "            md_content += f\"- Rank: {six_month_rank} out of {len(results)}\\n\"\n",
    "            md_content += f\"- Momentum Ratio: {six_month_momentum:.4f}\\n\"\n",
    "            md_content += f\"- Universe Mean: {rank_data['six_month']['universe_stats']['mean']:.4f}\\n\"\n",
    "            md_content += f\"- Universe StDev: {rank_data['six_month']['universe_stats']['stdev']:.4f}\\n\"\n",
    "            if six_month_z_score is not None:\n",
    "                md_content += f\"- Z-Score: {six_month_z_score:.4f}\\n\"\n",
    "\n",
    "        # Add dedicated Rank and Z-Score section\n",
    "        one_year_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('one_year')\n",
    "        six_month_z_score = result['technical_indicators']['momentum']['momentum_ratio']['z_scores'].get('six_month')\n",
    "        weighted_z_score_data = result['technical_indicators']['momentum']['momentum_ratio'].get('weighted_z_score', {})\n",
    "        normalized_z_score_data = result['technical_indicators']['momentum']['momentum_ratio'].get('normalized_z_score', {})\n",
    "\n",
    "        md_content += \"\\n## Rank and Z-Score Analysis\\n\\n\"\n",
    "        md_content += \"### Z-Scores\\n\"\n",
    "        if one_year_z_score is not None:\n",
    "            md_content += f\"- 1-Year Z-Score: {one_year_z_score:.4f}\\n\"\n",
    "        if six_month_z_score is not None:\n",
    "            md_content += f\"- 6-Month Z-Score: {six_month_z_score:.4f}\\n\"\n",
    "        if weighted_z_score_data.get('score') is not None:\n",
    "            md_content += f\"- Weighted Z-Score: {weighted_z_score_data['score']:.4f}\\n\"\n",
    "            md_content += \"  *(Calculated as: 1-Year Z-Score × 0.5 + 6-Month Z-Score × 0.5)*\\n\"\n",
    "        if normalized_z_score_data.get('score') is not None:\n",
    "            md_content += f\"- Normalized Z-Score: {normalized_z_score_data['score']:.4f}\\n\"\n",
    "            md_content += \"  *(Calculated as: IF(weighted_z_score>0, (1+weighted_z_score), (1-weighted_z_score)^-1))*\\n\"\n",
    "\n",
    "        md_content += \"\\n### Rankings\\n\"\n",
    "        if one_year_rank:\n",
    "            md_content += f\"- 1-Year Momentum Rank: {one_year_rank} out of {len(results)}\\n\"\n",
    "        if six_month_rank:\n",
    "            md_content += f\"- 6-Month Momentum Rank: {six_month_rank} out of {len(results)}\\n\"\n",
    "        if weighted_z_score_data.get('rank') is not None:\n",
    "            md_content += f\"- Weighted Z-Score Rank: {weighted_z_score_data['rank']} out of {weighted_z_score_data['total_stocks']}\\n\"\n",
    "        if normalized_z_score_data.get('rank') is not None:\n",
    "            md_content += f\"- Normalized Z-Score Rank: {normalized_z_score_data['rank']} out of {normalized_z_score_data['total_stocks']}\\n\"\n",
    "\n",
    "        # Add universe statistics only if they exist\n",
    "        md_content += \"\\n### Universe Statistics\\n\"\n",
    "        md_content += \"**1-Year Momentum:**\\n\"\n",
    "        if rank_data['one_year']['universe_stats']['mean'] is not None and not np.isnan(rank_data['one_year']['universe_stats']['mean']):\n",
    "            md_content += f\"- Mean: {rank_data['one_year']['universe_stats']['mean']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Mean: Not available\\n\"\n",
    "\n",
    "        if rank_data['one_year']['universe_stats']['stdev'] is not None and not np.isnan(rank_data['one_year']['universe_stats']['stdev']):\n",
    "            md_content += f\"- Standard Deviation: {rank_data['one_year']['universe_stats']['stdev']:.4f}\\n\\n\"\n",
    "        else:\n",
    "            md_content += \"- Standard Deviation: Not available\\n\\n\"\n",
    "\n",
    "        md_content += \"**6-Month Momentum:**\\n\"\n",
    "        if rank_data['six_month']['universe_stats']['mean'] is not None and not np.isnan(rank_data['six_month']['universe_stats']['mean']):\n",
    "            md_content += f\"- Mean: {rank_data['six_month']['universe_stats']['mean']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Mean: Not available\\n\"\n",
    "\n",
    "        if rank_data['six_month']['universe_stats']['stdev'] is not None and not np.isnan(rank_data['six_month']['universe_stats']['stdev']):\n",
    "            md_content += f\"- Standard Deviation: {rank_data['six_month']['universe_stats']['stdev']:.4f}\\n\"\n",
    "        else:\n",
    "            md_content += \"- Standard Deviation: Not available\\n\"\n",
    "\n",
    "        # Strengths and Weaknesses\n",
    "        md_content += \"\\n## Analysis\\n\\n\"\n",
    "        md_content += \"### Strengths\\n\"\n",
    "        for strength in result['analysis']['strengths']:\n",
    "            md_content += f\"- {strength}\\n\"\n",
    "\n",
    "        md_content += \"\\n### Weaknesses\\n\"\n",
    "        for weakness in result['analysis']['weaknesses']:\n",
    "            md_content += f\"- {weakness}\\n\"\n",
    "\n",
    "        # Add rank analysis content\n",
    "        md_content += \"\\n\" + rank_analysis_md\n",
    "\n",
    "        # Save markdown file with all analysis\n",
    "        analysis_file = os.path.join(output_dir, f\"{symbol}.md\")\n",
    "        with file_lock:\n",
    "            with open(analysis_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(md_content)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stock_list(stock_list_text):\n",
    "    stocks = []\n",
    "    lines = stock_list_text.strip().split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        if '\\t' in line:\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                company_name = parts[0].strip()\n",
    "                symbol = parts[1].strip()\n",
    "                if company_name not in ['Company Name', ''] and symbol not in ['Symbol', '']:\n",
    "                    stocks.append((company_name, symbol))\n",
    "\n",
    "    return stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebee1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "stock_list_nifty = \"\"\"Company Name\tSymbol\n",
    "Adani Enterprises Ltd.\tADANIENT\n",
    "Adani Ports and Special Economic Zone Ltd.\tADANIPORTS\n",
    "Apollo Hospitals Enterprise Ltd.\tAPOLLOHOSP\n",
    "Asian Paints Ltd.\tASIANPAINT\n",
    "Axis Bank Ltd.\tAXISBANK\n",
    "Bajaj Auto Ltd.\tBAJAJ-AUTO\n",
    "Bajaj Finance Ltd.\tBAJFINANCE\n",
    "Bajaj Finserv Ltd.\tBAJAJFINSV\n",
    "Bharat Electronics Ltd.\tBEL\n",
    "Bharti Airtel Ltd.\tBHARTIARTL\n",
    "Cipla Ltd.\tCIPLA\n",
    "Coal India Ltd.\tCOALINDIA\n",
    "Dr. Reddy's Laboratories Ltd.\tDRREDDY\n",
    "Eicher Motors Ltd.\tEICHERMOT\n",
    "Grasim Industries Ltd.\tGRASIM\n",
    "HCL Technologies Ltd.\tHCLTECH\n",
    "HDFC Bank Ltd.\tHDFCBANK\n",
    "HDFC Life Insurance Company Ltd.\tHDFCLIFE\n",
    "Hero MotoCorp Ltd.\tHEROMOTOCO\n",
    "Hindalco Industries Ltd.\tHINDALCO\n",
    "Hindustan Unilever Ltd.\tHINDUNILVR\n",
    "ICICI Bank Ltd.\tICICIBANK\n",
    "ITC Ltd.\tITC\n",
    "IndusInd Bank Ltd.\tINDUSINDBK\n",
    "Infosys Ltd.\tINFY\n",
    "JSW Steel Ltd.\tJSWSTEEL\n",
    "Jio Financial Services Ltd.\tJIOFIN\n",
    "Kotak Mahindra Bank Ltd.\tKOTAKBANK\n",
    "Larsen & Toubro Ltd.\tLT\n",
    "Mahindra & Mahindra Ltd.\tM&M\n",
    "Maruti Suzuki India Ltd.\tMARUTI\n",
    "NTPC Ltd.\tNTPC\n",
    "Nestle India Ltd.\tNESTLEIND\n",
    "Oil & Natural Gas Corporation Ltd.\tONGC\n",
    "Power Grid Corporation of India Ltd.\tPOWERGRID\n",
    "Reliance Industries Ltd.\tRELIANCE\n",
    "SBI Life Insurance Company Ltd.\tSBILIFE\n",
    "Shriram Finance Ltd.\tSHRIRAMFIN\n",
    "State Bank of India\tSBIN\n",
    "Sun Pharmaceutical Industries Ltd.\tSUNPHARMA\n",
    "Tata Consultancy Services Ltd.\tTCS\n",
    "Tata Consumer Products Ltd.\tTATACONSUM\n",
    "Tata Motors Ltd.\tTATAMOTORS\n",
    "Tata Steel Ltd.\tTATASTEEL\n",
    "Tech Mahindra Ltd.\tTECHM\n",
    "Titan Company Ltd.\tTITAN\n",
    "Trent Ltd.\tTRENT\n",
    "UltraTech Cement Ltd.\tULTRACEMCO\n",
    "Wipro Ltd.\tWIPRO\n",
    "Zomato Ltd.\tZOMATO\"\"\"\n",
    "\n",
    "stocks_nifty = parse_stock_list(stock_list_nifty)\n",
    "results_nifty50 = analyze_stocks(stocks_nifty, output_dir=\"stock_analysis/reports_v2/nifty_50\", max_workers=10, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e08bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_midcap = \"\"\"Company Name\tSymbol\n",
    "360 ONE WAM Ltd.\t360ONE\n",
    "3M India Ltd.\t3MINDIA\n",
    "ACC Ltd.\tACC\n",
    "AIA Engineering Ltd.\tAIAENG\n",
    "APL Apollo Tubes Ltd.\tAPLAPOLLO\n",
    "AU Small Finance Bank Ltd.\tAUBANK\n",
    "Abbott India Ltd.\tABBOTINDIA\n",
    "Adani Total Gas Ltd.\tATGL\n",
    "Adani Wilmar Ltd.\tAWL\n",
    "Aditya Birla Capital Ltd.\tABCAPITAL\n",
    "Aditya Birla Fashion and Retail Ltd.\tABFRL\n",
    "Ajanta Pharmaceuticals Ltd.\tAJANTPHARM\n",
    "Alkem Laboratories Ltd.\tALKEM\n",
    "Apar Industries Ltd.\tAPARINDS\n",
    "Apollo Tyres Ltd.\tAPOLLOTYRE\n",
    "Ashok Leyland Ltd.\tASHOKLEY\n",
    "Astral Ltd.\tASTRAL\n",
    "Aurobindo Pharma Ltd.\tAUROPHARMA\n",
    "BSE Ltd.\tBSE\n",
    "Balkrishna Industries Ltd.\tBALKRISIND\n",
    "Bandhan Bank Ltd.\tBANDHANBNK\n",
    "Bank of India\tBANKINDIA\n",
    "Bank of Maharashtra\tMAHABANK\n",
    "Berger Paints India Ltd.\tBERGEPAINT\n",
    "Bharat Dynamics Ltd.\tBDL\n",
    "Bharat Forge Ltd.\tBHARATFORG\n",
    "Bharat Heavy Electricals Ltd.\tBHEL\n",
    "Bharti Hexacom Ltd.\tBHARTIHEXA\n",
    "Biocon Ltd.\tBIOCON\n",
    "Blue Star Ltd.\tBLUESTARCO\n",
    "CRISIL Ltd.\tCRISIL\n",
    "Cochin Shipyard Ltd.\tCOCHINSHIP\n",
    "Coforge Ltd.\tCOFORGE\n",
    "Colgate Palmolive (India) Ltd.\tCOLPAL\n",
    "Container Corporation of India Ltd.\tCONCOR\n",
    "Coromandel International Ltd.\tCOROMANDEL\n",
    "Cummins India Ltd.\tCUMMINSIND\n",
    "Dalmia Bharat Ltd.\tDALBHARAT\n",
    "Deepak Nitrite Ltd.\tDEEPAKNTR\n",
    "Dixon Technologies (India) Ltd.\tDIXON\n",
    "Emami Ltd.\tEMAMILTD\n",
    "Endurance Technologies Ltd.\tENDURANCE\n",
    "Escorts Kubota Ltd.\tESCORTS\n",
    "Exide Industries Ltd.\tEXIDEIND\n",
    "FSN E-Commerce Ventures Ltd.\tNYKAA\n",
    "Federal Bank Ltd.\tFEDERALBNK\n",
    "Fortis Healthcare Ltd.\tFORTIS\n",
    "GE Vernova T&D India Ltd.\tGVT&D\n",
    "GMR Airports Ltd.\tGMRAIRPORT\n",
    "General Insurance Corporation of India\tGICRE\n",
    "Gland Pharma Ltd.\tGLAND\n",
    "Glaxosmithkline Pharmaceuticals Ltd.\tGLAXO\n",
    "Glenmark Pharmaceuticals Ltd.\tGLENMARK\n",
    "Global Health Ltd.\tMEDANTA\n",
    "Godrej Industries Ltd.\tGODREJIND\n",
    "Godrej Properties Ltd.\tGODREJPROP\n",
    "Gujarat Fluorochemicals Ltd.\tFLUOROCHEM\n",
    "Gujarat Gas Ltd.\tGUJGASLTD\n",
    "HDFC Asset Management Company Ltd.\tHDFCAMC\n",
    "Hindustan Petroleum Corporation Ltd.\tHINDPETRO\n",
    "Hindustan Zinc Ltd.\tHINDZINC\n",
    "Hitachi Energy India Ltd.\tPOWERINDIA\n",
    "Honeywell Automation India Ltd.\tHONAUT\n",
    "Housing & Urban Development Corporation Ltd.\tHUDCO\n",
    "IDFC First Bank Ltd.\tIDFCFIRSTB\n",
    "IRB Infrastructure Developers Ltd.\tIRB\n",
    "Indian Bank\tINDIANB\n",
    "Indian Railway Catering And Tourism Corporation Ltd.\tIRCTC\n",
    "Indian Renewable Energy Development Agency Ltd.\tIREDA\n",
    "Indraprastha Gas Ltd.\tIGL\n",
    "Indus Towers Ltd.\tINDUSTOWER\n",
    "Ipca Laboratories Ltd.\tIPCALAB\n",
    "J.K. Cement Ltd.\tJKCEMENT\n",
    "JSW Infrastructure Ltd.\tJSWINFRA\n",
    "Jindal Stainless Ltd.\tJSL\n",
    "Jubilant Foodworks Ltd.\tJUBLFOOD\n",
    "K.P.R. Mill Ltd.\tKPRMILL\n",
    "KEI Industries Ltd.\tKEI\n",
    "KPIT Technologies Ltd.\tKPITTECH\n",
    "Kalyan Jewellers India Ltd.\tKALYANKJIL\n",
    "L&T Finance Ltd.\tLTF\n",
    "L&T Technology Services Ltd.\tLTTS\n",
    "LIC Housing Finance Ltd.\tLICHSGFIN\n",
    "Linde India Ltd.\tLINDEINDIA\n",
    "Lloyds Metals And Energy Ltd.\tLLOYDSME\n",
    "Lupin Ltd.\tLUPIN\n",
    "MRF Ltd.\tMRF\n",
    "Mahindra & Mahindra Financial Services Ltd.\tM&MFIN\n",
    "Mangalore Refinery & Petrochemicals Ltd.\tMRPL\n",
    "Mankind Pharma Ltd.\tMANKIND\n",
    "Marico Ltd.\tMARICO\n",
    "Max Financial Services Ltd.\tMFSL\n",
    "Max Healthcare Institute Ltd.\tMAXHEALTH\n",
    "Mazagoan Dock Shipbuilders Ltd.\tMAZDOCK\n",
    "Motherson Sumi Wiring India Ltd.\tMSUMI\n",
    "Motilal Oswal Financial Services Ltd.\tMOTILALOFS\n",
    "MphasiS Ltd.\tMPHASIS\n",
    "Muthoot Finance Ltd.\tMUTHOOTFIN\n",
    "NHPC Ltd.\tNHPC\n",
    "NLC India Ltd.\tNLCINDIA\n",
    "NMDC Ltd.\tNMDC\n",
    "NTPC Green Energy Ltd.\tNTPCGREEN\n",
    "National Aluminium Co. Ltd.\tNATIONALUM\n",
    "Nippon Life India Asset Management Ltd.\tNAM-INDIA\n",
    "Oberoi Realty Ltd.\tOBEROIRLTY\n",
    "Oil India Ltd.\tOIL\n",
    "Ola Electric Mobility Ltd.\tOLAELEC\n",
    "One 97 Communications Ltd.\tPAYTM\n",
    "Oracle Financial Services Software Ltd.\tOFSS\n",
    "PB Fintech Ltd.\tPOLICYBZR\n",
    "PI Industries Ltd.\tPIIND\n",
    "Page Industries Ltd.\tPAGEIND\n",
    "Patanjali Foods Ltd.\tPATANJALI\n",
    "Persistent Systems Ltd.\tPERSISTENT\n",
    "Petronet LNG Ltd.\tPETRONET\n",
    "Phoenix Mills Ltd.\tPHOENIXLTD\n",
    "Polycab India Ltd.\tPOLYCAB\n",
    "Premier Energies Ltd.\tPREMIERENE\n",
    "Prestige Estates Projects Ltd.\tPRESTIGE\n",
    "Rail Vikas Nigam Ltd.\tRVNL\n",
    "SBI Cards and Payment Services Ltd.\tSBICARD\n",
    "SJVN Ltd.\tSJVN\n",
    "SRF Ltd.\tSRF\n",
    "Schaeffler India Ltd.\tSCHAEFFLER\n",
    "Solar Industries India Ltd.\tSOLARINDS\n",
    "Sona BLW Precision Forgings Ltd.\tSONACOMS\n",
    "Star Health and Allied Insurance Company Ltd.\tSTARHEALTH\n",
    "Steel Authority of India Ltd.\tSAIL\n",
    "Sun TV Network Ltd.\tSUNTV\n",
    "Sundaram Finance Ltd.\tSUNDARMFIN\n",
    "Supreme Industries Ltd.\tSUPREMEIND\n",
    "Suzlon Energy Ltd.\tSUZLON\n",
    "Syngene International Ltd.\tSYNGENE\n",
    "Tata Communications Ltd.\tTATACOMM\n",
    "Tata Elxsi Ltd.\tTATAELXSI\n",
    "Tata Investment Corporation Ltd.\tTATAINVEST\n",
    "Tata Technologies Ltd.\tTATATECH\n",
    "The New India Assurance Company Ltd.\tNIACL\n",
    "Thermax Ltd.\tTHERMAX\n",
    "Torrent Power Ltd.\tTORNTPOWER\n",
    "Tube Investments of India Ltd.\tTIINDIA\n",
    "UNO Minda Ltd.\tUNOMINDA\n",
    "UPL Ltd.\tUPL\n",
    "Union Bank of India\tUNIONBANK\n",
    "United Breweries Ltd.\tUBL\n",
    "Vishal Mega Mart Ltd.\tVMM\n",
    "Vodafone Idea Ltd.\tIDEA\n",
    "Voltas Ltd.\tVOLTAS\n",
    "Waaree Energies Ltd.\tWAAREEENER\n",
    "Yes Bank Ltd.\tYESBANK\"\"\"\n",
    "\n",
    "\n",
    "stocks_midcap = parse_stock_list(stock_list_midcap)\n",
    "results_midcap = analyze_stocks(stocks_midcap, output_dir=\"stock_analysis/reports_v2/midcap_150\", max_workers=10, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c89b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_smallcap = \"\"\"Company Name\tSymbol\n",
    "ACME Solar Holdings Ltd.\tACMESOLAR\n",
    "Aadhar Housing Finance Ltd.\tAADHARHFC\n",
    "Aarti Industries Ltd.\tAARTIIND\n",
    "Aavas Financiers Ltd.\tAAVAS\n",
    "Action Construction Equipment Ltd.\tACE\n",
    "Aditya Birla Real Estate Ltd.\tABREL\n",
    "Aditya Birla Sun Life AMC Ltd.\tABSLAMC\n",
    "Aegis Logistics Ltd.\tAEGISLOG\n",
    "Afcons Infrastructure Ltd.\tAFCONS\n",
    "Affle (India) Ltd.\tAFFLE\n",
    "Akums Drugs and Pharmaceuticals Ltd.\tAKUMS\n",
    "Alembic Pharmaceuticals Ltd.\tAPLLTD\n",
    "Alivus Life Sciences Ltd.\tALIVUS\n",
    "Alkyl Amines Chemicals Ltd.\tALKYLAMINE\n",
    "Alok Industries Ltd.\tALOKINDS\n",
    "Amara Raja Energy & Mobility Ltd.\tARE&M\n",
    "Amber Enterprises India Ltd.\tAMBER\n",
    "Anand Rathi Wealth Ltd.\tANANDRATHI\n",
    "Anant Raj Ltd.\tANANTRAJ\n",
    "Angel One Ltd.\tANGELONE\n",
    "Aptus Value Housing Finance India Ltd.\tAPTUS\n",
    "Asahi India Glass Ltd.\tASAHIINDIA\n",
    "Aster DM Healthcare Ltd.\tASTERDM\n",
    "AstraZenca Pharma India Ltd.\tASTRAZEN\n",
    "Atul Ltd.\tATUL\n",
    "Authum Investment & Infrastructure Ltd.\tAIIL\n",
    "BASF India Ltd.\tBASF\n",
    "BEML Ltd.\tBEML\n",
    "BLS International Services Ltd.\tBLS\n",
    "Balrampur Chini Mills Ltd.\tBALRAMCHIN\n",
    "Bata India Ltd.\tBATAINDIA\n",
    "Bayer Cropscience Ltd.\tBAYERCROP\n",
    "Bikaji Foods International Ltd.\tBIKAJI\n",
    "Birlasoft Ltd.\tBSOFT\n",
    "Blue Dart Express Ltd.\tBLUEDART\n",
    "Bombay Burmah Trading Corporation Ltd.\tBBTC\n",
    "Brainbees Solutions Ltd.\tFIRSTCRY\n",
    "Brigade Enterprises Ltd.\tBRIGADE\n",
    "C.E. Info Systems Ltd.\tMAPMYINDIA\n",
    "CCL Products (I) Ltd.\tCCL\n",
    "CESC Ltd.\tCESC\n",
    "Campus Activewear Ltd.\tCAMPUS\n",
    "Can Fin Homes Ltd.\tCANFINHOME\n",
    "Caplin Point Laboratories Ltd.\tCAPLIPOINT\n",
    "Capri Global Capital Ltd.\tCGCL\n",
    "Carborundum Universal Ltd.\tCARBORUNIV\n",
    "Castrol India Ltd.\tCASTROLIND\n",
    "Ceat Ltd.\tCEATLTD\n",
    "Central Bank of India\tCENTRALBK\n",
    "Central Depository Services (India) Ltd.\tCDSL\n",
    "Century Plyboards (India) Ltd.\tCENTURYPLY\n",
    "Cera Sanitaryware Ltd\tCERA\n",
    "Chalet Hotels Ltd.\tCHALET\n",
    "Chambal Fertilizers & Chemicals Ltd.\tCHAMBLFERT\n",
    "Chennai Petroleum Corporation Ltd.\tCHENNPETRO\n",
    "Cholamandalam Financial Holdings Ltd.\tCHOLAHLDNG\n",
    "City Union Bank Ltd.\tCUB\n",
    "Clean Science and Technology Ltd.\tCLEAN\n",
    "Computer Age Management Services Ltd.\tCAMS\n",
    "Concord Biotech Ltd.\tCONCORDBIO\n",
    "Craftsman Automation Ltd.\tCRAFTSMAN\n",
    "CreditAccess Grameen Ltd.\tCREDITACC\n",
    "Crompton Greaves Consumer Electricals Ltd.\tCROMPTON\n",
    "Cyient Ltd.\tCYIENT\n",
    "DCM Shriram Ltd.\tDCMSHRIRAM\n",
    "DOMS Industries Ltd.\tDOMS\n",
    "Data Patterns (India) Ltd.\tDATAPATTNS\n",
    "Deepak Fertilisers & Petrochemicals Corp. Ltd.\tDEEPAKFERT\n",
    "Delhivery Ltd.\tDELHIVERY\n",
    "Devyani International Ltd.\tDEVYANI\n",
    "Dr. Lal Path Labs Ltd.\tLALPATHLAB\n",
    "E.I.D. Parry (India) Ltd.\tEIDPARRY\n",
    "EIH Ltd.\tEIHOTEL\n",
    "Elecon Engineering Co. Ltd.\tELECON\n",
    "Elgi Equipments Ltd.\tELGIEQUIP\n",
    "Emcure Pharmaceuticals Ltd.\tEMCURE\n",
    "Engineers India Ltd.\tENGINERSIN\n",
    "Eris Lifesciences Ltd.\tERIS\n",
    "Fertilisers and Chemicals Travancore Ltd.\tFACT\n",
    "Finolex Cables Ltd.\tFINCABLES\n",
    "Finolex Industries Ltd.\tFINPIPE\n",
    "Firstsource Solutions Ltd.\tFSL\n",
    "Five-Star Business Finance Ltd.\tFIVESTAR\n",
    "Garden Reach Shipbuilders & Engineers Ltd.\tGRSE\n",
    "Gillette India Ltd.\tGILLETTE\n",
    "Go Digit General Insurance Ltd.\tGODIGIT\n",
    "Godawari Power & Ispat Ltd.\tGPIL\n",
    "Godfrey Phillips India Ltd.\tGODFRYPHLP\n",
    "Godrej Agrovet Ltd.\tGODREJAGRO\n",
    "Granules India Ltd.\tGRANULES\n",
    "Graphite India Ltd.\tGRAPHITE\n",
    "Gravita India Ltd.\tGRAVITA\n",
    "Great Eastern Shipping Co. Ltd.\tGESHIP\n",
    "Gujarat Mineral Development Corporation Ltd.\tGMDCLTD\n",
    "Gujarat Narmada Valley Fertilizers and Chemicals Ltd.\tGNFC\n",
    "Gujarat Pipavav Port Ltd.\tGPPL\n",
    "Gujarat State Petronet Ltd.\tGSPL\n",
    "H.E.G. Ltd.\tHEG\n",
    "HBL Engineering Ltd.\tHBLENGINE\n",
    "HFCL Ltd.\tHFCL\n",
    "Happiest Minds Technologies Ltd.\tHAPPSTMNDS\n",
    "Himadri Speciality Chemical Ltd.\tHSCL\n",
    "Hindustan Copper Ltd.\tHINDCOPPER\n",
    "Home First Finance Company India Ltd.\tHOMEFIRST\n",
    "Honasa Consumer Ltd.\tHONASA\n",
    "IDBI Bank Ltd.\tIDBI\n",
    "IFCI Ltd.\tIFCI\n",
    "IIFL Finance Ltd.\tIIFL\n",
    "INOX India Ltd.\tINOXINDIA\n",
    "IRCON International Ltd.\tIRCON\n",
    "ITI Ltd.\tITI\n",
    "Indegene Ltd.\tINDGN\n",
    "India Cements Ltd.\tINDIACEM\n",
    "Indiamart Intermesh Ltd.\tINDIAMART\n",
    "Indian Energy Exchange Ltd.\tIEX\n",
    "Indian Overseas Bank\tIOB\n",
    "Inox Wind Ltd.\tINOXWIND\n",
    "Intellect Design Arena Ltd.\tINTELLECT\n",
    "International Gemmological Institute (India) Ltd.\tIGIL\n",
    "Inventurus Knowledge Solutions Ltd.\tIKS\n",
    "J.B. Chemicals & Pharmaceuticals Ltd.\tJBCHEPHARM\n",
    "JBM Auto Ltd.\tJBMA\n",
    "JK Tyre & Industries Ltd.\tJKTYRE\n",
    "JM Financial Ltd.\tJMFINANCIL\n",
    "JSW Holdings Ltd.\tJSWHL\n",
    "Jaiprakash Power Ventures Ltd.\tJPPOWER\n",
    "Jammu & Kashmir Bank Ltd.\tJ&KBANK\n",
    "Jindal Saw Ltd.\tJINDALSAW\n",
    "Jubilant Ingrevia Ltd.\tJUBLINGREA\n",
    "Jubilant Pharmova Ltd.\tJUBLPHARMA\n",
    "Jupiter Wagons Ltd.\tJWL\n",
    "Justdial Ltd.\tJUSTDIAL\n",
    "Jyothy Labs Ltd.\tJYOTHYLAB\n",
    "Jyoti CNC Automation Ltd.\tJYOTICNC\n",
    "KNR Constructions Ltd.\tKNRCON\n",
    "Kajaria Ceramics Ltd.\tKAJARIACER\n",
    "Kalpataru Projects International Ltd.\tKPIL\n",
    "Kansai Nerolac Paints Ltd.\tKANSAINER\n",
    "Karur Vysya Bank Ltd.\tKARURVYSYA\n",
    "Kaynes Technology India Ltd.\tKAYNES\n",
    "Kec International Ltd.\tKEC\n",
    "Kfin Technologies Ltd.\tKFINTECH\n",
    "Kirloskar Brothers Ltd.\tKIRLOSBROS\n",
    "Kirloskar Oil Eng Ltd.\tKIRLOSENG\n",
    "Krishna Institute of Medical Sciences Ltd.\tKIMS\n",
    "LT Foods Ltd.\tLTFOODS\n",
    "Latent View Analytics Ltd.\tLATENTVIEW\n",
    "Laurus Labs Ltd.\tLAURUSLABS\n",
    "Lemon Tree Hotels Ltd.\tLEMONTREE\n",
    "MMTC Ltd.\tMMTC\n",
    "Mahanagar Gas Ltd.\tMGL\n",
    "Maharashtra Seamless Ltd.\tMAHSEAMLES\n",
    "Manappuram Finance Ltd.\tMANAPPURAM\n",
    "Mastek Ltd.\tMASTEK\n",
    "Metropolis Healthcare Ltd.\tMETROPOLIS\n",
    "Minda Corporation Ltd.\tMINDACORP\n",
    "Multi Commodity Exchange of India Ltd.\tMCX\n",
    "NATCO Pharma Ltd.\tNATCOPHARM\n",
    "NBCC (India) Ltd.\tNBCC\n",
    "NCC Ltd.\tNCC\n",
    "NMDC Steel Ltd.\tNSLNISP\n",
    "Narayana Hrudayalaya Ltd.\tNH\n",
    "Nava Ltd.\tNAVA\n",
    "Navin Fluorine International Ltd.\tNAVINFLUOR\n",
    "Netweb Technologies India Ltd.\tNETWEB\n",
    "Network18 Media & Investments Ltd.\tNETWORK18\n",
    "Neuland Laboratories Ltd.\tNEULANDLAB\n",
    "Newgen Software Technologies Ltd.\tNEWGEN\n",
    "Niva Bupa Health Insurance Company Ltd.\tNIVABUPA\n",
    "Nuvama Wealth Management Ltd.\tNUVAMA\n",
    "Olectra Greentech Ltd.\tOLECTRA\n",
    "PCBL Chemical Ltd.\tPCBL\n",
    "PG Electroplast Ltd.\tPGEL\n",
    "PNB Housing Finance Ltd.\tPNBHOUSING\n",
    "PNC Infratech Ltd.\tPNCINFRA\n",
    "PTC Industries Ltd.\tPTCIL\n",
    "PVR INOX Ltd.\tPVRINOX\n",
    "Pfizer Ltd.\tPFIZER\n",
    "Piramal Enterprises Ltd.\tPEL\n",
    "Piramal Pharma Ltd.\tPPLPHARMA\n",
    "Poly Medicure Ltd.\tPOLYMED\n",
    "Poonawalla Fincorp Ltd.\tPOONAWALLA\n",
    "Praj Industries Ltd.\tPRAJIND\n",
    "Quess Corp Ltd.\tQUESS\n",
    "R R Kabel Ltd.\tRRKABEL\n",
    "RBL Bank Ltd.\tRBLBANK\n",
    "RHI MAGNESITA INDIA LTD.\tRHIM\n",
    "RITES Ltd.\tRITES\n",
    "Radico Khaitan Ltd\tRADICO\n",
    "Railtel Corporation Of India Ltd.\tRAILTEL\n",
    "Rainbow Childrens Medicare Ltd.\tRAINBOW\n",
    "Ramkrishna Forgings Ltd.\tRKFORGE\n",
    "Rashtriya Chemicals & Fertilizers Ltd.\tRCF\n",
    "RattanIndia Enterprises Ltd.\tRTNINDIA\n",
    "Raymond Lifestyle Ltd.\tRAYMONDLSL\n",
    "Raymond Ltd.\tRAYMOND\n",
    "Redington Ltd.\tREDINGTON\n",
    "Reliance Power Ltd.\tRPOWER\n",
    "Route Mobile Ltd.\tROUTE\n",
    "SBFC Finance Ltd.\tSBFC\n",
    "SKF India Ltd.\tSKFINDIA\n",
    "Sagility India Ltd.\tSAGILITY\n",
    "Sai Life Sciences Ltd.\tSAILIFE\n",
    "Sammaan Capital Ltd.\tSAMMAANCAP\n",
    "Sapphire Foods India Ltd.\tSAPPHIRE\n",
    "Sarda Energy and Minerals Ltd.\tSARDAEN\n",
    "Saregama India Ltd\tSAREGAMA\n",
    "Schneider Electric Infrastructure Ltd.\tSCHNEIDER\n",
    "Shipping Corporation of India Ltd.\tSCI\n",
    "Shree Renuka Sugars Ltd.\tRENUKA\n",
    "Shyam Metalics and Energy Ltd.\tSHYAMMETL\n",
    "Signatureglobal (India) Ltd.\tSIGNATURE\n",
    "Sobha Ltd.\tSOBHA\n",
    "Sonata Software Ltd.\tSONATSOFTW\n",
    "Sterling and Wilson Renewable Energy Ltd.\tSWSOLAR\n",
    "Sumitomo Chemical India Ltd.\tSUMICHEM\n",
    "Suven Pharmaceuticals Ltd.\tSUVENPHAR\n",
    "Swan Energy Ltd.\tSWANENERGY\n",
    "Syrma SGS Technology Ltd.\tSYRMA\n",
    "TBO Tek Ltd.\tTBOTEK\n",
    "Tanla Platforms Ltd.\tTANLA\n",
    "Tata Chemicals Ltd.\tTATACHEM\n",
    "Tata Teleservices (Maharashtra) Ltd.\tTTML\n",
    "Techno Electric & Engineering Company Ltd.\tTECHNOE\n",
    "Tejas Networks Ltd.\tTEJASNET\n",
    "The Ramco Cements Ltd.\tRAMCOCEM\n",
    "Timken India Ltd.\tTIMKEN\n",
    "Titagarh Rail Systems Ltd.\tTITAGARH\n",
    "Transformers And Rectifiers (India) Ltd.\tTARIL\n",
    "Trident Ltd.\tTRIDENT\n",
    "Triveni Engineering & Industries Ltd.\tTRIVENI\n",
    "Triveni Turbine Ltd.\tTRITURBINE\n",
    "UCO Bank\tUCOBANK\n",
    "UTI Asset Management Company Ltd.\tUTIAMC\n",
    "Usha Martin Ltd.\tUSHAMART\n",
    "V-Guard Industries Ltd.\tVGUARD\n",
    "Valor Estate Ltd.\tDBREALTY\n",
    "Vardhman Textiles Ltd.\tVTL\n",
    "Vedant Fashions Ltd.\tMANYAVAR\n",
    "Vijaya Diagnostic Centre Ltd.\tVIJAYA\n",
    "Welspun Corp Ltd.\tWELCORP\n",
    "Welspun Living Ltd.\tWELSPUNLIV\n",
    "Westlife Foodworld Ltd.\tWESTLIFE\n",
    "Whirlpool of India Ltd.\tWHIRLPOOL\n",
    "Wockhardt Ltd.\tWOCKPHARMA\n",
    "ZF Commercial Vehicle Control Systems India Ltd.\tZFCVINDIA\n",
    "Zee Entertainment Enterprises Ltd.\tZEEL\n",
    "Zen Technologies Ltd.\tZENTEC\n",
    "Zensar Technolgies Ltd.\tZENSARTECH\n",
    "eClerx Services Ltd.\tECLERX\"\"\"\n",
    "\n",
    "\n",
    "stocks_smallcap = parse_stock_list(stock_list_smallcap)\n",
    "results_smallcap = analyze_stocks(stocks_smallcap, output_dir=\"stock_analysis/reports_v2/smallcap_250\", max_workers=10, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fc9a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_list_microcap = \"\"\"Company Name\tSymbol\n",
    "AGI Greenpac Ltd.\tAGI\n",
    "ASK Automotive Ltd.\tASKAUTOLTD\n",
    "Aarti Drugs Ltd.\tAARTIDRUGS\n",
    "Aarti Pharmalabs Ltd.\tAARTIPHARM\n",
    "Aditya Vision Ltd.\tAVL\n",
    "Advanced Enzyme Tech Ltd.\tADVENZYMES\n",
    "Aether Industries Ltd.\tAETHER\n",
    "Ahluwalia Contracts (India) Ltd.\tAHLUCONT\n",
    "Akzo Nobel India Ltd.\tAKZOINDIA\n",
    "Allcargo Logistics Ltd.\tALLCARGO\n",
    "Allied Blenders and Distillers Ltd.\tABDL\n",
    "Ami Organics Ltd.\tAMIORG\n",
    "Apeejay Surrendra Park Hotels Ltd.\tPARKHOTELS\n",
    "Archean Chemical Industries Ltd.\tACI\n",
    "Arvind Fashions Ltd.\tARVINDFASN\n",
    "Arvind Ltd.\tARVIND\n",
    "Ashoka Buildcon Ltd.\tASHOKA\n",
    "Astra Microwave Products Ltd.\tASTRAMICRO\n",
    "Aurionpro Solution Ltd.\tAURIONPRO\n",
    "Avalon Technologies Ltd.\tAVALON\n",
    "Avanti Feeds Ltd.\tAVANTIFEED\n",
    "Awfis Space Solutions Ltd.\tAWFIS\n",
    "Azad Engineering Ltd.\tAZAD\n",
    "Bajaj Hindusthan Sugar Ltd.\tBAJAJHIND\n",
    "Balaji Amines Ltd.\tBALAMINES\n",
    "Balu Forge Industries Ltd.\tBALUFORGE\n",
    "Banco Products (India) Ltd.\tBANCOINDIA\n",
    "Bansal Wire Industries Ltd.\tBANSALWIRE\n",
    "Bhansali Engineering Polymers Ltd.\tBEPL\n",
    "Bharat Bijlee Ltd.\tBBL\n",
    "Birla Corporation Ltd.\tBIRLACORPN\n",
    "Blue Jet Healthcare Ltd.\tBLUEJET\n",
    "Bombay Dyeing & Manufacturing Co. Ltd.\tBOMDYEING\n",
    "Borosil Ltd.\tBOROLTD\n",
    "Borosil Renewables Ltd.\tBORORENEW\n",
    "CIE Automotive India Ltd.\tCIEINDIA\n",
    "CMS Info Systems Ltd.\tCMSINFO\n",
    "CSB Bank Ltd.\tCSBBANK\n",
    "Cartrade Tech Ltd.\tCARTRADE\n",
    "Ceigall India Ltd.\tCEIGALL\n",
    "Cello World Ltd.\tCELLO\n",
    "Chemplast Sanmar Ltd.\tCHEMPLASTS\n",
    "Choice International Ltd.\tCHOICEIN\n",
    "Cigniti Technologies Ltd.\tCIGNITITEC\n",
    "Cyient DLM Ltd.\tCYIENTDLM\n",
    "DCB Bank Ltd.\tDCBBANK\n",
    "DCX Systems Ltd.\tDCXINDIA\n",
    "Datamatics Global Services Ltd.\tDATAMATICS\n",
    "Dhani Services Ltd.\tDHANI\n",
    "Dilip Buildcon Ltd.\tDBL\n",
    "Dishman Carbogen Amcis Ltd.\tDCAL\n",
    "Dodla Dairy Ltd.\tDODLA\n",
    "Dynamatic Technologies Ltd.\tDYNAMATECH\n",
    "EPL Ltd.\tEPL\n",
    "Easy Trip Planners Ltd.\tEASEMYTRIP\n",
    "Edelweiss Financial Services Ltd.\tEDELWEISS\n",
    "Electronics Mart India Ltd.\tEMIL\n",
    "Electrosteel Castings Ltd.\tELECTCAST\n",
    "Embassy Developments Ltd.\tEMBDL\n",
    "Entero Healthcare Solutions Ltd.\tENTERO\n",
    "Enviro Infra Engineers Ltd.\tEIEL\n",
    "Epigral Ltd.\tEPIGRAL\n",
    "Equitas Small Finance Bank Ltd.\tEQUITASBNK\n",
    "Ethos Ltd.\tETHOSLTD\n",
    "Eureka Forbes Ltd.\tEUREKAFORB\n",
    "FDC Ltd.\tFDC\n",
    "Fiem Industries Ltd\tFIEMIND\n",
    "Fine Organic Industries Ltd.\tFINEORG\n",
    "Fineotex Chemical Ltd.\tFCL\n",
    "Force Motors Ltd.\tFORCEMOT\n",
    "G R Infraprojects Ltd.\tGRINFRA\n",
    "GHCL Ltd.\tGHCL\n",
    "GMM Pfaudler Ltd.\tGMMPFAUDLR\n",
    "GMR Power and Urban Infra Ltd.\tGMRP&UI\n",
    "Gabriel India Ltd.\tGABRIEL\n",
    "Ganesh Housing Corporation Ltd.\tGANESHHOUC\n",
    "Ganesha Ecosphere Ltd.\tGANECOS\n",
    "Garware Hi-Tech Films Ltd.\tGRWRHITECH\n",
    "Garware Technical Fibres Ltd.\tGARFIBRES\n",
    "Gateway Distriparks Ltd.\tGATEWAY\n",
    "Gokaldas Exports Ltd.\tGOKEX\n",
    "Gopal Snacks Ltd.\tGOPAL\n",
    "Greaves Cotton Ltd.\tGREAVESCOT\n",
    "Greenpanel Industries Ltd.\tGREENPANEL\n",
    "Greenply Industries Ltd.\tGREENPLY\n",
    "Gujarat Ambuja Exports Ltd.\tGAEL\n",
    "Gujarat State Fertilizers & Chemicals Ltd.\tGSFC\n",
    "Gulf Oil Lubricants India Ltd.\tGULFOILLUB\n",
    "H.G. Infra Engineering Ltd.\tHGINFRA\n",
    "Hathway Cable & Datacom Ltd.\tHATHWAY\n",
    "Healthcare Global Enterprises Ltd.\tHCG\n",
    "HeidelbergCement India Ltd.\tHEIDELBERG\n",
    "Hemisphere Properties India Ltd.\tHEMIPROP\n",
    "Heritage Foods Ltd.\tHERITGFOOD\n",
    "Hikal Ltd.\tHIKAL\n",
    "Hindustan Construction Co. Ltd.\tHCC\n",
    "IFB Industries Ltd.\tIFBIND\n",
    "IIFL Capital Services Ltd.\tIIFLCAPS\n",
    "ITD Cementation India Ltd.\tITDCEM\n",
    "Imagicaaworld Entertainment Ltd.\tIMAGICAA\n",
    "India Glycols Ltd.\tINDIAGLYCO\n",
    "India Shelter Finance Corporation Ltd.\tINDIASHLTR\n",
    "Indian Metals & Ferro Alloys Ltd.\tIMFA\n",
    "Indigo Paints Ltd.\tINDIGOPNTS\n",
    "Indo Count Industries Ltd.\tICIL\n",
    "Infibeam Avenues Ltd.\tINFIBEAM\n",
    "Ingersoll Rand (India) Ltd.\tINGERRAND\n",
    "Innova Captab Ltd.\tINNOVACAP\n",
    "Inox Green Energy Services Ltd.\tINOXGREEN\n",
    "Ion Exchange (India) Ltd.\tIONEXCHANG\n",
    "Isgec Heavy Engineering Ltd.\tISGEC\n",
    "J.Kumar Infraprojects Ltd.\tJKIL\n",
    "JK Lakshmi Cement Ltd.\tJKLAKSHMI\n",
    "JK Paper Ltd.\tJKPAPER\n",
    "JTL Industries Ltd.\tJTLIND\n",
    "Jai Balaji Industries Ltd.\tJAIBALAJI\n",
    "Jai Corp Ltd.\tJAICORPLTD\n",
    "Jain Irrigation Systems Ltd.\tJISLJALEQS\n",
    "Jamna Auto Industries Ltd.\tJAMNAAUTO\n",
    "Jana Small Finance Bank Ltd.\tJSFB\n",
    "Jindal Worldwide Ltd.\tJINDWORLD\n",
    "Johnson Controls - Hitachi Air Conditioning India Ltd.\tJCHAC\n",
    "KPI Green Energy Ltd.\tKPIGREEN\n",
    "KRBL Ltd.\tKRBL\n",
    "KSB Ltd.\tKSB\n",
    "Kalyani Steels Ltd.\tKSL\n",
    "Karnataka Bank Ltd.\tKTKBANK\n",
    "Kaveri Seed Company Ltd.\tKSCL\n",
    "Kirloskar Pneumatic Company Ltd.\tKIRLPNU\n",
    "LMW Ltd.\tLMW\n",
    "Laxmi Organic Industries Ltd.\tLXCHEM\n",
    "Le Travenues Technology Ltd.\tIXIGO\n",
    "Lloyds Engineering Works Ltd.\tLLOYDSENGG\n",
    "Lloyds Enterprises Ltd.\tLLOYDSENT\n",
    "Lux Industries Ltd.\tLUXIND\n",
    "MOIL Ltd.\tMOIL\n",
    "MSTC Ltd.\tMSTCLTD\n",
    "MTAR Technologies Ltd.\tMTARTECH\n",
    "Maharashtra Scooters Ltd.\tMAHSCOOTER\n",
    "Mahindra Lifespace Developers Ltd.\tMAHLIFE\n",
    "Man Infraconstruction Ltd.\tMANINFRA\n",
    "Marksans Pharma Ltd.\tMARKSANS\n",
    "Max Estates Ltd.\tMAXESTATES\n",
    "Medplus Health Services Ltd.\tMEDPLUS\n",
    "Mishra Dhatu Nigam Ltd.\tMIDHANI\n",
    "Mrs. Bectors Food Specialities Ltd.\tBECTORFOOD\n",
    "NEOGEN CHEMICALS LTD.\tNEOGEN\n",
    "NESCO Ltd.\tNESCO\n",
    "NOCIL Ltd.\tNOCIL\n",
    "National Fertilizers Ltd.\tNFL\n",
    "Nazara Technologies Ltd.\tNAZARA\n",
    "Nuvoco Vistas Corporation Ltd.\tNUVOCO\n",
    "Optiemus Infracom Ltd.\tOPTIEMUS\n",
    "Orchid Pharma Ltd.\tORCHPHARMA\n",
    "Orient Cement Ltd.\tORIENTCEM\n",
    "Orissa Min Dev Co Ltd.\tORISSAMINE\n",
    "P N Gadgil Jewellers Ltd.\tPNGJL\n",
    "PC Jeweller Ltd.\tPCJEWELLER\n",
    "PTC India Ltd.\tPTC\n",
    "Paisalo Digital Ltd.\tPAISALO\n",
    "Paradeep Phosphates Ltd.\tPARADEEP\n",
    "Paras Defence and Space Technologies Ltd.\tPARAS\n",
    "Patel Engineering Ltd.\tPATELENG\n",
    "Pearl Global Industries Ltd.\tPGIL\n",
    "Polyplex Corporation Ltd.\tPOLYPLEX\n",
    "Power Mech Projects Ltd.\tPOWERMECH\n",
    "Pricol Ltd.\tPRICOLLTD\n",
    "Prince Pipes and Fittings Ltd.\tPRINCEPIPE\n",
    "Prism Johnson Ltd.\tPRSMJOHNSN\n",
    "Prudent Corporate Advisory Services Ltd.\tPRUDENT\n",
    "Rain Industries Ltd\tRAIN\n",
    "Rajesh Exports Ltd.\tRAJESHEXPO\n",
    "Rallis India Ltd.\tRALLIS\n",
    "Rategain Travel Technologies Ltd.\tRATEGAIN\n",
    "RattanIndia Power Ltd.\tRTNPOWER\n",
    "Redtape Ltd.\tREDTAPE\n",
    "Refex Industries Ltd.\tREFEX\n",
    "Reliance Infrastructure Ltd.\tRELINFRA\n",
    "Religare Enterprises Ltd.\tRELIGARE\n",
    "Responsive Industries Ltd.\tRESPONIND\n",
    "Restaurant Brands Asia Ltd.\tRBA\n",
    "Rossari Biotech Ltd.\tROSSARI\n",
    "Safari Industries (India) Ltd.\tSAFARI\n",
    "Samhi Hotels Ltd.\tSAMHI\n",
    "Sanofi Consumer Healthcare India Ltd.\tSANOFICONR\n",
    "Sanofi India Ltd.\tSANOFI\n",
    "Sansera Engineering Ltd.\tSANSERA\n",
    "Senco Gold Ltd.\tSENCO\n",
    "Sequent Scientific Ltd.\tSEQUENT\n",
    "Shaily Engineering Plastics Ltd.\tSHAILY\n",
    "Shakti Pumps (India) Ltd.\tSHAKTIPUMP\n",
    "Sharda Cropchem Ltd.\tSHARDACROP\n",
    "Share India Securities Ltd.\tSHAREINDIA\n",
    "Sheela Foam Ltd.\tSFL\n",
    "Shilpa Medicare Ltd.\tSHILPAMED\n",
    "Shivalik Bimetal Controls Ltd.\tSBCL\n",
    "Shoppers Stop Ltd.\tSHOPERSTOP\n",
    "Shriram Pistons & Rings Ltd.\tSHRIPISTON\n",
    "Skipper Ltd.\tSKIPPER\n",
    "South Indian Bank Ltd.\tSOUTHBANK\n",
    "Spandana Sphoorty Financial Ltd.\tSPANDANA\n",
    "Star Cement Ltd.\tSTARCEMENT\n",
    "Sterlite Technologies Ltd.\tSTLTECH\n",
    "Strides Pharma Science Ltd.\tSTAR\n",
    "Stylam Industries Ltd.\tSTYLAMIND\n",
    "Subros Ltd.\tSUBROS\n",
    "Sudarshan Chemical Industries Ltd.\tSUDARSCHEM\n",
    "Sula Vineyards Ltd.\tSULA\n",
    "Sun Pharma Advanced Research Company Ltd.\tSPARC\n",
    "Sunflag Iron & Steel Company Ltd.\tSUNFLAG\n",
    "Sunteck Realty Ltd.\tSUNTECK\n",
    "Suprajit Engineering Ltd.\tSUPRAJIT\n",
    "Supriya Lifescience Ltd.\tSUPRIYA\n",
    "Surya Roshni Ltd.\tSURYAROSNI\n",
    "Symphony Ltd.\tSYMPHONY\n",
    "TARC Ltd.\tTARC\n",
    "TD Power Systems Ltd.\tTDPOWERSYS\n",
    "TVS Supply Chain Solutions Ltd.\tTVSSCS\n",
    "Teamlease Services Ltd.\tTEAMLEASE\n",
    "Technocraft Industries (India) Ltd.\tTIIL\n",
    "Tega Industries Ltd.\tTEGA\n",
    "Texmaco Rail & Eng. Ltd.\tTEXRAIL\n",
    "Thangamayil Jewellery Ltd.\tTHANGAMAYL\n",
    "The Anup Engineering Ltd.\tANUP\n",
    "Thirumalai Chemicals Ltd.\tTIRUMALCHM\n",
    "Thomas Cook (India) Ltd.\tTHOMASCOOK\n",
    "Tilaknagar Industries Ltd.\tTI\n",
    "Time Technoplast Ltd.\tTIMETECHNO\n",
    "Tips Music Ltd.\tTIPSMUSIC\n",
    "Transrail Lighting Ltd.\tTRANSRAILL\n",
    "Ujjivan Small Finance Bank Ltd.\tUJJIVANSFB\n",
    "Unimech Aerospace and Manufacturing Ltd.\tUNIMECH\n",
    "V-Mart Retail Ltd.\tVMART\n",
    "V.I.P. Industries Ltd.\tVIPIND\n",
    "VST Industries Ltd.\tVSTIND\n",
    "Va Tech Wabag Ltd.\tWABAG\n",
    "Vaibhav Global Ltd.\tVAIBHAVGBL\n",
    "Varroc Engineering Ltd.\tVARROC\n",
    "Ventive Hospitality Ltd.\tVENTIVE\n",
    "Venus Pipes & Tubes Ltd.\tVENUSPIPES\n",
    "Vesuvius India Ltd.\tVESUVIUS\n",
    "Voltamp Transformers Ltd\tVOLTAMP\n",
    "Websol Energy System Ltd.\tWEBELSOLAR\n",
    "Welspun Enterprises Ltd.\tWELENT\n",
    "Wonderla Holidays Ltd.\tWONDERLA\n",
    "Yatharth Hospital & Trauma Care Services Ltd.\tYATHARTH\n",
    "Zaggle Prepaid Ocean Services Ltd.\tZAGGLE\n",
    "Zinka Logistics Solutions Ltd.\tBLACKBUCK\n",
    "Zydus Wellness Ltd.\tZYDUSWELL\n",
    "eMudhra Ltd.\tEMUDHRA\"\"\"\n",
    "\n",
    "\n",
    "stocks_microcap = parse_stock_list(stock_list_microcap)\n",
    "results_microcap = analyze_stocks(stocks_microcap, output_dir=\"stock_analysis/reports_v2/microcap_250\", max_workers=10, end_date=end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Green color ANSI escape code\n",
    "GREEN = '\\033[92m'\n",
    "RESET = '\\033[0m'  # Reset color code\n",
    "\n",
    "print(f\"{GREEN}Nifty Stocks Processed {len(stocks_nifty)} \\nMidcap Stocks Processed {len(stocks_midcap)} \\nSmallcap Stocks Processed {len(stocks_smallcap)} \\nMicrocap Stocks Processed {len(stocks_microcap)}{RESET}\")\n",
    "print(f\"{GREEN}Total Stocks Processed {len(stocks_nifty)+len(stocks_midcap)+len(stocks_smallcap)+len(stocks_microcap)}{RESET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b657d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(path)\n",
    "print(\"folders \", folders)\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    # Extract category name from folder path (nifty_50, midcap_150, etc.)\n",
    "    category = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        # Extract stock symbol from filename (e.g., HDFC.md -> HDFC)\n",
    "        filename = os.path.basename(doc.metadata[\"source\"])\n",
    "        symbol = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Add structured metadata\n",
    "        doc.metadata[\"category\"] = category  # Explicit category field\n",
    "        doc.metadata[\"stock_category\"] = category  # Alternative name for filtering\n",
    "        doc.metadata[\"symbol\"] = symbol  # Add symbol for stock-specific searches\n",
    "        doc.metadata[\"doc_type\"] = category  # Keep original for compatibility\n",
    "        doc.metadata[\"doc_id\"] = f\"{symbol}_{category}\"  # Add a unique document ID\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(f\"Loaded {len(folder_docs)} documents from {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456bc66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add preprocessing to ensure key information like normalized z-score rank is prominently included\n",
    "documents_with_enhanced_context = []\n",
    "for doc in documents:\n",
    "    # Extract the most important ranking information from the document\n",
    "    normalized_z_rank = re.findall(r'Normalized Z-Score Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    weighted_z_rank = re.findall(r'Weighted Z-Score Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    one_year_rank = re.findall(r'1-Year Momentum Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "    six_month_rank = re.findall(r'6-Month Momentum Rank: (\\d+) out of (\\d+)', doc.page_content)\n",
    "\n",
    "    # Get universe/category information\n",
    "    universe_match = re.search(r'Universe/Category: ([^\\n]+)', doc.page_content)\n",
    "    universe = universe_match.group(1) if universe_match else None\n",
    "\n",
    "    # Get stock name and symbol from the first line (usually contains \"# Company Name (SYMBOL) Analysis\")\n",
    "    stock_info = re.search(r'# (.*?) \\((.*?)\\)', doc.page_content)\n",
    "    company_name = stock_info.group(1) if stock_info else \"Unknown Company\"\n",
    "    symbol = stock_info.group(2) if stock_info else \"UNKNOWN\"\n",
    "\n",
    "    # Extract current price and performance metrics\n",
    "    current_price_match = re.search(r'Current Price: Rs\\.([0-9.]+)', doc.page_content)\n",
    "    current_price = current_price_match.group(1) if current_price_match else None\n",
    "\n",
    "    one_year_change_match = re.search(r'1-Year Change: ([0-9.-]+)%', doc.page_content)\n",
    "    one_year_change = one_year_change_match.group(1) if one_year_change_match else None\n",
    "\n",
    "    six_month_change_match = re.search(r'6-Month Change: ([0-9.-]+)%', doc.page_content)\n",
    "    six_month_change = six_month_change_match.group(1) if six_month_change_match else None\n",
    "\n",
    "    # If the document contains rank information, enhance its representation\n",
    "    rank_summary = f\"STOCK RANKING INFORMATION - HIGH PRIORITY:\\n\"\n",
    "    rank_summary += f\"Stock: {company_name} ({symbol})\\n\"\n",
    "\n",
    "    if universe:\n",
    "        rank_summary += f\"Universe/Category: {universe}\\n\"\n",
    "\n",
    "    if current_price:\n",
    "        rank_summary += f\"Current Price: Rs.{current_price}\\n\"\n",
    "\n",
    "    if one_year_change:\n",
    "        rank_summary += f\"1-Year Change: {one_year_change}%\\n\"\n",
    "\n",
    "    if six_month_change:\n",
    "        rank_summary += f\"6-Month Change: {six_month_change}%\\n\"\n",
    "\n",
    "    # Add ranking information with special formatting to make it stand out\n",
    "    if normalized_z_rank:\n",
    "        rank_summary += f\"===NORMALIZED Z-SCORE RANK: {normalized_z_rank[0][0]} OUT OF {normalized_z_rank[0][1]}===\\n\"\n",
    "\n",
    "    if weighted_z_rank:\n",
    "        rank_summary += f\"===WEIGHTED Z-SCORE RANK: {weighted_z_rank[0][0]} OUT OF {weighted_z_rank[0][1]}===\\n\"\n",
    "\n",
    "    if one_year_rank:\n",
    "        rank_summary += f\"===1-YEAR MOMENTUM RANK: {one_year_rank[0][0]} OUT OF {one_year_rank[0][1]}===\\n\"\n",
    "\n",
    "    if six_month_rank:\n",
    "        rank_summary += f\"===6-MONTH MOMENTUM RANK: {six_month_rank[0][0]} OUT OF {six_month_rank[0][1]}===\\n\"\n",
    "\n",
    "    # Prepend the summary to the document content (THREE times to heavily emphasize it)\n",
    "    # This ensures that the ranking information gets the highest weight in the embedding\n",
    "    doc.page_content = rank_summary + \"\\n\" + rank_summary + \"\\n\" + rank_summary + \"\\n\" + doc.page_content\n",
    "\n",
    "    # Also add key information to metadata for filtering\n",
    "    if normalized_z_rank:\n",
    "        doc.metadata['normalized_z_rank'] = int(normalized_z_rank[0][0])\n",
    "        doc.metadata['normalized_z_total'] = int(normalized_z_rank[0][1])\n",
    "\n",
    "    if weighted_z_rank:\n",
    "        doc.metadata['weighted_z_rank'] = int(weighted_z_rank[0][0])\n",
    "        doc.metadata['weighted_z_total'] = int(weighted_z_rank[0][1])\n",
    "\n",
    "    if one_year_rank:\n",
    "        doc.metadata['one_year_rank'] = int(one_year_rank[0][0])\n",
    "        doc.metadata['one_year_total'] = int(one_year_rank[0][1])\n",
    "\n",
    "    if six_month_rank:\n",
    "        doc.metadata['six_month_rank'] = int(six_month_rank[0][0])\n",
    "        doc.metadata['six_month_total'] = int(six_month_rank[0][1])\n",
    "\n",
    "    documents_with_enhanced_context.append(doc)\n",
    "\n",
    "# Use the enhanced documents for chunking - adjust chunking strategy for performance\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n#### \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# First, add headers to each document that identify the stock even after chunking\n",
    "for doc in documents_with_enhanced_context:\n",
    "    symbol = doc.metadata.get(\"symbol\", \"UNKNOWN\")\n",
    "    category = doc.metadata.get(\"category\", \"UNKNOWN\")\n",
    "    company_match = re.search(r'# (.*?) \\((.*?)\\)', doc.page_content)\n",
    "    company_name = company_match.group(1) if company_match else \"Unknown Company\"\n",
    "\n",
    "    # Create an identifier header that will be present in every chunk\n",
    "    stock_header = f\"STOCK IDENTIFIER: {company_name} ({symbol}) - Category: {category}\\n\\n\"\n",
    "\n",
    "    # Prepend this to the document\n",
    "    doc.page_content = stock_header + doc.page_content\n",
    "\n",
    "chunks = text_splitter.split_documents(documents_with_enhanced_context)\n",
    "\n",
    "# Ensure all chunks have the correct metadata\n",
    "for chunk in chunks:\n",
    "    # Extract symbol and category from the first line if metadata was lost\n",
    "    first_line = chunk.page_content.split(\"\\n\")[0]\n",
    "    if \"STOCK:\" in first_line and \"CATEGORY:\" in first_line:\n",
    "        try:\n",
    "            # Extract stock info from the content if needed\n",
    "            stock_id_match = re.search(r'STOCK IDENTIFIER: (.*?) \\((.*?)\\) - Category: (.*?)$',\n",
    "                              chunk.page_content, re.MULTILINE)\n",
    "\n",
    "            if stock_id_match:\n",
    "                company_name = stock_id_match.group(1)\n",
    "                symbol = stock_id_match.group(2)\n",
    "                category = stock_id_match.group(3)\n",
    "\n",
    "                # Update metadata with extracted info\n",
    "                chunk.metadata[\"symbol\"] = symbol\n",
    "                chunk.metadata[\"category\"] = category\n",
    "                chunk.metadata[\"stock_category\"] = category\n",
    "                chunk.metadata[\"doc_type\"] = category\n",
    "                chunk.metadata[\"company_name\"] = company_name\n",
    "        except:\n",
    "            pass  # Skip if extraction fails\n",
    "\n",
    "# Print some examples of the metadata to verify\n",
    "for i in range(min(3, len(chunks))):\n",
    "    print(f\"Chunk {i} metadata: {chunks[i].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36321c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a266eea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49957e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True, \"batch_size\": 32}\n",
    ")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=db_name,\n",
    "    collection_metadata={\n",
    "        \"hnsw:space\": \"cosine\",\n",
    "        \"hnsw:M\": 16,              # Increase graph connectivity\n",
    "        \"ef_construction\": 200,    # Higher precision during construction (removed \"hnsw:\" prefix)\n",
    "        \"ef\": 50                  # Removed \"hnsw:\" prefix\n",
    "        }  # Explicitly specify distance metric\n",
    ")\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84060f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework for chroma vectors\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['nifty_50', 'midcap_150', 'smallcap_250','microcap_250'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b26144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_2d_visualization():\n",
    "    # We humans find it easier to visalize things in 2D!\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Create the 2D scatter plot with improved layout for side-by-side display\n",
    "    fig = go.Figure(data=[go.Scatter(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(size=5, color=colors, opacity=0.8),\n",
    "        text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        margin=dict(r=10, b=10, l=10, t=10),  # Minimize margins\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,\n",
    "        hovermode=\"closest\",\n",
    "        height=None,  # Let height be determined by container\n",
    "        width=None    # Let width be determined by container\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def create_3d_visualization():\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=min(25, len(vectors) - 1))\n",
    "    reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    # Create the 3D scatter plot with improved layout for side-by-side display\n",
    "    fig = go.Figure(data=[go.Scatter3d(\n",
    "        x=reduced_vectors[:, 0],\n",
    "        y=reduced_vectors[:, 1],\n",
    "        z=reduced_vectors[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=4, color=colors, opacity=0.7),  # Smaller markers for better performance\n",
    "        text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "        hoverinfo='text'\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='x',\n",
    "            yaxis_title='y',\n",
    "            zaxis_title='z',\n",
    "            aspectmode='data'  # Better fitting for the container\n",
    "        ),\n",
    "        margin=dict(r=0, b=0, l=0, t=0),  # Minimize margins\n",
    "        template=\"plotly_white\",\n",
    "        autosize=True,\n",
    "        height=None,  # Let height be determined by container\n",
    "        width=None    # Let width be determined by container\n",
    "    )\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b45bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total chunks in vectorstore: {len(chunks)}\")\n",
    "print(f\"Total unique stocks: 621\")\n",
    "print(f\"Average chunks per stock: {len(chunks)/621:.2f}\")\n",
    "\n",
    "# Calculate recommended k value\n",
    "recommended_k = len(chunks)  # Start with maximum possible chunks\n",
    "print(f\"Recommended k value: {recommended_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE = \"\"\"\n",
    "# You are a financial advisor specializing in the Indian stock market analysis.\n",
    "# Use the following pieces of context to answer the question at the end.\n",
    "# Always respond in English only.\n",
    "# Focus on providing factual information from the given context.\n",
    "\n",
    "# EXTREMELY IMPORTANT RULES:\n",
    "# 1. When asked for stocks from a specific category (Nifty 50, Midcap 150, Smallcap 250, Microcap 250):\n",
    "#    - ONLY include stocks from the EXACT category requested\n",
    "#    - NEVER mix stocks from different categories\n",
    "\n",
    "# 2. When asked for \"top stocks\" or rankings:\n",
    "#    - ALWAYS sort by Normalized Z-Score Rank in ascending order (Rank 1 is best)\n",
    "#    - Present EXACTLY the number of stocks requested (e.g., \"top 10\" means exactly ranks 1-10)\n",
    "#    - Format each entry as: \"Company Name (SYMBOL) - Normalized Z-Score Rank: X out of Y\"\n",
    "\n",
    "# 3. For ANY ranking-related question:\n",
    "#    - Include COMPLETE ranking information (rank number AND total stocks)\n",
    "#    - NEVER omit the rank numbers or total count\n",
    "\n",
    "# If the question is about a specific category of stocks (like \"Nifty 50\", \"midcap\", \"smallcap\", or \"microcap\"):\n",
    "# - Group your answer by the requested category\n",
    "# - Present information as a well-organized list\n",
    "# - Include key metrics like price, momentum scores, and rankings when available\n",
    "# - When asked for rankings or \"top stocks\", ALWAYS sort by Normalized Z-Score Rank in ascending order (Rank 1 is best) unless another specific rank type is mentioned\n",
    "# - When asked for \"top 10\" or similar, return EXACTLY the stocks with ranks 1-10, not any random 10 stocks\n",
    "# - Be very precise about the category - never mix stocks from different categories (e.g., don't include microcap stocks when asked about smallcap or don't mix stocks from one universe to other)\n",
    "\n",
    "# If the question mentions \"normalized z-score\" or \"normalized z-score rank\":\n",
    "# - THIS IS EXTREMELY IMPORTANT: Sort results by Normalized Z-Score Rank in ascending order (Rank 1 is best)\n",
    "# - Include ONLY stocks that have this specific rank information\n",
    "# - Present results with lowest rank numbers first (1, 2, 3, etc.)\n",
    "# - Include the stock symbol, company name, and the exact Normalized Z-Score Rank value\n",
    "# - Example format for each stock: \"Company Name (SYMBOL) - Normalized Z-Score Rank: X out of Y\"\n",
    "\n",
    "# If the question mentions \"weighted z-score\" or \"weighted z-score rank\":\n",
    "# - Sort results by Weighted Z-Score Rank in ascending order (Rank 1 is best)\n",
    "# - Include ONLY stocks that have this specific rank information\n",
    "# - Present results with lowest rank numbers first (1, 2, 3, etc.)\n",
    "# - Include the stock symbol, company name, and the exact Weighted Z-Score Rank value\n",
    "\n",
    "# If the question mentions \"momentum\", \"positive momentum\", \"strong performance\" or similar terms:\n",
    "# - Focus on stocks with positive momentum indicators such as:\n",
    "#   * Positive 1-year or 6-month price change (>0%)\n",
    "#   * Strong RSI values (above 50)\n",
    "#   * Good momentum ranks (in top 40% of their category)\n",
    "#   * Positive MACD indicators\n",
    "#   * Price above key moving averages\n",
    "# - Sort results by momentum ranks when available (Rank 1 is best)\n",
    "# - Be explicit about which momentum criteria you're using\n",
    "\n",
    "# If the question is about a specific stock, make sure to include:\n",
    "# - Technical indicators (RSI, MACD, etc.) if available\n",
    "# - Price performance data\n",
    "# - Strength and weakness analysis\n",
    "# - Momentum rankings if available\n",
    "# - Genearate Buy and Sell recomendations\n",
    "\n",
    "# If you don't know the answer based on the given context, just say you don't have enough information.\n",
    "# Don't make up information that isn't provided in the context.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "\n",
    "# Helpful Answer (in English only, structured and precise):\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE = \"\"\"\n",
    "# You are a financial advisor specializing in Indian stock market analysis.\n",
    "# Use the provided context to answer the question below.\n",
    "# Respond in English only and focus strictly on facts from the context.\n",
    "\n",
    "# CRITICAL RULES:\n",
    "\n",
    "# 1. Stock Categories (Nifty 50, Midcap 150, Smallcap 250, Microcap 250):\n",
    "#    - Include ONLY stocks from the EXACT category requested\n",
    "#    - NEVER mix categories\n",
    "\n",
    "# 2. Ranking Queries (\"top stocks\", \"top 10\", etc.):\n",
    "#    - Use **Normalized Z-Score Rank**, sorted in ascending order (Rank 1 = best)\n",
    "#    - Include EXACTLY the number of stocks requested (e.g., \"top 10\" = ranks 1–10)\n",
    "#    - Format: \"Company Name (SYMBOL) - Normalized Z-Score Rank: X out of Y\"\n",
    "\n",
    "# 3. Category-Specific Questions:\n",
    "#    - Group responses by the requested category\n",
    "#    - Show relevant metrics (price, momentum, rankings, etc.)\n",
    "#    - NEVER mix stocks across different universes\n",
    "\n",
    "# 4. If question mentions:\n",
    "#    - **Normalized Z-Score**: Include ONLY stocks with that rank, sorted ASC by rank\n",
    "#    - **Weighted Z-Score**: Same rule, using Weighted Z-Score Rank\n",
    "#    - **Momentum** or similar:\n",
    "#      * Focus on strong momentum indicators:\n",
    "#        - Positive 1Y or 6M price change (>0%)\n",
    "#        - RSI > 50\n",
    "#        - Top 40% momentum rank\n",
    "#        - Positive MACD\n",
    "#        - Price above key MAs\n",
    "#      * Sort by momentum rank (ascending)\n",
    "\n",
    "# 5. Specific Stock Questions:\n",
    "#    - Include RSI, MACD, price trends, performance, momentum, strengths/weaknesses\n",
    "#    - Provide Buy/Sell recommendation based on indicators\n",
    "\n",
    "# 6. When data is missing:\n",
    "#    - Clearly state that there's not enough information\n",
    "#    - DO NOT fabricate or guess missing details\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question:\n",
    "# {question}\n",
    "\n",
    "# Helpful Answer (English only, structured and precise):\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "You are a financial advisor specializing in Indian stock market analysis.\n",
    "Use the provided context to answer the question below.\n",
    "Respond in English only and focus strictly on facts from the context.\n",
    "\n",
    "CONTEXT INFORMATION:\n",
    "{context}\n",
    "\n",
    "USER QUESTION:\n",
    "{question}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Use ONLY information from the context.\n",
    "2. When presenting stocks with rankings, always format as: \"Company Name (SYMBOL) - Rank: X out of Y\"\n",
    "3. For any stock list, include at least four key metrics for each stock (e.g., current price, momentum rank, normalized rank, RSI)\n",
    "4. When asked about \"top stocks\", ALWAYS sort by Normalized Z-Score Rank in ascending order\n",
    "5. If specific data is missing from the context, clearly state this rather than making assumptions\n",
    "6. When asked about any stock's details, include all available data, prioritizing rank and price at the top\n",
    "7. Always include a BUY/SELL/HOLD or WAIT for buying zone/price recommendation based on the analysis, regardless of the query type\n",
    "8. If a stock falls under BUY or HOLD, also provide an estimated target price for selling\n",
    "9. Always format the response in the best possible way based on the query (e.g., use lists, tables, or sections for clarity and readability)\n",
    "10. When the query is about where to invest a certain amount (e.g., \"Where to invest X Rs for long/short term\"), recommend specific stocks and mention their respective category/universe (e.g., Nifty 50, Midcap 150, Smallcap 250, etc.) alongside each stock name, rank, and normalized z-score rank\n",
    "11. Anything related to the below lines add at the bottom as a NOTE please after any warnings in the response:\n",
    "    - To make informed investment decisions, it's essential to analyze multiple metrics and consider various factors beyond just the Normalized Z-Score Rank.\n",
    "    - It's recommended to consult with a financial advisor or conduct your own research before making any investment decisions.\n",
    "    - Please note that these are general observations and not personalized investment advice.\n",
    "\n",
    "Helpful Answer (English only, structured and precise):\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a090648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_context_window(documents, query, max_tokens=6000):\n",
    "    \"\"\"\n",
    "    Optimize the context window by:\n",
    "    1. Prioritizing documents with key information based on query intent\n",
    "    2. Trimming repetitive content\n",
    "    3. Ensuring context stays within token limits\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "    optimized_docs = []\n",
    "    total_tokens = 0\n",
    "    estimated_tokens_per_char = 0.25  # Rough estimate of tokens per character\n",
    "\n",
    "    # Extract query intent\n",
    "    is_rank_query = any(term in query_lower for term in [\"rank\", \"top\", \"best\"])\n",
    "    is_performance_query = any(term in query_lower for term in [\"performance\", \"momentum\", \"growth\"])\n",
    "    is_comparison_query = re.search(r'compare|vs|versus', query_lower) is not None\n",
    "    is_stock_specific = re.search(r'\\b[A-Z]{2,8}\\b', query) is not None\n",
    "\n",
    "    # 1. Score documents based on relevance to query intent\n",
    "    scored_docs = []\n",
    "    for doc in documents:\n",
    "        score = 0\n",
    "        content = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "\n",
    "        # Prioritize documents with the most relevant information based on query\n",
    "        if is_rank_query and \"normalized_z_rank\" in metadata:\n",
    "            score += 100  # Highest priority for rank queries\n",
    "        if is_performance_query and (\"momentum\" in content.lower() or \"performance\" in content.lower()):\n",
    "            score += 75\n",
    "        if is_comparison_query and re.search(r'comparison|versus|vs', content.lower()):\n",
    "            score += 90\n",
    "        if is_stock_specific:\n",
    "            stock_pattern = re.search(r'\\b([A-Z]{2,8})\\b', query)\n",
    "            if stock_pattern and stock_pattern.group(1) == metadata.get(\"symbol\", \"\"):\n",
    "                score += 120  # Highest priority for exact stock match\n",
    "\n",
    "        # General score based on content quality\n",
    "        if \"STOCK RANKING INFORMATION\" in content:\n",
    "            score += 50\n",
    "        if \"Normalized Z-Score Rank\" in content:\n",
    "            score += 40\n",
    "        if \"Weighted Z-Score Rank\" in content:\n",
    "            score += 30\n",
    "        if \"Current Price\" in content:\n",
    "            score += 20\n",
    "\n",
    "        scored_docs.append((doc, score))\n",
    "\n",
    "    # Sort documents by score (highest first)\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 2. Prepare a list of document sections and their priorities\n",
    "    doc_sections = []\n",
    "    for doc, score in scored_docs:\n",
    "        content = doc.page_content\n",
    "\n",
    "        # Split document into sections - this helps remove redundancy\n",
    "        sections = re.split(r'\\n#{2,3} ', content)\n",
    "\n",
    "        for i, section in enumerate(sections):\n",
    "            if i > 0:\n",
    "                section = \"## \" + section  # Re-add header that was removed by split\n",
    "\n",
    "            # Skip nearly empty sections\n",
    "            if len(section.strip()) < 50:\n",
    "                continue\n",
    "\n",
    "            # Calculate priority based on section content\n",
    "            section_priority = score\n",
    "\n",
    "            if \"STOCK RANKING INFORMATION\" in section:\n",
    "                section_priority += 30\n",
    "            if \"STOCK IDENTIFIER\" in section:\n",
    "                section_priority += 20\n",
    "            if \"Technical Indicators\" in section:\n",
    "                section_priority += 15\n",
    "            if \"Current Price\" in section:\n",
    "                section_priority += 10\n",
    "\n",
    "            # Store section with its priority and metadata\n",
    "            doc_sections.append({\n",
    "                \"section\": section,\n",
    "                \"priority\": section_priority,\n",
    "                \"metadata\": doc.metadata,\n",
    "                \"tokens\": len(section) * estimated_tokens_per_char\n",
    "            })\n",
    "\n",
    "    # Sort sections by priority\n",
    "    doc_sections.sort(key=lambda x: x[\"priority\"], reverse=True)\n",
    "\n",
    "    # 3. Build optimized context while staying within token limit\n",
    "    used_sections = set()  # Track sections we've already used to avoid duplication\n",
    "    essential_metadata = {}  # Track essential metadata like stocks and their ranks\n",
    "\n",
    "    # Create a result with sections organized by priority\n",
    "    result_docs = []\n",
    "\n",
    "    for section_data in doc_sections:\n",
    "        section = section_data[\"section\"]\n",
    "        section_hash = hash(section[:100])  # Use first 100 chars as unique identifier\n",
    "        metadata = section_data[\"metadata\"]\n",
    "\n",
    "        # Skip if we've already added this section or very similar one\n",
    "        if section_hash in used_sections:\n",
    "            continue\n",
    "\n",
    "        # Skip if this would exceed token limit (leaving some room for formatting)\n",
    "        if total_tokens + section_data[\"tokens\"] > max_tokens - 200:\n",
    "            break\n",
    "\n",
    "        # Add the section and update tracking\n",
    "        section_doc = Document(page_content=section, metadata=metadata)\n",
    "        result_docs.append(section_doc)\n",
    "        used_sections.add(section_hash)\n",
    "        total_tokens += section_data[\"tokens\"]\n",
    "\n",
    "        # Track essential metadata we've included\n",
    "        symbol = metadata.get(\"symbol\", \"\")\n",
    "        if symbol:\n",
    "            essential_metadata[symbol] = essential_metadata.get(symbol, 0) + 1\n",
    "\n",
    "    # 4. If we're still below token limit, add any stock metadata that might be missing\n",
    "    stock_pattern = re.search(r'\\b([A-Z]{2,8})\\b', query)\n",
    "    if stock_pattern and total_tokens < max_tokens - 500:\n",
    "        query_stock = stock_pattern.group(1)\n",
    "        if query_stock not in essential_metadata:\n",
    "            # Find any document about this stock\n",
    "            for doc, _ in scored_docs:\n",
    "                if doc.metadata.get(\"symbol\", \"\") == query_stock:\n",
    "                    # Add a summarized version\n",
    "                    summary = f\"STOCK IDENTIFIER: {query_stock}\\n\"\n",
    "                    summary += \"\\n\".join(doc.page_content.split(\"\\n\")[:10])  # Take first 10 lines\n",
    "                    summary_doc = Document(page_content=summary, metadata=doc.metadata)\n",
    "                    result_docs.append(summary_doc)\n",
    "                    break\n",
    "\n",
    "    print(f\"Optimized context: {len(result_docs)} sections, ~{total_tokens:.0f} tokens\")\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c33d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_response(response, query):\n",
    "    # Check if this is a category or ranking query\n",
    "    query_lower = query.lower()\n",
    "    categories = [\"nifty\", \"midcap\", \"smallcap\", \"microcap\"]\n",
    "    ranking_terms = [\"top\", \"rank\", \"best\", \"performance\"]\n",
    "\n",
    "    # Skip expensive regex processing for simple queries\n",
    "    if not any(term in query_lower for term in categories + ranking_terms):\n",
    "        return response\n",
    "\n",
    "    # Only extract symbols if needed (optimization)\n",
    "    if any(cat in query_lower for cat in categories):\n",
    "        # Extract any stock symbols from the response\n",
    "        symbol_matches = re.findall(r'(?:\\(([A-Z]{2,8})\\)|\\b([A-Z]{2,8})\\b)', response)\n",
    "        stock_symbols = set()\n",
    "        for match in symbol_matches:\n",
    "            if match[0]:  # Format (SYMBOL)\n",
    "                stock_symbols.add(match[0])\n",
    "            elif match[1]:  # Format SYMBOL\n",
    "                stock_symbols.add(match[1])\n",
    "\n",
    "        # Check if the stock symbols mentioned actually exist in our database\n",
    "        verified_symbols = []\n",
    "        unverified_symbols = []\n",
    "\n",
    "        # Use the collection to verify if these stocks exist in our database\n",
    "        if stock_symbols:  # Only process if we found symbols\n",
    "            collection = vectorstore._collection\n",
    "            for symbol in stock_symbols:\n",
    "                # Check if any document has this symbol\n",
    "                result = collection.get(\n",
    "                    where={\"symbol\": symbol},\n",
    "                    limit=1\n",
    "                )\n",
    "                if result['ids'] and len(result['ids']) > 0:\n",
    "                    verified_symbols.append(symbol)\n",
    "                else:\n",
    "                    unverified_symbols.append(symbol)\n",
    "\n",
    "        # Add warning if unverified symbols were found\n",
    "        if unverified_symbols:\n",
    "            response += f\"\\n\\n⚠️ Warning: The following symbols mentioned in the response may not exist in our database: {', '.join(unverified_symbols)}\"\n",
    "\n",
    "    # Add the standard post-processing checks\n",
    "    is_category_query = any(cat in query_lower for cat in categories)\n",
    "    is_ranking_query = any(term in query_lower for term in ranking_terms)\n",
    "\n",
    "    if is_category_query or is_ranking_query:\n",
    "        # Check if response contains rank information when it should\n",
    "        if is_ranking_query and not re.search(r'Rank:\\s*\\d+\\s*out of\\s*\\d+', response):\n",
    "            response += \"\\n\\nNote: Results are sorted by Normalized Z-Score Rank (Rank 1 is best).\"\n",
    "\n",
    "        # Check if response might be mixing categories\n",
    "        for cat in categories:\n",
    "            if cat in query_lower:\n",
    "                other_cats = [c for c in categories if c != cat and c in response.lower()]\n",
    "                if other_cats:\n",
    "                    response += f\"\\n\\nNote: This query was specifically about {cat.title()} stocks. Results from other categories have been excluded.\"\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add after creating vectorstore\n",
    "\n",
    "def verify_vectorstore_consistency():\n",
    "    \"\"\"Verify that stocks in vector database match the actual markdown files\"\"\"\n",
    "    print(\"Verifying vector database consistency...\")\n",
    "\n",
    "    # Get all stock symbols from the original markdown files\n",
    "    original_symbols = set()\n",
    "    for folder in folders:\n",
    "        md_files = glob.glob(os.path.join(folder, \"*.md\"))\n",
    "        for file in md_files:\n",
    "            symbol = os.path.splitext(os.path.basename(file))[0]\n",
    "            original_symbols.add(symbol)\n",
    "\n",
    "    # Get all symbols from the vector database\n",
    "    collection = vectorstore._collection\n",
    "    result = collection.get(include=['metadatas'])\n",
    "    db_symbols = set()\n",
    "    for metadata in result['metadatas']:\n",
    "        if 'symbol' in metadata:\n",
    "            db_symbols.add(metadata['symbol'])\n",
    "\n",
    "    # Compare\n",
    "    missing_symbols = original_symbols - db_symbols\n",
    "    extra_symbols = db_symbols - original_symbols\n",
    "\n",
    "    print(f\"Total symbols in markdown files: {len(original_symbols)}\")\n",
    "    print(f\"Total symbols in vector database: {len(db_symbols)}\")\n",
    "\n",
    "    if missing_symbols:\n",
    "        print(f\"Warning: {len(missing_symbols)} symbols from markdown files are missing in the database:\")\n",
    "        print(\", \".join(sorted(missing_symbols)))\n",
    "\n",
    "    if extra_symbols:\n",
    "        print(f\"Warning: {len(extra_symbols)} symbols in database don't exist in markdown files:\")\n",
    "        print(\", \".join(sorted(extra_symbols)))\n",
    "\n",
    "    return len(missing_symbols) == 0 and len(extra_symbols) == 0\n",
    "\n",
    "# Run the verification\n",
    "db_is_consistent = verify_vectorstore_consistency()\n",
    "if not db_is_consistent:\n",
    "    print(\"⚠️ Vector database consistency check failed - search results may be inaccurate\")\n",
    "else:\n",
    "    print(\"✅ Vector database is consistent with markdown files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f00163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base retriever parameters\n",
    "max_k = min(100, len(chunks) // 10)  # Cap at 100 or 10% of chunks, whichever is smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_vectorstore_filter(query):\n",
    "    \"\"\"Create advanced filters based on query intent and content\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    filters = {}\n",
    "\n",
    "    # Extract category filters\n",
    "    categories_map = {\n",
    "        r'\\b(?:nifty(?:\\s*50)?|nifty50)\\b': \"nifty_50\",\n",
    "        r'\\b(?:midcap|mid\\s*cap|mid-cap)\\b': \"midcap_150\",\n",
    "        r'\\b(?:smallcap|small\\s*cap|small-cap)\\b': \"smallcap_250\",\n",
    "        r'\\b(?:microcap|micro\\s*cap|micro-cap)\\b': \"microcap_250\"\n",
    "    }\n",
    "\n",
    "    for pattern, category in categories_map.items():\n",
    "        if re.search(pattern, query_lower):\n",
    "            filters[\"category\"] = category\n",
    "            break\n",
    "\n",
    "    # Extract specific rank type filters\n",
    "    if \"normalized z-score\" in query_lower or \"normalized rank\" in query_lower:\n",
    "        # Add metadata filter to prioritize documents with normalized z-score information\n",
    "        filters[\"has_normalized_rank\"] = True\n",
    "    elif \"weighted z-score\" in query_lower or \"weighted rank\" in query_lower:\n",
    "        # Add metadata filter to prioritize documents with weighted z-score information\n",
    "        filters[\"has_weighted_rank\"] = True\n",
    "\n",
    "    # Extract performance filters\n",
    "    if any(term in query_lower for term in [\"momentum\", \"performance\", \"growth\"]):\n",
    "        filters[\"performance_focus\"] = True\n",
    "\n",
    "    # Extract price filters\n",
    "    if any(term in query_lower for term in [\"price\", \"undervalued\", \"expensive\"]):\n",
    "        filters[\"price_focus\"] = True\n",
    "\n",
    "    # Extract time period filters\n",
    "    if \"1-year\" in query_lower or \"one year\" in query_lower or \"12 month\" in query_lower:\n",
    "        filters[\"timeframe\"] = \"one_year\"\n",
    "    elif \"6-month\" in query_lower or \"six month\" in query_lower or \"half year\" in query_lower:\n",
    "        filters[\"timeframe\"] = \"six_month\"\n",
    "\n",
    "    # Extract comparison intent\n",
    "    comparison_match = re.search(r'(?:compare|vs|versus|against)\\s+(\\w+)\\s+(?:and|with|to)\\s+(\\w+)', query_lower)\n",
    "    if comparison_match:\n",
    "        filters[\"comparison\"] = [comparison_match.group(1), comparison_match.group(2)]\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a398746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_category_query(query, vectorstore):\n",
    "    \"\"\"Add category-specific processing for better retrieval\"\"\"\n",
    "    # Get enhanced filters based on query\n",
    "    filters = enhanced_vectorstore_filter(query)\n",
    "\n",
    "    # Check for comparison queries first (highest priority)\n",
    "    if \"comparison\" in filters:\n",
    "        symbols = [s.upper() for s in filters[\"comparison\"]]\n",
    "        print(f\"Detected comparison between: {' and '.join(symbols)}\")\n",
    "        # Use OR filter to get documents for both stocks\n",
    "        filtered_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": 20,\n",
    "                \"fetch_k\": 40,\n",
    "                \"lambda_mult\": 0.5,\n",
    "                \"filter\": {\n",
    "                    \"$or\": [\n",
    "                        {\"symbol\": symbols[0]},\n",
    "                        {\"symbol\": symbols[1]}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        return filtered_retriever, None, True\n",
    "\n",
    "    # Handle category-specific filtering\n",
    "    if \"category\" in filters:\n",
    "        # Use category filter with performance focus if needed\n",
    "        filter_dict = {\"doc_type\": filters[\"category\"]}\n",
    "\n",
    "        # If this is a specific stock query within a category\n",
    "        symbol_match = re.search(r'\\b([A-Z]{2,8})\\b', query)\n",
    "        if symbol_match:\n",
    "            symbol = symbol_match.group(1)\n",
    "            print(f\"Detected stock {symbol} in {filters['category']}\")\n",
    "            filter_dict = {\n",
    "                \"$and\": [\n",
    "                    {\"doc_type\": filters[\"category\"]},\n",
    "                    {\"symbol\": symbol}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "        filtered_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": min(75, len(chunks) // 8),\n",
    "                \"fetch_k\": min(150, len(chunks) // 4),\n",
    "                \"lambda_mult\": 0.5,\n",
    "                \"filter\": filter_dict\n",
    "            }\n",
    "        )\n",
    "        return filtered_retriever, filters[\"category\"], True\n",
    "\n",
    "    # Handle specific stock queries\n",
    "    stock_query_match = re.search(r'\\b([A-Z]{2,8})\\b', query)\n",
    "    if stock_query_match:\n",
    "        symbol = stock_query_match.group(1)\n",
    "        print(f\"Detected potential stock symbol: {symbol}\")\n",
    "        filtered_retriever = vectorstore.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\n",
    "                \"k\": min(15, len(chunks) // 30),\n",
    "                \"fetch_k\": min(30, len(chunks) // 15),\n",
    "                \"lambda_mult\": 0.3,\n",
    "                \"filter\": {\"symbol\": symbol}\n",
    "            }\n",
    "        )\n",
    "        return filtered_retriever, None, False\n",
    "\n",
    "    # Default retriever with improved parameters\n",
    "    momentum_filter = any(term in query.lower() for term in\n",
    "                         [\"momentum\", \"rank\", \"performance\", \"top\", \"best\"])\n",
    "\n",
    "    return vectorstore.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": min(40, len(chunks) // 10),\n",
    "            \"fetch_k\": min(80, len(chunks) // 6),\n",
    "            \"lambda_mult\": 0.6,\n",
    "        }\n",
    "    ), None, momentum_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dad6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(query):\n",
    "    \"\"\"Test function to check what documents are being retrieved for a query\"\"\"\n",
    "    # Get the query keywords\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # Check for category keywords\n",
    "    categories = {\n",
    "        \"nifty 50\": \"nifty_50\",\n",
    "        \"nifty50\": \"nifty_50\",\n",
    "        \"nifty\": \"nifty_50\",\n",
    "        \"midcap\": \"midcap_150\",\n",
    "        \"mid-cap\": \"midcap_150\",\n",
    "        \"mid cap\": \"midcap_150\",\n",
    "        \"smallcap\": \"smallcap_250\",\n",
    "        \"small-cap\": \"smallcap_250\",\n",
    "        \"small cap\": \"smallcap_250\",\n",
    "        \"microcap\": \"microcap_250\",\n",
    "        \"micro-cap\": \"microcap_250\",\n",
    "        \"micro cap\": \"microcap_250\"\n",
    "    }\n",
    "\n",
    "    detected = False\n",
    "    for keyword, category in categories.items():\n",
    "        if keyword in query_lower:\n",
    "            print(f\"Found category keyword '{keyword}' -> '{category}'\")\n",
    "            detected = True\n",
    "\n",
    "    if not detected:\n",
    "        print(\"No category keyword detected in query\")\n",
    "\n",
    "    # Test retrieval with the process_category_query\n",
    "    specific_retriever = process_category_query(query, vectorstore)\n",
    "    docs = specific_retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Check document metadata\n",
    "    print(f\"\\nRetrieved {len(docs)} documents\")\n",
    "    categories_found = {}\n",
    "\n",
    "    for i, doc in enumerate(docs[:5]):  # Check first 5 docs\n",
    "        doc_type = doc.metadata.get('doc_type', 'unknown')\n",
    "        categories_found[doc_type] = categories_found.get(doc_type, 0) + 1\n",
    "        if i < 3:  # Show details for first 3 docs\n",
    "            print(f\"\\nDocument {i+1}:\")\n",
    "            print(f\"  Metadata: {doc.metadata}\")\n",
    "            print(f\"  Content (first 100 chars): {doc.page_content[:100]}...\")\n",
    "\n",
    "    print(\"\\nDocument categories distribution:\")\n",
    "    for cat, count in categories_found.items():\n",
    "        print(f\"  - {cat}: {count} documents\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "# Uncomment and run this to debug specific queries\n",
    "# debug_retrieval(\"Show me Nifty 50 stocks\")\n",
    "# debug_retrieval(\"List midcap stocks with good RSI\")\n",
    "# debug_retrieval(\"What are the smallcap stocks with strong momentum?\")\n",
    "# debug_retrieval(\"Show me microcap stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b305937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_semantic_search(query, vectorstore):\n",
    "    \"\"\"\n",
    "    Improve semantic search with hybrid retrieval strategies:\n",
    "    1. Use query expansion with related financial terms\n",
    "    2. Apply reranking for better result ordering\n",
    "    3. Handle ambiguous queries with multiple strategies\n",
    "    \"\"\"\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    # 1. Expand query with relevant financial terms if needed\n",
    "    expanded_query = query\n",
    "    financial_expansions = {\n",
    "        \"good stocks\": \"stocks with positive momentum high rank normalized z-score\",\n",
    "        \"best performers\": \"stocks with top normalized z-score rank performance\",\n",
    "        \"undervalued\": \"stocks with good fundamentals but lower price performance\",\n",
    "        \"growth stocks\": \"stocks with strong momentum positive performance\",\n",
    "        \"compare\": \"comparison analysis performance metrics between stocks\",\n",
    "    }\n",
    "\n",
    "    # Add relevant domain-specific terms to query\n",
    "    for key_term, expansion in financial_expansions.items():\n",
    "        if key_term in query_lower and expansion not in query_lower:\n",
    "            expanded_query = f\"{query} {expansion}\"\n",
    "            print(f\"Expanded query: {expanded_query}\")\n",
    "            break\n",
    "\n",
    "    # 2. Handle ambiguous queries with multiple retrieval strategies\n",
    "    results = []\n",
    "\n",
    "    # Regular vector search with expanded query\n",
    "    retriever, category, is_ranking = process_category_query(expanded_query, vectorstore)\n",
    "    main_results = retriever.get_relevant_documents(expanded_query)\n",
    "\n",
    "    # If results seem weak or query is ambiguous, try alternative approaches\n",
    "    if len(main_results) < 5 or \"?\" in query or \"or\" in query_lower:\n",
    "        print(\"Query might be ambiguous - trying alternative retrieval strategies\")\n",
    "\n",
    "        # Try filtering by potential stock symbols as a fallback\n",
    "        symbols = re.findall(r'\\b([A-Z]{2,8})\\b', query)\n",
    "        if symbols:\n",
    "            for symbol in symbols:\n",
    "                symbol_retriever = vectorstore.as_retriever(\n",
    "                    search_type=\"similarity\",\n",
    "                    search_kwargs={\n",
    "                        \"k\": 3,\n",
    "                        \"filter\": {\"symbol\": symbol}\n",
    "                    }\n",
    "                )\n",
    "                symbol_results = symbol_retriever.get_relevant_documents(expanded_query)\n",
    "                if symbol_results:\n",
    "                    results.extend(symbol_results)\n",
    "\n",
    "    # Add main results to any special results\n",
    "    results.extend(main_results)\n",
    "\n",
    "    # 3. Apply reranking based on query intent\n",
    "    reranked_results = []\n",
    "\n",
    "    # Determine intent for reranking\n",
    "    is_rank_query = any(term in query_lower for term in\n",
    "                       [\"rank\", \"top\", \"best\", \"normalized\", \"weighted\"])\n",
    "    is_performance_query = any(term in query_lower for term in\n",
    "                             [\"performance\", \"momentum\", \"growing\", \"growth\"])\n",
    "\n",
    "    # Score documents for reranking\n",
    "    scored_docs = []\n",
    "    for doc in results:\n",
    "        content = doc.page_content\n",
    "        score = 0\n",
    "\n",
    "        # Base relevance score from embedding similarity is already applied\n",
    "        # Add domain-specific relevance scoring\n",
    "        if is_rank_query:\n",
    "            # For rank queries, prioritize documents with ranking information\n",
    "            normalized_match = re.search(r'Normalized Z-Score Rank:\\s*(\\d+)\\s*out of\\s*(\\d+)', content)\n",
    "            if normalized_match:\n",
    "                rank = int(normalized_match.group(1))\n",
    "                total = int(normalized_match.group(2))\n",
    "                # Lower ranks (better) get higher scores\n",
    "                score += (total - rank + 1) / total * 100\n",
    "\n",
    "        if is_performance_query:\n",
    "            # For performance queries, prioritize documents with performance metrics\n",
    "            if \"momentum ratio\" in content.lower():\n",
    "                score += 50\n",
    "            if \"positive momentum\" in content.lower():\n",
    "                score += 30\n",
    "\n",
    "        # General relevance factors\n",
    "        if \"STOCK RANKING INFORMATION\" in content:\n",
    "            score += 40\n",
    "        if \"STOCK IDENTIFIER\" in content:\n",
    "            score += 20\n",
    "\n",
    "        scored_docs.append((doc, score))\n",
    "\n",
    "    # Sort by score (highest first) and get docs\n",
    "    scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "    reranked_results = [doc for doc, _ in scored_docs]\n",
    "\n",
    "    # Return optimized results\n",
    "    return optimize_context_window(reranked_results, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d9c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCategoryRetriever(BaseRetriever):\n",
    "    vectorstore: Any = Field(description=\"Vector store for embeddings\")\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Use the improved semantic search approach (new addition)\n",
    "        docs = improved_semantic_search(query, self.vectorstore)\n",
    "        return docs\n",
    "\n",
    "    async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        # Just call the synchronous version for simplicity\n",
    "        return self.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba7752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL = \"meta-llama/llama-4-maverick:free\"\n",
    "\n",
    "# OPTIMUS_ALPHA = os.getenv('OPTIMUS_ALPHA')\n",
    "\n",
    "# # With this (for OpenRouter integration with LangChain):\n",
    "# llm = ChatOpenAI(\n",
    "#     openai_api_key=OPTIMUS_ALPHA,\n",
    "#     openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "#     max_tokens=3072,  # Reduced from 2048 to 1500\n",
    "#     temperature=0.4,  # Reduced from 0.4 to 0.3\n",
    "#     model_name=MODEL, # \"openrouter/optimus-alpha\",  # Replace with your chosen model\n",
    "#     # timeout= 45,            # Add timeout parameter\n",
    "#     model_kwargs={\n",
    "#         \"top_p\": 0.3,              # Reduced from 0.2 to 0.1\n",
    "#         \"presence_penalty\": -0.1,\n",
    "#         \"frequency_penalty\": 0.2,\n",
    "#         \"seed\": int(datetime.now().strftime('%Y%m%d')),\n",
    "#         # \"stream\": True           # Enable streaming for faster perceived response\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with GenerativeAI\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "llm = ChatGoogleGenerativeAI(model=MODEL, temperature=0.7, google_api_key = GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8301af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the conversation memory for the chat - implement sliding window\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True,\n",
    "    output_key='answer',\n",
    "    k=3  # Only keep last 3 exchanges to limit context size\n",
    ")\n",
    "\n",
    "# Create the dynamic retriever\n",
    "retriever = DynamicCategoryRetriever(vectorstore=vectorstore)\n",
    "\n",
    "# Create the conversation chain as a global variable so it's accessible\n",
    "global conversation_chain\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587fedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to inspect the vectorstore for category distribution\n",
    "def inspect_vectorstore_categories(vectorstore):\n",
    "    collection = vectorstore._collection\n",
    "    result = collection.get(include=['metadatas'], limit=1000)\n",
    "\n",
    "    metadatas = result['metadatas']\n",
    "    categories = {}\n",
    "\n",
    "    print(f\"Inspecting {len(metadatas)} documents in vectorstore\")\n",
    "\n",
    "    # Count documents by doc_type (category)\n",
    "    for metadata in metadatas:\n",
    "        doc_type = metadata.get('doc_type', 'unknown')\n",
    "        categories[doc_type] = categories.get(doc_type, 0) + 1\n",
    "\n",
    "    print(\"\\nDocument distribution by category:\")\n",
    "    for category, count in categories.items():\n",
    "        print(f\"  - {category}: {count} documents\")\n",
    "\n",
    "    return categories\n",
    "\n",
    "# Run this to check your vectorstore category distribution\n",
    "categories_in_db = inspect_vectorstore_categories(vectorstore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c304937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    global conversation_chain\n",
    "    try:\n",
    "        # Log the incoming query\n",
    "        print(f\"\\n>>> Processing query: '{message}'\")\n",
    "\n",
    "        # Add cross-verification for category detection\n",
    "        detected_categories = []\n",
    "        for pattern, category in [\n",
    "            (r'\\b(?:nifty(?:\\s*50)?|nifty50)\\b', \"nifty_50\"),\n",
    "            (r'\\b(?:midcap|mid\\s*cap|mid-cap)\\b', \"midcap_150\"),\n",
    "            (r'\\b(?:smallcap|small\\s*cap|small-cap)\\b', \"smallcap_250\"),\n",
    "            (r'\\b(?:microcap|micro\\s*cap|micro-cap)\\b', \"microcap_250\")\n",
    "        ]:\n",
    "            if re.search(pattern, message.lower()):\n",
    "                detected_categories.append(category)\n",
    "                print(f\"  Detected category: {category}\")\n",
    "\n",
    "        # System context creation\n",
    "        system_context = \"You are analyzing Indian stock market data. \"\n",
    "        if detected_categories:\n",
    "            system_context += f\"Focus on {', '.join(detected_categories)} stocks. \"\n",
    "\n",
    "        # Process stock symbols mentioned\n",
    "        verified_symbols = []\n",
    "        unverified_symbols = []\n",
    "        stock_symbols = re.findall(r'\\b([A-Z]{2,8})\\b', message)\n",
    "\n",
    "        if stock_symbols:\n",
    "            print(f\"  Potential stock symbols in query: {', '.join(stock_symbols)}\")\n",
    "            system_context += f\"Pay special attention to symbols: {', '.join(stock_symbols)}. \"\n",
    "\n",
    "            # Check symbols against database\n",
    "            collection = vectorstore._collection\n",
    "            for symbol in stock_symbols:\n",
    "                result = collection.get(\n",
    "                    where={\"symbol\": symbol},\n",
    "                    limit=1\n",
    "                )\n",
    "\n",
    "                if result['ids'] and len(result['ids']) > 0:\n",
    "                    verified_symbols.append(symbol)\n",
    "                    print(f\"  Confirmed symbol exists: {symbol}\")\n",
    "                else:\n",
    "                    unverified_symbols.append(symbol)\n",
    "                    print(f\"  ⚠️ Symbol not found in database: {symbol}\")\n",
    "\n",
    "        # Process query with timeout\n",
    "        try:\n",
    "            result = conversation_chain.invoke({\n",
    "                \"question\": message\n",
    "            }, config={\"timeout\": 45})\n",
    "\n",
    "            # Check if result is None\n",
    "            if result is None:\n",
    "                print(\"Warning: conversation_chain.invoke() returned None\")\n",
    "                return \"I couldn't process your request. Please try again with a different question.\"\n",
    "\n",
    "            # For non-streaming response\n",
    "            answer = result.get(\"answer\", \"No answer was generated.\")\n",
    "\n",
    "            # Apply post-processing\n",
    "            processed_answer = post_process_response(answer, message)\n",
    "\n",
    "            # Log completed response\n",
    "            print(f\">>> Completed processing query: '{message}'\")\n",
    "            return processed_answer\n",
    "\n",
    "        except AttributeError as ae:\n",
    "            # This might happen if result is a streaming object\n",
    "            print(f\"Handling streaming response: {ae}\")\n",
    "            if result is None:\n",
    "                return \"I couldn't generate a response. Please try again.\"\n",
    "            elif hasattr(result, \"content\"):\n",
    "                answer = result.content\n",
    "            else:\n",
    "                answer = \"I processed your query, but had trouble formatting the response. Please try again.\"\n",
    "\n",
    "            processed_answer = post_process_response(answer, message)\n",
    "            return processed_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in chat function: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # Print the full traceback for more debugging info\n",
    "        return f\"Sorry, an error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82727fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom CSS for better spacing and layout\n",
    "\n",
    "# css = \"\"\"\n",
    "# .gradio-container {max-width: 1200px !important; margin-left: auto !important; margin-right: auto !important;}\n",
    "# .plot-container {width: 100% !important; display: flex !important; justify-content: center !important;}\n",
    "# .visualization-row {display: flex !important; justify-content: space-between !important; width: 100% !important;}\n",
    "# .visualization-column {flex: 1 !important; padding: 0 10px !important;}\n",
    "# \"\"\"\n",
    "\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 100% !important;\n",
    "    width: 100% !important;\n",
    "    margin: 0 !important;\n",
    "    padding: 0 !important;\n",
    "    min-height: 100vh !important;\n",
    "}\n",
    "\n",
    "/* Main layout */\n",
    ".app-container {\n",
    "    display: flex;\n",
    "    flex-direction: row;\n",
    "    min-height: 100vh;\n",
    "}\n",
    "\n",
    "/* Left side: chat */\n",
    ".chat-container {\n",
    "    flex: 1;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    padding: 20px;\n",
    "    height: 100vh;\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    ".chat-container > .prose {\n",
    "    flex: 1;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "\n",
    ".message-wrap, .chatbot-message-container {\n",
    "    height: 100% !important;\n",
    "    overflow-y: auto !important;\n",
    "}\n",
    "\n",
    "/* Right side: plots */\n",
    ".plots-container {\n",
    "    flex: 1;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    padding: 20px;\n",
    "    height: 100vh;\n",
    "    box-sizing: border-box;\n",
    "}\n",
    "\n",
    "/* Vertical stacking of plots without extra space */\n",
    ".visualization-row {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    justify-content: space-between;  /* evenly distribute space */\n",
    "    width: 100%;\n",
    "    height: 100%;\n",
    "    gap: 0; /* remove space between plots */\n",
    "}\n",
    "\n",
    "/* Each plot container takes 50% */\n",
    ".plot-column {\n",
    "    flex: 1;\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    margin-bottom: 0;\n",
    "    padding: 0;\n",
    "}\n",
    "\n",
    "/* Center and fit plots */\n",
    ".plot-container {\n",
    "    width: 100% !important;\n",
    "    height: 100% !important;\n",
    "    display: flex;\n",
    "    justify-content: center;\n",
    "    align-items: center;\n",
    "    margin: 0;\n",
    "}\n",
    "\n",
    "/* Mobile responsiveness */\n",
    "@media (max-width: 992px) {\n",
    "    .app-container {\n",
    "        flex-direction: column;\n",
    "    }\n",
    "\n",
    "    .plots-container,\n",
    "    .chat-container {\n",
    "        height: auto;\n",
    "        min-height: 50vh;\n",
    "    }\n",
    "\n",
    "    .visualization-row {\n",
    "        flex-direction: column;\n",
    "        height: auto;\n",
    "    }\n",
    "\n",
    "    .plot-column {\n",
    "        height: 40vh;\n",
    "    }\n",
    "\n",
    "    .message-wrap, .chatbot-message-container {\n",
    "        height: calc(50vh - 120px) !important;\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7436eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Blocks interface with visualizations\n",
    "with gr.Blocks(css=css, theme=\"soft\") as view:\n",
    "    with gr.Row(elem_classes=\"app-container\"):\n",
    "        # Left side - Chat interface\n",
    "        with gr.Column(elem_classes=\"chat-container\"):\n",
    "            gr.HTML(f\"<h2 style='text-align:center;'>Stock Market Analysis By AJ14314</h2><p style='text-align:center;'>Using model: <b>{MODEL}</b></p>\")\n",
    "            chat_interface = gr.ChatInterface(\n",
    "                fn=chat,\n",
    "                examples=[\n",
    "                    \"List the top 10 Nifty 50 stocks\",\n",
    "                    \"List the top 20 performing midcap stocks\",\n",
    "                    \"What are the smallcap stocks with positive momentum?\",\n",
    "                    \"List microcap stocks with highest ranks from 1-25\",\n",
    "                    \"Compare Reliance and TCS performance\"\n",
    "                ],\n",
    "                description=\"Ask questions about Indian stocks to get insights based on technical analysis reports.\",\n",
    "                theme=\"soft\",\n",
    "                autofocus=True,\n",
    "                fill_height=True\n",
    "            )\n",
    "\n",
    "        # Right side - Plots container with 3D on top and 2D below\n",
    "        with gr.Column(elem_classes=\"plots-container\"):\n",
    "            # 3D Plot on top\n",
    "            with gr.Column(elem_classes=\"plot-3d-container\"):\n",
    "                gr.HTML(\"<h3 style='text-align:center;'>3D Vector Space Visualization</h3>\")\n",
    "                plot_3d = gr.Plot(create_3d_visualization(), elem_id=\"plot-3d\", container=True)\n",
    "\n",
    "            # 2D Plot below\n",
    "            with gr.Column(elem_classes=\"plot-2d-container\"):\n",
    "                gr.HTML(\"<h3 style='text-align:center;'>2D Vector Space Visualization</h3>\")\n",
    "                plot_2d = gr.Plot(create_2d_visualization(), elem_id=\"plot-2d\", container=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "view.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee217852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
